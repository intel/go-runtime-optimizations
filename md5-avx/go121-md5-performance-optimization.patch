From 2e6913a73f22f34770daf76d0953eecb84f69044 Mon Sep 17 00:00:00 2001
From: parkerha1 <priya.n.vaidya@intel.com>
Date: Mon, 5 Dec 2022 18:18:59 +0000
Subject: [PATCH] md5: performance optimization

AVX512 assembly optimization of md5 block function to achive significant performance gain
uses vpternlod instruction

Change-Id: I4725edaa9b300c8b02480348ec1009043db14d61
---
 src/crypto/md5/md5block_amd64.go |   9 +
 src/crypto/md5/md5block_amd64.s  | 278 ++++++++++++++++++++++++++++++-
 src/internal/cpu/cpu.go          |  43 ++---
 src/internal/cpu/cpu_x86.go      |  33 +++-
 4 files changed, 329 insertions(+), 34 deletions(-)
 create mode 100644 src/crypto/md5/md5block_amd64.go

diff --git a/src/crypto/md5/md5block_amd64.go b/src/crypto/md5/md5block_amd64.go
new file mode 100644
index 0000000000..819787a3d9
--- /dev/null
+++ b/src/crypto/md5/md5block_amd64.go
@@ -0,0 +1,9 @@
+// Copyright 2017 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package md5
+
+import "internal/cpu"
+
+var useAVX512 = cpu.X86.HasAVX512F && cpu.X86.HasAVX512VL
diff --git a/src/crypto/md5/md5block_amd64.s b/src/crypto/md5/md5block_amd64.s
index 7c7d92d7e8..d473c3107b 100644
--- a/src/crypto/md5/md5block_amd64.s
+++ b/src/crypto/md5/md5block_amd64.s
@@ -14,6 +14,9 @@
 // in the public domain.
 
 TEXT	·block(SB),NOSPLIT,$8-32
+        CMPB     ·useAVX512(SB), $1
+       JE avx
+
 	MOVQ	dig+0(FP),	BP
 	MOVQ	p+8(FP),	SI
 	MOVQ	p_len+16(FP), DX
@@ -40,10 +43,11 @@ loop:
 
 #define ROUND1(a, b, c, d, index, const, shift) \
 	XORL	c, R9; \
-	LEAL	const(a)(R8*1), a; \
+        ADDL $const, a; \
 	ANDL	b, R9; \
+        ADDL  R8, a; \
 	XORL d, R9; \
-	MOVL (index*4)(SI), R8; \
+        MOVL (index*4)(SI), R8; \
 	ADDL R9, a; \
 	ROLL $shift, a; \
 	MOVL c, R9; \
@@ -72,13 +76,15 @@ loop:
 
 #define ROUND2(a, b, c, d, index, const, shift) \
 	NOTL	R9; \
-	LEAL	const(a)(R8*1),a; \
+	ADDL $const, a; \
+        ANDL    c,   R9; \
+        ADDL    R8, a; \
+        MOVL    d,   R10; \
+        ADDL    R9,  a ; \
 	ANDL	b,		R10; \
-	ANDL	c,		R9; \
+        ADDL    R10, a; \
 	MOVL	(index*4)(SI),R8; \
-	ORL	R9,		R10; \
 	MOVL	c,		R9; \
-	ADDL	R10,		a; \
 	MOVL	c,		R10; \
 	ROLL	$shift,	a; \
 	ADDL	b,		a
@@ -104,7 +110,8 @@ loop:
 	MOVL	CX,		R9
 
 #define ROUND3(a, b, c, d, index, const, shift) \
-	LEAL	const(a)(R8*1),a; \
+	ADDL	$const,a; \
+        ADDL    R8, a; \
 	MOVL	(index*4)(SI),R8; \
 	XORL	d,		R9; \
 	XORL	b,		R9; \
@@ -135,7 +142,8 @@ loop:
 	XORL	DX,		R9
 
 #define ROUND4(a, b, c, d, index, const, shift) \
-	LEAL	const(a)(R8*1),a; \
+        ADDL    $const, a; \
+        ADDL    R8, a; \
 	ORL	b,		R9; \
 	XORL	c,		R9; \
 	ADDL	R9,		a; \
@@ -177,3 +185,257 @@ end:
 	MOVL	CX,		(2*4)(BP)
 	MOVL	DX,		(3*4)(BP)
 	RET
+
+
+avx:
+	MOVQ	dig+0(FP),	BP
+	MOVQ	p+8(FP),	SI
+	MOVQ	p_len+16(FP), DX
+	SHRQ	$6,		DX
+	SHLQ	$6,		DX
+        MOVQ    $K256<>(SB), CX
+
+        VMOVDQA32 (CX), X16
+ 
+ 
+	LEAQ	(SI)(DX*1),	DI
+	MOVL	(0*4)(BP),	X0
+	MOVL	(1*4)(BP),	X1
+	MOVL	(2*4)(BP),	X2
+	MOVL	(3*4)(BP),	X3
+
+	CMPQ	SI,		DI
+	JEQ	end1
+
+loop1:
+
+	MOVD 	X0,		X12
+	MOVD	X1,		X13
+	MOVD	X2,		X14
+	MOVD	X3,		X15
+
+	MOVL	(0*4)(SI),	X8
+	MOVD	X3,		X9
+
+#define ROUND11(a, b, c, d, index, const, shift) \
+	VPXORD	X9, c, X9; \
+        VPADDD const(CX), a, a; \
+        VPTERNLOGD $0x6c, b, d, X9; \
+        VPADDD  a, X8, a; \
+        MOVD (index*4)(SI), X8; \
+	VPADDD a, X9, a; \
+	VPROLD $shift, a, a; \
+	MOVD c, X9; \
+	VPADDD a, b, a
+
+	ROUND11(X0,X1,X2,X3, 1,0, 7);
+	ROUND11(X3,X0,X1,X2, 2,4,12);
+	ROUND11(X2,X3,X0,X1, 3,8,17);
+	ROUND11(X1,X2,X3,X0, 4,12,22);
+	ROUND11(X0,X1,X2,X3, 5,16, 7);
+	ROUND11(X3,X0,X1,X2, 6,20,12);
+	ROUND11(X2,X3,X0,X1, 7,24,17);
+	ROUND11(X1,X2,X3,X0, 8,28,22);
+	ROUND11(X0,X1,X2,X3, 9,32, 7);
+	ROUND11(X3,X0,X1,X2,10,36,12);
+	ROUND11(X2,X3,X0,X1,11,40,17);
+	ROUND11(X1,X2,X3,X0,12,44,22);
+	ROUND11(X0,X1,X2,X3,13,48, 7);
+	ROUND11(X3,X0,X1,X2,14,52,12);
+	ROUND11(X2,X3,X0,X1,15,56,17);
+	ROUND11(X1,X2,X3,X0, 0,60,22);
+
+	MOVD	(1*4)(SI),	X8
+	MOVD	X3,		X9
+	MOVD	X3,		X10
+        MOVD    $0xffffffff,      BX
+        MOVD    BX, X4
+
+#define ROUND21(a, b, c, d, index, const, shift) \
+        VPANDN X4, X9, X9; \
+        VPADDD  const(CX), a, a; \  
+        VPANDD    X9, c,   X9; \
+        VPADDD    a, X8, a; \
+        MOVD    d,   X10; \
+        VPTERNLOGD $0xec, b, X9, X10; \
+        VPADDD  a,  X10, a; \
+	MOVD	(index*4)(SI),X8; \
+	MOVD	c,		X9; \
+	MOVD	c,		X10; \
+	VPROLD	$shift,	a, a; \
+	VPADDD	a, b,		a
+
+	ROUND21(X0,X1,X2,X3, 6,64, 5);
+        ROUND21(X3,X0,X1,X2,11,68, 9);
+        ROUND21(X2,X3,X0,X1, 0,72,14);
+        ROUND21(X1,X2,X3,X0, 5,76,20);
+        ROUND21(X0,X1,X2,X3,10,80, 5);
+        ROUND21(X3,X0,X1,X2,15,84, 9);
+        ROUND21(X2,X3,X0,X1, 4,88,14);
+        ROUND21(X1,X2,X3,X0, 9,92,20);
+        ROUND21(X0,X1,X2,X3,14,96, 5);
+        ROUND21(X3,X0,X1,X2, 3,100, 9);
+        ROUND21(X2,X3,X0,X1, 8,104,14);
+        ROUND21(X1,X2,X3,X0,13,108,20);
+        ROUND21(X0,X1,X2,X3, 2,112, 5);
+        ROUND21(X3,X0,X1,X2, 7,116, 9);
+        ROUND21(X2,X3,X0,X1,12,120,14);
+        ROUND21(X1,X2,X3,X0, 0,124,20);
+
+	MOVD	(5*4)(SI),	X8
+	MOVD	X2,		X9
+
+#define ROUND31(a, b, c, d, index, const, shift) \
+        VPADDD  const(CX),a , a; \
+        VPADDD    a,X8, a; \
+        VPTERNLOGD $0x96, b,d, X9; \
+	MOVD    (index*4)(SI), X8; \
+	VPADDD	a, X9,		a; \
+	VPROLD	$shift,	a,	a; \
+	MOVD	b,		X9; \
+	VPADDD	a, b,		a
+
+	ROUND31(X0,X1,X2,X3, 8,128, 4);
+	ROUND31(X3,X0,X1,X2,11,132,11);
+	ROUND31(X2,X3,X0,X1,14,136,16);
+	ROUND31(X1,X2,X3,X0, 1,140,23);
+	ROUND31(X0,X1,X2,X3, 4,144, 4);
+	ROUND31(X3,X0,X1,X2, 7,148,11);
+	ROUND31(X2,X3,X0,X1,10,152,16);
+	ROUND31(X1,X2,X3,X0,13,156,23);
+	ROUND31(X0,X1,X2,X3, 0,160, 4);
+	ROUND31(X3,X0,X1,X2, 3,164,11);
+	ROUND31(X2,X3,X0,X1, 6,168,16);
+	ROUND31(X1,X2,X3,X0, 9,172,23);
+	ROUND31(X0,X1,X2,X3,12,176, 4);
+	ROUND31(X3,X0,X1,X2,15,180,11);
+	ROUND31(X2,X3,X0,X1, 2,184,16);
+	ROUND31(X1,X2,X3,X0, 0,188,23);
+
+	MOVD	(0*4)(SI),	X8
+        MOVD    BX, X9
+        VPXORD    X3, X9, X9
+
+#define ROUND41(a, b, c, d, index, const, shift) \
+        VPADDD    const(CX), a, a; \
+        VPADDD    a, X8, a; \
+        VPTERNLOGD $0x36, b, c, X9; \
+	VPADDD	X9,a,		a; \
+	MOVD	(index*4)(SI),X8; \
+	VPROLD	$shift,a,	a	; \
+	VPXORD	X4, c, 		X9; \
+	VPADDD	a, b,		a
+
+	ROUND41(X0,X1,X2,X3, 7,192, 6);
+	ROUND41(X3,X0,X1,X2,14,196,10);
+	ROUND41(X2,X3,X0,X1, 5,200,15);
+	ROUND41(X1,X2,X3,X0,12,204,21);
+	ROUND41(X0,X1,X2,X3, 3,208, 6);
+	ROUND41(X3,X0,X1,X2,10,212,10);
+	ROUND41(X2,X3,X0,X1, 1,216,15);
+	ROUND41(X1,X2,X3,X0, 8,220,21);
+	ROUND41(X0,X1,X2,X3,15,224, 6);
+	ROUND41(X3,X0,X1,X2, 6,228,10);
+	ROUND41(X2,X3,X0,X1,13,232,15);
+	ROUND41(X1,X2,X3,X0, 4,236,21);
+	ROUND41(X0,X1,X2,X3,11,240, 6);
+	ROUND41(X3,X0,X1,X2, 2,244,10);
+	ROUND41(X2,X3,X0,X1, 9,248,15);
+	ROUND41(X1,X2,X3,X0, 0,252,21);
+
+	VPADDD	X0, X12,	X0
+	VPADDD	X1, X13,	X1
+	VPADDD	X2, X14,	X2
+	VPADDD	X3, X15,	X3
+
+	ADDQ	$64,		SI
+	CMPQ	SI,		DI
+	JB	loop1
+
+end1:
+	MOVL	X0,		(0*4)(BP)
+	MOVL	X1,		(1*4)(BP)
+	MOVL	X2,		(2*4)(BP)
+	MOVL	X3,		(3*4)(BP)
+
+	RET
+
+
+// Round specific constants
+DATA K256<>+0x00(SB)/4, $0xd76aa478 // k1
+DATA K256<>+0x04(SB)/4, $0xe8c7b756 // k2
+DATA K256<>+0x08(SB)/4, $0x242070db // k3
+DATA K256<>+0x0c(SB)/4, $0xc1bdceee // k4
+DATA K256<>+0x10(SB)/4, $0xf57c0faf // k1
+DATA K256<>+0x14(SB)/4, $0x4787c62a // k2
+DATA K256<>+0x18(SB)/4, $0xa8304613 // k3
+DATA K256<>+0x1c(SB)/4, $0xfd469501 // k4
+
+DATA K256<>+0x20(SB)/4, $0x698098d8 // k5 - k8
+DATA K256<>+0x24(SB)/4, $0x8b44f7af
+DATA K256<>+0x28(SB)/4, $0xffff5bb1
+DATA K256<>+0x2c(SB)/4, $0x895cd7be
+DATA K256<>+0x30(SB)/4, $0x6b901122
+DATA K256<>+0x34(SB)/4, $0xfd987193
+DATA K256<>+0x38(SB)/4, $0xa679438e
+DATA K256<>+0x3c(SB)/4, $0x49b40821
+
+
+DATA K256<>+0x40(SB)/4, $0xf61e2562 // k1
+DATA K256<>+0x44(SB)/4, $0xc040b340 // k2
+DATA K256<>+0x48(SB)/4, $0x265e5a51 // k3
+DATA K256<>+0x4c(SB)/4, $0xe9b6c7aa // k4
+DATA K256<>+0x50(SB)/4, $0xd62f105d // k1
+DATA K256<>+0x54(SB)/4, $0x02441453 // k2
+DATA K256<>+0x58(SB)/4, $0xd8a1e681 // k3
+DATA K256<>+0x5c(SB)/4, $0xe7d3fbc8 // k4
+
+DATA K256<>+0x60(SB)/4, $0x21e1cde6 // k5 - k8
+DATA K256<>+0x64(SB)/4, $0xc33707d6
+DATA K256<>+0x68(SB)/4, $0xf4d50d87
+DATA K256<>+0x6c(SB)/4, $0x455a14ed
+DATA K256<>+0x70(SB)/4, $0xa9e3e905
+DATA K256<>+0x74(SB)/4, $0xfcefa3f8
+DATA K256<>+0x78(SB)/4, $0x676f02d9
+DATA K256<>+0x7c(SB)/4, $0x8d2a4c8a
+
+DATA K256<>+0x80(SB)/4, $0xfffa3942 // k1
+DATA K256<>+0x84(SB)/4, $0x8771f681 // k2
+DATA K256<>+0x88(SB)/4, $0x6d9d6122 // k3
+DATA K256<>+0x8c(SB)/4, $0xfde5380c // k4
+DATA K256<>+0x90(SB)/4, $0xa4beea44 // k1
+DATA K256<>+0x94(SB)/4, $0x4bdecfa9 // k2
+DATA K256<>+0x98(SB)/4, $0xf6bb4b60 // k3
+DATA K256<>+0x9c(SB)/4, $0xbebfbc70 // k4
+
+DATA K256<>+0xa0(SB)/4, $0x289b7ec6 // k5 - k8
+DATA K256<>+0xa4(SB)/4, $0xeaa127fa
+DATA K256<>+0xa8(SB)/4, $0xd4ef3085
+DATA K256<>+0xac(SB)/4, $0x04881d05
+DATA K256<>+0xb0(SB)/4, $0xd9d4d039
+DATA K256<>+0xb4(SB)/4, $0xe6db99e5
+DATA K256<>+0xb8(SB)/4, $0x1fa27cf8
+DATA K256<>+0xbc(SB)/4, $0xc4ac5665
+
+DATA K256<>+0xc0(SB)/4, $0xf4292244 // k1
+DATA K256<>+0xc4(SB)/4, $0x432aff97 // k2
+DATA K256<>+0xc8(SB)/4, $0xab9423a7 // k3
+DATA K256<>+0xcc(SB)/4, $0xfc93a039 // k4
+DATA K256<>+0xd0(SB)/4, $0x655b59c3 // k1
+DATA K256<>+0xd4(SB)/4, $0x8f0ccc92 // k2
+DATA K256<>+0xd8(SB)/4, $0xffeff47d // k3
+DATA K256<>+0xdc(SB)/4, $0x85845dd1 // k4
+
+DATA K256<>+0xe0(SB)/4, $0x6fa87e4f // k5 - k8
+DATA K256<>+0xe4(SB)/4, $0xfe2ce6e0
+DATA K256<>+0xe8(SB)/4, $0xa3014314
+DATA K256<>+0xec(SB)/4, $0x4e0811a1
+DATA K256<>+0xf0(SB)/4, $0xf7537e82
+DATA K256<>+0xf4(SB)/4, $0xbd3af235
+DATA K256<>+0xf8(SB)/4, $0x2ad7d2bb
+DATA K256<>+0xfc(SB)/4, $0xeb86d391
+
+
+
+GLOBL K256<>(SB), (NOPTR + RODATA), $256
+
diff --git a/src/internal/cpu/cpu.go b/src/internal/cpu/cpu.go
index 1352810f42..4687e54b9b 100644
--- a/src/internal/cpu/cpu.go
+++ b/src/internal/cpu/cpu.go
@@ -24,25 +24,30 @@ var CacheLineSize uintptr = CacheLinePadSize
 // in addition to the cpuid feature bit being set.
 // The struct is padded to avoid false sharing.
 var X86 struct {
-	_            CacheLinePad
-	HasAES       bool
-	HasADX       bool
-	HasAVX       bool
-	HasAVX2      bool
-	HasBMI1      bool
-	HasBMI2      bool
-	HasERMS      bool
-	HasFMA       bool
-	HasOSXSAVE   bool
-	HasPCLMULQDQ bool
-	HasPOPCNT    bool
-	HasRDTSCP    bool
-	HasSHA       bool
-	HasSSE3      bool
-	HasSSSE3     bool
-	HasSSE41     bool
-	HasSSE42     bool
-	_            CacheLinePad
+	_             CacheLinePad
+	HasAES        bool
+	HasADX        bool
+	HasAVX        bool
+	HasAVX2       bool
+	HasAVX512     bool
+	HasAVX512F    bool
+	HasAVX512VL   bool
+	HasVAES       bool
+	HasVPCLMULQDQ bool
+	HasBMI1       bool
+	HasBMI2       bool
+	HasERMS       bool
+	HasFMA        bool
+	HasOSXSAVE    bool
+	HasPCLMULQDQ  bool
+	HasPOPCNT     bool
+	HasRDTSCP     bool
+	HasSHA        bool
+	HasSSE3       bool
+	HasSSSE3      bool
+	HasSSE41      bool
+	HasSSE42      bool
+	_             CacheLinePad
 }
 
 // The booleans in ARM contain the correspondingly named cpu feature bit.
diff --git a/src/internal/cpu/cpu_x86.go b/src/internal/cpu/cpu_x86.go
index 96b8ef92b5..e97ec0ffa7 100644
--- a/src/internal/cpu/cpu_x86.go
+++ b/src/internal/cpu/cpu_x86.go
@@ -33,13 +33,19 @@ const (
 	cpuid_OSXSAVE   = 1 << 27
 	cpuid_AVX       = 1 << 28
 
+	// ecx bits, eax = 0x7
+	cpuid_VAES       = 1 << 9
+	cpuid_VPCLMULQDQ = 1 << 10
+
 	// ebx bits
-	cpuid_BMI1 = 1 << 3
-	cpuid_AVX2 = 1 << 5
-	cpuid_BMI2 = 1 << 8
-	cpuid_ERMS = 1 << 9
-	cpuid_ADX  = 1 << 19
-	cpuid_SHA  = 1 << 29
+	cpuid_BMI1     = 1 << 3
+	cpuid_AVX2     = 1 << 5
+	cpuid_BMI2     = 1 << 8
+	cpuid_ERMS     = 1 << 9
+	cpuid_ADX      = 1 << 19
+	cpuid_SHA      = 1 << 29
+	cpuid_AVX512F  = 1 << 16
+	cpuid_AVX512VL = 1 << 31
 
 	// edx bits for CPUID 0x80000001
 	cpuid_RDTSCP = 1 << 27
@@ -73,6 +79,9 @@ func doinit() {
 		options = append(options,
 			option{Name: "avx", Feature: &X86.HasAVX},
 			option{Name: "avx2", Feature: &X86.HasAVX2},
+			option{Name: "avx512", Feature: &X86.HasAVX512},
+			option{Name: "avx512f", Feature: &X86.HasAVX512F},
+			option{Name: "avx512vl", Feature: &X86.HasAVX512VL},
 			option{Name: "bmi1", Feature: &X86.HasBMI1},
 			option{Name: "bmi2", Feature: &X86.HasBMI2},
 			option{Name: "fma", Feature: &X86.HasFMA})
@@ -108,11 +117,13 @@ func doinit() {
 	X86.HasFMA = isSet(ecx1, cpuid_FMA) && X86.HasOSXSAVE
 
 	osSupportsAVX := false
+	osSupportsAVX512 := false
 	// For XGETBV, OSXSAVE bit is required and sufficient.
 	if X86.HasOSXSAVE {
 		eax, _ := xgetbv()
 		// Check if XMM and YMM registers have OS support.
 		osSupportsAVX = isSet(eax, 1<<1) && isSet(eax, 1<<2)
+		osSupportsAVX512 = osSupportsAVX && isSet(eax, 1<<5) && isSet(eax, 1<<6) && isSet(eax, 1<<7)
 	}
 
 	X86.HasAVX = isSet(ecx1, cpuid_AVX) && osSupportsAVX
@@ -121,7 +132,7 @@ func doinit() {
 		return
 	}
 
-	_, ebx7, _, _ := cpuid(7, 0)
+	_, ebx7, ecx7, _ := cpuid(7, 0)
 	X86.HasBMI1 = isSet(ebx7, cpuid_BMI1)
 	X86.HasAVX2 = isSet(ebx7, cpuid_AVX2) && osSupportsAVX
 	X86.HasBMI2 = isSet(ebx7, cpuid_BMI2)
@@ -129,6 +140,14 @@ func doinit() {
 	X86.HasADX = isSet(ebx7, cpuid_ADX)
 	X86.HasSHA = isSet(ebx7, cpuid_SHA)
 
+	X86.HasAVX512 = isSet(ebx7, cpuid_AVX512F) && osSupportsAVX512
+	if X86.HasAVX512 {
+		X86.HasAVX512F = true
+		X86.HasAVX512VL = isSet(ebx7, cpuid_AVX512VL)
+		X86.HasVAES = isSet(ecx7, cpuid_VAES)
+		X86.HasVPCLMULQDQ = isSet(ecx7, cpuid_VPCLMULQDQ)
+	}
+
 	var maxExtendedInformation uint32
 	maxExtendedInformation, _, _, _ = cpuid(0x80000000, 0)
 
-- 
2.34.1

