From c5242bd8589ce6a344f86c3a2b3694b6c34da9d3 Mon Sep 17 00:00:00 2001
From: ted <ted.painter@intel.com>
Date: Wed, 14 Jul 2021 14:03:01 -0400
Subject: [PATCH] crypto/aes: Implement new and improved AES-GCM ciphers
 optimized with AVX-512 VAES and VPCLMULQDQ

New features:
  New vectorized AES-GCM encrypt and decrypt functions are implemented in gcmv_amd64.s using the
  AVX-512 instructions VAES and VPCLMULQDQ. The existing client API is maintained.
  This new CL includes two improvements vs. the earlier contribution CL https://go-review.googlesource.com/c/go/+/286852:
    1. Performance is significantly improved on all GCM benchmarks, i.e., increased MB/s throughput for go test -bench=GCM
    2. Fallback to scalar AES-NI has been added for non-vaes host systems

References:
  1. https://www.tomshardware.com/news/intel-10nm-xeon-ice-lake-sp-sunny-cove-core-architecture
  2. https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-instruction-set-reference-manual-325383.pdf

Addresses #42726

Change-Id: I4b25082f54cff3cb4a6c922728fb9514651bba81
---
 src/crypto/aes/aes_gcm.go    |  427 ++++-
 src/crypto/aes/aes_test.go   |    2 +-
 src/crypto/aes/cipher.go     |    3 +-
 src/crypto/aes/cipher_asm.go |    2 +-
 src/crypto/aes/gcmv_amd64.h  |  105 ++
 src/crypto/aes/gcmv_amd64.s  | 3141 ++++++++++++++++++++++++++++++++++
 6 files changed, 3658 insertions(+), 22 deletions(-)
 create mode 100644 src/crypto/aes/gcmv_amd64.h
 create mode 100644 src/crypto/aes/gcmv_amd64.s

diff --git a/src/crypto/aes/aes_gcm.go b/src/crypto/aes/aes_gcm.go
index 036705feca..737f407d22 100644
--- a/src/crypto/aes/aes_gcm.go
+++ b/src/crypto/aes/aes_gcm.go
@@ -11,9 +11,20 @@ import (
 	"crypto/internal/alias"
 	"crypto/subtle"
 	"errors"
+	"unsafe"
+
+	"golang.org/x/sys/cpu"
 )
 
+// aes_gcm.go contains aes-gcm encryption/decryption functions
+// that export two asm-optimized implementations:
+// 1. scalar aes-ni
+// 2. vector vaes
+// NewGCM() detects cpu instruction set support
+// and selects the best version for the host cpu
+
 // The following functions are defined in gcm_*.s.
+// for the aes-ni scalar implementation
 
 //go:noescape
 func gcmAesInit(productTable *[256]byte, ks []uint32)
@@ -30,44 +41,310 @@ func gcmAesDec(productTable *[256]byte, dst, src []byte, ctr, T *[16]byte, ks []
 //go:noescape
 func gcmAesFinish(productTable *[256]byte, tagMask, T *[16]byte, pLen, dLen uint64)
 
+// The following functions are defined in gcmv_amd64.s.
+// for the vaes vector implementation
+
+// key expansion and GHASH table pre-computation, 128/192/256-bit
+//
+//go:noescape
+func aesKeyExp128_avx2(key, keyData []byte)
+func aesKeyExp192_avx2(key, keyData []byte)
+func aesKeyExp256_avx2(key, keyData []byte)
+func gcmAesGhashPrecomp128_vaes(keyData []byte)
+func gcmAesGhashPrecomp192_vaes(keyData []byte)
+func gcmAesGhashPrecomp256_vaes(keyData []byte)
+
+// encrypt/decrypt single call api, 128/192/256-bit
+//
+//go:noescape
+func gcmAesEnc128_vaes(keyData []byte, ctx *gcmContext, out, in, iv, aad, authTag []byte)
+func gcmAesEnc192_vaes(keyData []byte, ctx *gcmContext, out, in, iv, aad, authTag []byte)
+func gcmAesEnc256_vaes(keyData []byte, ctx *gcmContext, out, in, iv, aad, authTag []byte)
+func gcmAesDec128_vaes(keyData []byte, ctx *gcmContext, out, in, iv, aad, authTag []byte)
+func gcmAesDec192_vaes(keyData []byte, ctx *gcmContext, out, in, iv, aad, authTag []byte)
+func gcmAesDec256_vaes(keyData []byte, ctx *gcmContext, out, in, iv, aad, authTag []byte)
+
+// encrypt/decrypt multi call api, 128/192/256-bit
+//
+//go:noescape
+func gcmAesInitVarIv_vaes(keyData []byte, ctx *gcmContext, iv, aad []byte)
+func gcmAesEncUpdate128_vaes(keyData []byte, ctx *gcmContext, out, in []byte)
+func gcmAesEncUpdate192_vaes(keyData []byte, ctx *gcmContext, out, in []byte)
+func gcmAesEncUpdate256_vaes(keyData []byte, ctx *gcmContext, out, in []byte)
+func gcmAesDecUpdate128_vaes(keyData []byte, ctx *gcmContext, out, in []byte)
+func gcmAesDecUpdate192_vaes(keyData []byte, ctx *gcmContext, out, in []byte)
+func gcmAesDecUpdate256_vaes(keyData []byte, ctx *gcmContext, out, in []byte)
+func gcmAesFinish128_vaes(keyData []byte, ctx *gcmContext, authTag []byte)
+func gcmAesFinish192_vaes(keyData []byte, ctx *gcmContext, authTag []byte)
+func gcmAesFinish256_vaes(keyData []byte, ctx *gcmContext, authTag []byte)
+
 const (
+	align                = 64 // buffer alignment, in bytes, requried for AVX-512 buffers
 	gcmBlockSize         = 16
 	gcmTagSize           = 16
 	gcmMinimumTagSize    = 12 // NIST SP 800-38D recommends tags with 12 or more bytes.
 	gcmStandardNonceSize = 12
 )
 
+// vaes AVX-512 constants for which 64-byte alignment is required
+var shufMaskStr = "0F0E0D0C0B0A09080706050403020100" +
+	"0F0E0D0C0B0A09080706050403020100" +
+	"0F0E0D0C0B0A09080706050403020100" +
+	"0F0E0D0C0B0A09080706050403020100"
+
+var ddqAddBE4444Str = "00000000000000000000000000000004" +
+	"00000000000000000000000000000004" +
+	"00000000000000000000000000000004" +
+	"00000000000000000000000000000004"
+
+var ddqAddBE1234Str = "00000000000000000000000000000001" +
+	"00000000000000000000000000000002" +
+	"00000000000000000000000000000003" +
+	"00000000000000000000000000000004"
+
+var byte64LenToMaskTableStr = "0000000000000000" + "0100000000000000" +
+	"0300000000000000" + "0700000000000000" +
+	"0f00000000000000" + "1f00000000000000" +
+	"3f00000000000000" + "7f00000000000000" +
+	"ff00000000000000" + "ff01000000000000" +
+	"ff03000000000000" + "ff07000000000000" +
+	"ff0f000000000000" + "ff1f000000000000" +
+	"ff3f000000000000" + "ff7f000000000000" +
+	"ffff000000000000" + "ffff010000000000" +
+	"ffff030000000000" + "ffff070000000000" +
+	"ffff0f0000000000" + "ffff1f0000000000" +
+	"ffff3f0000000000" + "ffff7f0000000000" +
+	"ffffff0000000000" + "ffffff0100000000" +
+	"ffffff0300000000" + "ffffff0700000000" +
+	"ffffff0f00000000" + "ffffff1f00000000" +
+	"ffffff3f00000000" + "ffffff7f00000000" +
+	"ffffffff00000000" + "ffffffff01000000" +
+	"ffffffff03000000" + "ffffffff07000000" +
+	"ffffffff0f000000" + "ffffffff1f000000" +
+	"ffffffff3f000000" + "ffffffff7f000000" +
+	"ffffffffff000000" + "ffffffffff010000" +
+	"ffffffffff030000" + "ffffffffff070000" +
+	"ffffffffff0f0000" + "ffffffffff1f0000" +
+	"ffffffffff3f0000" + "ffffffffff7f0000" +
+	"ffffffffffff0000" + "ffffffffffff0100" +
+	"ffffffffffff0300" + "ffffffffffff0700" +
+	"ffffffffffff0f00" + "ffffffffffff1f00" +
+	"ffffffffffff3f00" + "ffffffffffff7f00" +
+	"ffffffffffffff00" + "ffffffffffffff01" +
+	"ffffffffffffff03" + "ffffffffffffff07" +
+	"ffffffffffffff0f" + "ffffffffffffff1f" +
+	"ffffffffffffff3f" + "ffffffffffffff7f" +
+	"ffffffffffffffff"
+
+// vaes encrypt/decrypt function pointer for either single or multi-call
+type gcmEncDecFunc func([]byte, *gcmContext, []byte, []byte, []byte, []byte, []byte)
+
+type gcmAsmVaes struct {
+	// keyData contains the key schedule and binary-field product table
+	keyData []byte
+	// ctx contains cipher state variables and 64-byte aligned constants for AVX-512 SIMD
+	ctx gcmContext
+	// nonceSize contains the expected size of the nonce, in bytes.
+	nonceSize int
+	// tag contains the authenication tag
+	// per NIST 800-38D: 128, 120, 112, 104, or 96, 64, or 32 bits
+	tag []byte
+	// tagSize contains the size of the tag, in bytes.
+	tagSize int
+	// encrypt/decrypt point to cipher implementations that match key and nonce lengths
+	encrypt gcmEncDecFunc
+	decrypt gcmEncDecFunc
+}
+
+type gcmContext struct {
+	aad_hash              [gcmBlockSize]byte
+	aad_length            uint64
+	in_length             uint64
+	partial_block_enc_key [gcmBlockSize]byte
+	orig_IV               [gcmBlockSize]byte
+	current_counter       [gcmBlockSize]byte
+	partial_block_length  uint64
+	// below are fields needed to compensate for lack of
+	// 64-byte alignment primitives in Go asm db tables;
+	// each constant is brute-force algined to 64-bytes
+	// at the Go level to support the underlying asm function
+	ddqAddBE4444   *byte
+	ddqAddBE1234   *byte
+	shuffleMask    *byte
+	byte64Len2Mask *byte
+}
+
+// vaes encrypt, multi-call (variable-length iv), 128-bit key
+func gcmAesEncVarIv128_vaes(keyData []byte, ctx *gcmContext, out, plaintext, nonce, data []byte, tag []byte) {
+	gcmAesInitVarIv_vaes(keyData, ctx, nonce, data)
+	gcmAesEncUpdate128_vaes(keyData, ctx, out, plaintext)
+	gcmAesFinish128_vaes(keyData, ctx, tag)
+}
+
+// vaes encrypt, multi-call (variable-length iv), 192-bit key
+func gcmAesEncVarIv192_vaes(keyData []byte, ctx *gcmContext, out, plaintext, nonce, data []byte, tag []byte) {
+	gcmAesInitVarIv_vaes(keyData, ctx, nonce, data)
+	gcmAesEncUpdate192_vaes(keyData, ctx, out, plaintext)
+	gcmAesFinish192_vaes(keyData, ctx, tag)
+}
+
+// vaes encrypt, multi-call (variable-length iv), 256-bit key
+func gcmAesEncVarIv256_vaes(keyData []byte, ctx *gcmContext, out, plaintext, nonce, data []byte, tag []byte) {
+	gcmAesInitVarIv_vaes(keyData, ctx, nonce, data)
+	gcmAesEncUpdate256_vaes(keyData, ctx, out, plaintext)
+	gcmAesFinish256_vaes(keyData, ctx, tag)
+}
+
+// vaes decrypt, multi-call (variable-length iv), 128-bit key
+func gcmAesDecVarIv128_vaes(keyData []byte, ctx *gcmContext, out, plaintext, nonce, data []byte, tag []byte) {
+	gcmAesInitVarIv_vaes(keyData, ctx, nonce, data)
+	gcmAesDecUpdate128_vaes(keyData, ctx, out, plaintext)
+	gcmAesFinish128_vaes(keyData, ctx, tag)
+}
+
+// vaes decrypt, multi-call (variable-length iv), 192-bit key
+func gcmAesDecVarIv192_vaes(keyData []byte, ctx *gcmContext, out, plaintext, nonce, data []byte, tag []byte) {
+	gcmAesInitVarIv_vaes(keyData, ctx, nonce, data)
+	gcmAesDecUpdate192_vaes(keyData, ctx, out, plaintext)
+	gcmAesFinish192_vaes(keyData, ctx, tag)
+}
+
+// vaes decrypt, multi-call (variable-length iv), 256-bit key
+func gcmAesDecVarIv256_vaes(keyData []byte, ctx *gcmContext, out, plaintext, nonce, data []byte, tag []byte) {
+	gcmAesInitVarIv_vaes(keyData, ctx, nonce, data)
+	gcmAesDecUpdate256_vaes(keyData, ctx, out, plaintext)
+	gcmAesFinish256_vaes(keyData, ctx, tag)
+}
+
+// vaes key expansion and GHASH table init, 128-bit
+func gcmAesInit128_vaes(key []byte, key_data []byte) {
+	aesKeyExp128_avx2(key, key_data)
+	gcmAesGhashPrecomp128_vaes(key_data)
+}
+
+// vaes key expansion and GHASH table init, 192-bit
+func gcmAesInit192_vaes(key []byte, key_data []byte) {
+	aesKeyExp192_avx2(key, key_data)
+	gcmAesGhashPrecomp192_vaes(key_data)
+}
+
+// vaes key expansion and GHASH table init, 256-bit
+func gcmAesInit256_vaes(key []byte, key_data []byte) {
+	aesKeyExp256_avx2(key, key_data)
+	gcmAesGhashPrecomp256_vaes(key_data)
+}
+
+// get buffer pointer offset to force alignment on align boundary
+func getAlignmentOffset(ptr uintptr, align uint) uint {
+	alignment := uintptr(align)
+	unaligned := (ptr & (alignment - uintptr(1))) != 0
+	var offset uintptr
+	if unaligned {
+		offset = alignment - ptr%uintptr(alignment)
+	}
+	return uint(offset)
+}
+
+// initialize an aligned buffer with string contents;
+// used for avx-512 gcm constants
+func initVectAlign(srcData string, dstVect **byte) []byte {
+	bufLen := len(srcData) / 2
+	buf := make([]byte, bufLen+align)
+	ptr := uintptr(unsafe.Pointer(&buf[0]))
+	offset := getAlignmentOffset(ptr, align)
+	if nil != dstVect {
+		*dstVect = &(buf[offset])
+	}
+	dataBytes := decodeString(srcData)
+	for i := 0; i < bufLen; i++ {
+		buf[int(offset)+i] = dataBytes[i]
+	}
+	return (buf[offset:])
+}
+
+// allocate an aligned slice
+func allocVectAlign(numBytes int64) []byte {
+	buf := make([]byte, numBytes+align)
+	ptr := uintptr(unsafe.Pointer(&buf[0]))
+	offset := getAlignmentOffset(ptr, align)
+	return (buf[offset:])
+}
+
+// vaes gcmInit
+func gcmInit(key []byte, nonceSize, tagSize int) (keyData, tag []byte, ctx gcmContext, encryptFunc, decryptFunc gcmEncDecFunc) {
+
+	// alloc key expansion buffer
+	keyData = allocVectAlign(1024)
+
+	// alloc authentication tag buffer
+	tag = make([]byte, tagSize)
+
+	// init context with gcm constants
+	initVectAlign(shufMaskStr, &(ctx.shuffleMask))
+	initVectAlign(byte64LenToMaskTableStr, &(ctx.byte64Len2Mask))
+	initVectAlign(ddqAddBE4444Str, &(ctx.ddqAddBE4444))
+	initVectAlign(ddqAddBE1234Str, &(ctx.ddqAddBE1234))
+
+	// compute key expansions and init gcm function pointers given iv and key lengths
+	switch len(key) {
+	case 16:
+		gcmAesInit128_vaes(key, keyData)
+		if gcmStandardNonceSize == nonceSize {
+			encryptFunc = gcmAesEnc128_vaes
+			decryptFunc = gcmAesDec128_vaes
+		} else {
+			encryptFunc = gcmAesEncVarIv128_vaes
+			decryptFunc = gcmAesDecVarIv128_vaes
+		}
+	case 24:
+		gcmAesInit192_vaes(key, keyData)
+		if gcmStandardNonceSize == nonceSize {
+			encryptFunc = gcmAesEnc192_vaes
+			decryptFunc = gcmAesDec192_vaes
+		} else {
+			encryptFunc = gcmAesEncVarIv192_vaes
+			decryptFunc = gcmAesDecVarIv192_vaes
+		}
+	case 32:
+		gcmAesInit256_vaes(key, keyData)
+		if gcmStandardNonceSize == nonceSize {
+			encryptFunc = gcmAesEnc256_vaes
+			decryptFunc = gcmAesDec256_vaes
+		} else {
+			encryptFunc = gcmAesEncVarIv256_vaes
+			decryptFunc = gcmAesDecVarIv256_vaes
+		}
+	}
+	return keyData, tag, ctx, encryptFunc, decryptFunc
+}
+
 var errOpen = errors.New("cipher: message authentication failed")
 
 // Assert that aesCipherGCM implements the gcmAble interface.
 var _ gcmAble = (*aesCipherGCM)(nil)
 
+var supportsAVX512VAES = cpu.X86.HasAVX512 && cpu.X86.HasAVX512VAES && cpu.X86.HasAVX512VPCLMULQDQ
+
 // NewGCM returns the AES cipher wrapped in Galois Counter Mode. This is only
 // called by [crypto/cipher.NewGCM] via the gcmAble interface.
 func (c *aesCipherGCM) NewGCM(nonceSize, tagSize int) (cipher.AEAD, error) {
-	g := &gcmAsm{ks: c.enc, nonceSize: nonceSize, tagSize: tagSize}
-	gcmAesInit(&g.productTable, g.ks)
-	return g, nil
-}
-
-type gcmAsm struct {
-	// ks is the key schedule, the length of which depends on the size of
-	// the AES key.
-	ks []uint32
-	// productTable contains pre-computed multiples of the binary-field
-	// element used in GHASH.
-	productTable [256]byte
-	// nonceSize contains the expected size of the nonce, in bytes.
-	nonceSize int
-	// tagSize contains the size of the tag, in bytes.
-	tagSize int
+	if supportsAVX512VAES {
+		g := &gcmAsmVaes{nonceSize: nonceSize, tagSize: tagSize}
+		g.keyData, g.tag, g.ctx, g.encrypt, g.decrypt = gcmInit(c.key, nonceSize, tagSize)
+		return g, nil
+	} else {
+		g := &gcmAsm{ks: c.enc, nonceSize: nonceSize, tagSize: tagSize}
+		gcmAesInit(&g.productTable, g.ks)
+		return g, nil
+	}
 }
 
-func (g *gcmAsm) NonceSize() int {
+// vaes implementation
+func (g *gcmAsmVaes) NonceSize() int {
 	return g.nonceSize
 }
 
-func (g *gcmAsm) Overhead() int {
+func (g *gcmAsmVaes) Overhead() int {
 	return g.tagSize
 }
 
@@ -86,7 +363,78 @@ func sliceForAppend(in []byte, n int) (head, tail []byte) {
 	return
 }
 
-// Seal encrypts and authenticates plaintext. See the [cipher.AEAD] interface for
+// Seal encrypts and authenticates plaintext. See the cipher.AEAD interface for details.
+func (g *gcmAsmVaes) Seal(dst, nonce, plaintext, data []byte) []byte {
+	if len(nonce) != g.nonceSize {
+		panic("crypto/cipher: incorrect nonce length given to GCM")
+	}
+	if uint64(len(plaintext)) > ((1<<32)-2)*BlockSize {
+		panic("crypto/cipher: message too large for GCM")
+	}
+	ret, out := sliceForAppend(dst, len(plaintext)+g.tagSize)
+	g.encrypt(g.keyData, &(g.ctx), out, plaintext, nonce, data, g.tag)
+	if alias.InexactOverlap(out[:len(plaintext)], plaintext) {
+		panic("crypto/cipher: invalid buffer overlap")
+	}
+	copy(out[len(plaintext):], g.tag[:])
+	return ret
+}
+
+// Open authenticates and decrypts ciphertext. See the cipher.AEAD interface for details.
+func (g *gcmAsmVaes) Open(dst, nonce, ciphertext, data []byte) ([]byte, error) {
+	if len(nonce) != g.nonceSize {
+		panic("crypto/cipher: incorrect nonce length given to GCM")
+	}
+	// Sanity check to prevent the authentication from always succeeding if an implementation
+	// leaves tagSize uninitialized, for example.
+	if g.tagSize < gcmMinimumTagSize {
+		panic("crypto/cipher: incorrect GCM tag size")
+	}
+	if len(ciphertext) < g.tagSize {
+		return nil, errOpen
+	}
+	if uint64(len(ciphertext)) > ((1<<32)-2)*uint64(BlockSize)+uint64(g.tagSize) {
+		return nil, errOpen
+	}
+	tag := ciphertext[len(ciphertext)-g.tagSize:]
+	ciphertext = ciphertext[:len(ciphertext)-g.tagSize]
+	ret, out := sliceForAppend(dst, len(ciphertext))
+	if alias.InexactOverlap(out, ciphertext) {
+		panic("crypto/cipher: invalid buffer overlap")
+	}
+	g.decrypt(g.keyData, &(g.ctx), out, ciphertext, nonce, data, g.tag)
+	if subtle.ConstantTimeCompare(g.tag[:g.tagSize], tag) != 1 {
+		for i := range out {
+			out[i] = 0
+		}
+		return nil, errOpen
+	}
+	return ret, nil
+}
+
+// aes-ni implementation
+type gcmAsm struct {
+	// ks is the key schedule, the length of which depends on the size of
+	// the AES key.
+	ks []uint32
+	// productTable contains pre-computed multiples of the binary-field
+	// element used in GHASH.
+	productTable [256]byte
+	// nonceSize contains the expected size of the nonce, in bytes.
+	nonceSize int
+	// tagSize contains the size of the tag, in bytes.
+	tagSize int
+}
+
+func (g *gcmAsm) NonceSize() int {
+	return g.nonceSize
+}
+
+func (g *gcmAsm) Overhead() int {
+	return g.tagSize
+}
+
+// Seal encrypts and authenticates plaintext. See the cipher.AEAD interface for
 // details.
 func (g *gcmAsm) Seal(dst, nonce, plaintext, data []byte) []byte {
 	if len(nonce) != g.nonceSize {
@@ -184,3 +532,44 @@ func (g *gcmAsm) Open(dst, nonce, ciphertext, data []byte) ([]byte, error) {
 
 	return ret, nil
 }
+
+// gcmInit helper functions
+// required to avoid dependencies on hex string package
+// these convert string constants to byte arrays
+// future - remove helper funcs, replace ascii strings with byte arrays
+// future +1 - add support for 64-byte aligned constants to go asm
+// making mask and constant declarations unnecessary at the go level
+// replace with aligned defined byte tables in gcmv_amd64.s
+
+// fromHexChar converts a hex character into its value and a success flag.
+func fromHexChar(c byte) byte {
+	switch {
+	case '0' <= c && c <= '9':
+		return c - '0'
+	case 'a' <= c && c <= 'f':
+		return c - 'a' + 10
+	case 'A' <= c && c <= 'F':
+		return c - 'A' + 10
+	}
+	return 0
+}
+
+// Decode decodes src into DecodedLen(len(src)) bytes,
+// returning the actual number of bytes written to dst.
+func decode(dst, src []byte) int {
+	i, j := 0, 1
+	for ; j < len(src); j += 2 {
+		a := fromHexChar(src[j-1])
+		b := fromHexChar(src[j])
+		dst[i] = (a << 4) | b
+		i++
+	}
+	return i
+}
+
+// DecodeString returns the bytes represented by the hexadecimal string s.
+func decodeString(s string) []byte {
+	src := []byte(s)
+	n := decode(src, src)
+	return src[:n]
+}
diff --git a/src/crypto/aes/aes_test.go b/src/crypto/aes/aes_test.go
index 1e8bac4bb5..6d37856632 100644
--- a/src/crypto/aes/aes_test.go
+++ b/src/crypto/aes/aes_test.go
@@ -375,7 +375,7 @@ func BenchmarkDecrypt(b *testing.B) {
 func BenchmarkExpand(b *testing.B) {
 	tt := encryptTests[0]
 	n := len(tt.key) + 28
-	c := &aesCipher{make([]uint32, n), make([]uint32, n)}
+	c := &aesCipher{make([]uint32, n), make([]uint32, n), tt.key}
 	b.ResetTimer()
 	for i := 0; i < b.N; i++ {
 		expandKey(tt.key, c.enc, c.dec)
diff --git a/src/crypto/aes/cipher.go b/src/crypto/aes/cipher.go
index a9e6208696..a14205d592 100644
--- a/src/crypto/aes/cipher.go
+++ b/src/crypto/aes/cipher.go
@@ -18,6 +18,7 @@ const BlockSize = 16
 type aesCipher struct {
 	enc []uint32
 	dec []uint32
+	key []byte
 }
 
 type KeySizeError int
@@ -48,7 +49,7 @@ func NewCipher(key []byte) (cipher.Block, error) {
 // implemented in pure Go.
 func newCipherGeneric(key []byte) (cipher.Block, error) {
 	n := len(key) + 28
-	c := aesCipher{make([]uint32, n), make([]uint32, n)}
+	c := aesCipher{make([]uint32, n), make([]uint32, n), key}
 	expandKeyGo(key, c.enc, c.dec)
 	return &c, nil
 }
diff --git a/src/crypto/aes/cipher_asm.go b/src/crypto/aes/cipher_asm.go
index 90031c5e2c..f1d7326aa2 100644
--- a/src/crypto/aes/cipher_asm.go
+++ b/src/crypto/aes/cipher_asm.go
@@ -45,7 +45,7 @@ func newCipher(key []byte) (cipher.Block, error) {
 		return newCipherGeneric(key)
 	}
 	n := len(key) + 28
-	c := aesCipherAsm{aesCipher{make([]uint32, n), make([]uint32, n)}}
+	c := aesCipherAsm{aesCipher{make([]uint32, n), make([]uint32, n), key}}
 	var rounds int
 	switch len(key) {
 	case 128 / 8:
diff --git a/src/crypto/aes/gcmv_amd64.h b/src/crypto/aes/gcmv_amd64.h
new file mode 100644
index 0000000000..0edd1811a8
--- /dev/null
+++ b/src/crypto/aes/gcmv_amd64.h
@@ -0,0 +1,105 @@
+//
+// Copyright (c) 2019-2021, Intel Corporation
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//     * Redistributions of source code must retain the above copyright notice,
+//       this list of conditions and the following disclaimer.
+//     * Redistributions in binary form must reproduce the above copyright
+//       notice, this list of conditions and the following disclaimer in the
+//       documentation and/or other materials provided with the distribution.
+//     * Neither the name of Intel Corporation nor the names of its contributors
+//       may be used to endorse or promote products derived from this software
+//       without specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
+// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+
+#ifndef GCMV_AMD64_INCLUDED
+#define GCMV_AMD64_INCLUDED
+
+//
+// Key structure holds up to 48 ghash keys
+//
+#define HashKey_48      (16*15)   // HashKey^48 <<1 mod poly
+#define HashKey_47      (16*16)   // HashKey^47 <<1 mod poly
+#define HashKey_46      (16*17)   // HashKey^46 <<1 mod poly
+#define HashKey_45      (16*18)   // HashKey^45 <<1 mod poly
+#define HashKey_44      (16*19)   // HashKey^44 <<1 mod poly
+#define HashKey_43      (16*20)   // HashKey^43 <<1 mod poly
+#define HashKey_42      (16*21)   // HashKey^42 <<1 mod poly
+#define HashKey_41      (16*22)   // HashKey^41 <<1 mod poly
+#define HashKey_40      (16*23)   // HashKey^40 <<1 mod poly
+#define HashKey_39      (16*24)   // HashKey^39 <<1 mod poly
+#define HashKey_38      (16*25)   // HashKey^38 <<1 mod poly
+#define HashKey_37      (16*26)   // HashKey^37 <<1 mod poly
+#define HashKey_36      (16*27)   // HashKey^36 <<1 mod poly
+#define HashKey_35      (16*28)   // HashKey^35 <<1 mod poly
+#define HashKey_34      (16*29)   // HashKey^34 <<1 mod poly
+#define HashKey_33      (16*30)   // HashKey^33 <<1 mod poly
+#define HashKey_32      (16*31)   // HashKey^32 <<1 mod poly
+#define HashKey_31      (16*32)   // HashKey^31 <<1 mod poly
+#define HashKey_30      (16*33)   // HashKey^30 <<1 mod poly
+#define HashKey_29      (16*34)   // HashKey^29 <<1 mod poly
+#define HashKey_28      (16*35)   // HashKey^28 <<1 mod poly
+#define HashKey_27      (16*36)   // HashKey^27 <<1 mod poly
+#define HashKey_26      (16*37)   // HashKey^26 <<1 mod poly
+#define HashKey_25      (16*38)   // HashKey^25 <<1 mod poly
+#define HashKey_24      (16*39)   // HashKey^24 <<1 mod poly
+#define HashKey_23      (16*40)   // HashKey^23 <<1 mod poly
+#define HashKey_22      (16*41)   // HashKey^22 <<1 mod poly
+#define HashKey_21      (16*42)   // HashKey^21 <<1 mod poly
+#define HashKey_20      (16*43)   // HashKey^20 <<1 mod poly
+#define HashKey_19      (16*44)   // HashKey^19 <<1 mod poly
+#define HashKey_18      (16*45)   // HashKey^18 <<1 mod poly
+#define HashKey_17      (16*46)   // HashKey^17 <<1 mod poly
+#define HashKey_16      (16*47)   // HashKey^16 <<1 mod poly
+#define HashKey_15      (16*48)   // HashKey^15 <<1 mod poly
+#define HashKey_14      (16*49)   // HashKey^14 <<1 mod poly
+#define HashKey_13      (16*50)   // HashKey^13 <<1 mod poly
+#define HashKey_12      (16*51)   // HashKey^12 <<1 mod poly
+#define HashKey_11      (16*52)   // HashKey^11 <<1 mod poly
+#define HashKey_10      (16*53)   // HashKey^10 <<1 mod poly
+#define HashKey_9       (16*54)   // HashKey^9 <<1 mod poly
+#define HashKey_8       (16*55)   // HashKey^8 <<1 mod poly
+#define HashKey_7       (16*56)   // HashKey^7 <<1 mod poly
+#define HashKey_6       (16*57)   // HashKey^6 <<1 mod poly
+#define HashKey_5       (16*58)   // HashKey^5 <<1 mod poly
+#define HashKey_4       (16*59)   // HashKey^4 <<1 mod poly
+#define HashKey_3       (16*60)   // HashKey^3 <<1 mod poly
+#define HashKey_2       (16*61)   // HashKey^2 <<1 mod poly
+#define HashKey_1       (16*62)   // HashKey <<1 mod poly
+#define HashKey         (16*62)   // HashKey <<1 mod poly
+
+// define _NT_LDST to use non-temporal load/store
+//#define _NT_LDST
+#ifdef _NT_LDST
+	#define _NT_LD
+	#define _NT_ST
+#endif
+
+// enable non-temporal loads
+#ifdef _NT_LD
+	#define	VX512LDR vmovntdqa
+#else
+	#define	VX512LDR VMOVDQU8
+#endif
+
+// enable non-temporal stores
+#ifdef _NT_ST
+	%define	VX512STR vmovntdq
+#else
+	#define	VX512STR VMOVDQU8
+#endif
+
+#endif // GCMV_AMD64_INCLUDED
diff --git a/src/crypto/aes/gcmv_amd64.s b/src/crypto/aes/gcmv_amd64.s
new file mode 100644
index 0000000000..8099918108
--- /dev/null
+++ b/src/crypto/aes/gcmv_amd64.s
@@ -0,0 +1,3141 @@
+// Copyright 2021 Intel Corporation. All rights reserved.
+//
+// This is an optimized implementation of AES-GCM using AES-NI, CLMUL-NI, VAES, and VPCLMULQDQ
+// It has been ported from C/nasm to Go/asm based on the open source project
+// Intel Multi-Buffer crypto for ipsec, which can be found 
+// on GitHub here: https://github.com/intel/intel-ipsec-mb and https://github.com/intel/intel-ipsec-mb/releases
+// comments in this source referring to "ipsec" are making reference to the practices
+// and programming idioms found in the ipsec-mb project under the avx-512 vaes aes-gcm implementation
+//
+// This implementation also uses some optimizations as described in:
+// [1] Gueron, S., Kounavis, M.E.: Intel® Carry-Less Multiplication
+//     Instruction and its Usage for Computing the GCM Mode rev. 2.02
+// [2] Gueron, S., Krasnov, V.: Speeding up Counter Mode in Software and
+//     Hardware
+
+#include "textflag.h"
+#include "gcmv_amd64.h"
+
+// notes: 1) if not 64-byte aligned could seg fault avx-512 ops
+//        2) redundant with go-level shufMask forced to 64-byte alignment, replace
+DATA shufMaskU<>+0x00(SB)/8, $0x08090A0B0C0D0E0F 
+DATA shufMaskU<>+0x08(SB)/8, $0x0001020304050607
+DATA shufMaskU<>+0x10(SB)/8, $0x08090A0B0C0D0E0F 
+DATA shufMaskU<>+0x18(SB)/8, $0x0001020304050607
+DATA shufMaskU<>+0x20(SB)/8, $0x08090A0B0C0D0E0F 
+DATA shufMaskU<>+0x28(SB)/8, $0x0001020304050607
+DATA shufMaskU<>+0x30(SB)/8, $0x08090A0B0C0D0E0F 
+DATA shufMaskU<>+0x38(SB)/8, $0x0001020304050607
+GLOBL shufMaskU<>(SB), (NOPTR+RODATA), $64 
+
+// note: align 16
+DATA poly<>+0x00(SB)/8, $0x0000000000000001
+DATA poly<>+0x08(SB)/8, $0xC200000000000000
+GLOBL poly<>(SB), (NOPTR+RODATA), $16
+
+// note: align 64
+DATA poly2<>+0x00(SB)/8, $0x00000001C2000000 
+DATA poly2<>+0x08(SB)/8, $0xC200000000000000
+DATA poly2<>+0x10(SB)/8, $0x00000001C2000000 
+DATA poly2<>+0x18(SB)/8, $0xC200000000000000
+DATA poly2<>+0x20(SB)/8, $0x00000001C2000000 
+DATA poly2<>+0x28(SB)/8, $0xC200000000000000
+DATA poly2<>+0x30(SB)/8, $0x00000001C2000000 
+DATA poly2<>+0x38(SB)/8, $0xC200000000000000
+GLOBL poly2<>(SB), (NOPTR+RODATA), $64
+
+// note: align 16
+DATA twoone<>+0x00(SB)/8, $0x0000000000000001
+DATA twoone<>+0x08(SB)/8, $0x0000000100000000
+GLOBL twoone<>(SB), (NOPTR+RODATA), $16
+
+// select single DQ for insertion under K 
+DATA kMask3<>+0x00(SB)/2, $0x00C0
+GLOBL kMask3<>(SB), (NOPTR+RODATA), $2
+DATA kMask2<>+0x00(SB)/2, $0x0030
+GLOBL kMask2<>(SB), (NOPTR+RODATA), $2
+DATA kMask1<>+0x00(SB)/2, $0x000C
+GLOBL kMask1<>(SB), (NOPTR+RODATA), $2
+DATA kMask0<>+0x00(SB)/2, $0x0003
+GLOBL kMask0<>(SB), (NOPTR+RODATA), $2
+
+// mask high-order block under VPTERNLOGQ 
+DATA kMaskTopBlock<>+0x00(SB)/2, $0x003F
+GLOBL kMaskTopBlock<>(SB), (NOPTR+RODATA), $2
+
+// note: align 16
+DATA oneF<>+0x00(SB)/8, $0x0000000000000000
+DATA oneF<>+0x08(SB)/8, $0x0100000000000000
+GLOBL oneF<>(SB), (NOPTR+RODATA), $16
+
+// note: align 16
+DATA one<>+0x00(SB)/8, $0x0000000000000001
+DATA one<>+0x08(SB)/8, $0x0000000000000000
+GLOBL one<>(SB), (NOPTR+RODATA), $16
+
+//func aesKeyExp128_avx2(key *byte, keyData *byte)
+TEXT ·aesKeyExp128_avx2(SB),NOSPLIT,$0
+
+#define _key 				DI
+#define _keyData 		SI
+
+#define generateRoundKey(round,rcon) 																											    \
+  VAESKEYGENASSIST				$rcon, X1, X2																												\
+	VPSHUFD 								$0b11111111, X2, X2																									\
+	VSHUFPS									$0b00010000, X1, X3, X3																							\
+	VPXOR										X3, X1, X1																													\
+	VSHUFPS									$0b10001100, X1, X3, X3																							\
+	VPXOR										X3, X1, X1																													\
+	VPXOR										X2, X1, X1																													\
+	VMOVDQA									X1, (16*round)(_keyData)	
+
+	MOVQ 										key+0(FP), _key							// pointer to the AES key
+	MOVQ										keyData+24(FP), _keyData	  // pointer to the AES round keys
+	VMOVDQU									(16*0)(_key), X1									
+	VMOVDQA									X1, (16*0)(_keyData)				// round key 0 = AES key
+	VPXOR										X3, X3, X3
+
+	// generate round keys 1,2, ..., 10
+	generateRoundKey        (1, 0x1)
+	generateRoundKey				(2, 0x2)
+	generateRoundKey        (3, 0x4)
+	generateRoundKey				(4, 0x8)
+	generateRoundKey        (5, 0x10)
+	generateRoundKey				(6, 0x20)
+	generateRoundKey        (7, 0x40)
+	generateRoundKey				(8, 0x80)
+	generateRoundKey        (9, 0x1b)
+	generateRoundKey				(10, 0x36)
+
+#undef _key
+#undef _keyData
+RET
+
+TEXT ·aesKeyExp192_avx2(SB),NOSPLIT,$0
+
+#define _key 				DI
+#define _keyData 		SI
+
+#define generateRoundKey192p2_NOP(round) 
+
+#define generateRoundKey192p2(round) \
+  VMOVDQA                 X4, X5                                                              \
+  VPSLLDQ                 $4, X5, X5                                                          \
+  VSHUFPS                 $0b11110000, X1, X6, X6                                             \
+  VPXOR                   X5, X6, X6	                                                        \
+  VPXOR                   X6, X4, X4                                                          \
+  VPSHUFD                 $0b00001110, X4, X7                                                 \
+  VMOVDQU                 X7, (16+24*round)(_keyData)
+
+#define generateRoundKey192(round,rcon,generateRoundPart2) \
+  VAESKEYGENASSIST				$rcon, X4, X2																												\
+	VPSHUFD 								$0b11111111, X2, X2																									\
+	VSHUFPS									$0b00010000, X1, X3, X3																							\
+	VPXOR										X3, X1, X1																													\
+	VSHUFPS									$0b10001100, X1, X3, X3																							\
+	VPXOR										X3, X1, X1																													\
+	VPXOR										X2, X1, X1																													\
+	VMOVDQU									X1, (24*round)(_keyData)                                            \
+  generateRoundPart2      (round)
+
+	MOVQ 										key+0(FP), _key							          // pointer to the AES key
+	MOVQ										keyData+24(FP), _keyData			        // pointer to the AES round keys
+	VMOVQ									  (16*1)(_key), X7                      // load top 64 bits									
+	VMOVQ									  X7, (16*1)(_keyData)                  // copy to round key 0
+	VPSHUFD									$0b01001111, X7, X4 
+	VMOVDQU									(16*0)(_key), X1					            // load bottom 128 bits
+	VMOVDQU									X1, (16*0)(_keyData)					        // round key 0 = AES key
+	VPXOR										X3, X3, X3
+	VPXOR										X6, X6, X6
+
+	// generate round keys 1,2, ..., 12
+	generateRoundKey192     (1, 0x1, generateRoundKey192p2)       // round key 1, 2
+	generateRoundKey192			(2, 0x2, generateRoundKey192p2)       // round key 3, 4
+	generateRoundKey192     (3, 0x4, generateRoundKey192p2)       // round key 4, 5
+	generateRoundKey192		  (4, 0x8, generateRoundKey192p2)       // round key 6, 7
+	generateRoundKey192     (5, 0x10, generateRoundKey192p2)      // round key 7, 8
+	generateRoundKey192			(6, 0x20, generateRoundKey192p2)      // round key 9, 10
+	generateRoundKey192     (7, 0x40, generateRoundKey192p2)      // round key 10, 11
+	generateRoundKey192			(8, 0x80, generateRoundKey192p2_NOP)  // round key 12
+
+#undef _key
+#undef _keyData
+RET
+
+TEXT ·aesKeyExp256_avx2(SB),NOSPLIT,$0
+
+#define pKey 				DI
+#define pKeyData 		SI
+
+#define generateRoundKey256e(round,rcon) \
+  VAESKEYGENASSIST				$rcon, X4, X2																												\
+	VPSHUFD 								$0b11111111, X2, X2																									\
+	VSHUFPS									$0b00010000, X1, X3, X3																							\
+	VPXOR										X3, X1, X1																													\
+	VSHUFPS									$0b10001100, X1, X3, X3																							\
+	VPXOR										X3, X1, X1																													\
+	VPXOR										X2, X1, X1																													\
+	VMOVDQA									X1, (16*round)(pKeyData)                                            \
+
+#define generateRoundKey256o(round,rcon) \
+  VAESKEYGENASSIST				$rcon, X1, X2																												\
+	VPSHUFD 								$0b10101010, X2, X2																									\
+	VSHUFPS									$0b00010000, X4, X3, X3																							\
+	VPXOR										X3, X4, X4																													\
+	VSHUFPS									$0b10001100, X4, X3, X3																							\
+	VPXOR										X3, X4, X4																													\
+	VPXOR										X2, X4, X4																													\
+	VMOVDQA									X4, (16*round)(pKeyData)                                            \
+
+	MOVQ 										key+0(FP), pKey							          // pointer to the AES key
+	MOVQ										keyData+24(FP), pKeyData			        // pointer to the AES round keys
+	VMOVDQU									(16*0)(pKey), X1					            // load bottom 128 bits
+	VMOVDQA									X1, (16*0)(pKeyData)			    		    // copy to round 0 key
+	VMOVDQU									(16*1)(pKey), X4					            // load top 128 bits
+	VMOVDQA									X4, (16*1)(pKeyData)					        // copy to round 1 key
+	VPXOR										X3, X3, X3
+
+	// generate round keys 2, ..., 14
+	generateRoundKey256e     (2, 0x1)       // round key 2
+	generateRoundKey256o     (3, 0x1)       // round key 3
+	generateRoundKey256e     (4, 0x2)       // round key 4
+	generateRoundKey256o     (5, 0x2)       // round key 5
+	generateRoundKey256e     (6, 0x4)       // round key 6
+	generateRoundKey256o     (7, 0x4)       // round key 7
+	generateRoundKey256e     (8, 0x8)       // round key 8
+	generateRoundKey256o     (9, 0x8)       // round key 9
+	generateRoundKey256e     (10, 0x10)     // round key 10
+	generateRoundKey256o     (11, 0x10)     // round key 11
+	generateRoundKey256e     (12, 0x20)     // round key 12
+	generateRoundKey256o     (13, 0x20)     // round key 13
+	generateRoundKey256e     (14, 0x40)     // round key 14
+
+#undef pKey
+#undef pKeyData
+RET
+
+////////////////////////////////////////////////////////////////////////////////////////
+/// GHASH_MUL MACRO to implement: Data*HashKey mod (128,127,126,121,0)
+/// Input: A and B (128-bits each, bit-reflected)
+/// Output: C = A*B*x mod poly, (i.e. >>1 )
+/// To compute GH = GH*HashKey mod poly, give HK = HashKey<<1 mod poly as input
+/// GH = GH * HK * x mod poly which is equivalent to GH*HashKey mod poly.
+/////////////////////////////////////////////////////////////////////////////////////////
+// gh 			[in/out] xmm/ymm/zmm with multiply operand(s) (128-bits)
+// hk   		[in] xmm/ymm/zmm with hash key value(s) (128-bits)
+// t1..t5 	[clobbered] xmm/ymm/zmm
+//
+#define ghashMul(gh,hk,t1,t2,t3,t4,t5) \
+  VPCLMULQDQ 			$0x11, hk, gh, t1 			/* t1 = a1*b1 */         														\
+	VPCLMULQDQ			$0x00, hk, gh, t2				/* t2 = a0*b0 */         														\
+	VPCLMULQDQ      $0x01, hk, gh, t3       /* t3 = a1*b0 */         														\
+	VPCLMULQDQ      $0x10, hk, gh, gh       /* gh = a0*b1 */ 			   														\
+	VPXORQ					t3, gh, gh 	                                     														\	
+	VPSRLDQ					$8, gh, t3				      /* shift-R gh 2 DWs */   														\
+  VPSLLDQ 				$8, gh, gh              /* shift-L gh 2 DWs */  														\
+	VPXORQ					t3, t1, t1                                       														\
+	VPXORQ          t2, gh, gh 																		   														\ 
+	VMOVDQU64				poly2<>(SB), t3					/* first phase reduction */ 												\
+	VPCLMULQDQ      $0x01, gh, t3, t2 																													\
+	VPSLLDQ         $8, t2, t2 																																	\       
+  VPXORQ          t2, gh, gh 																																	\
+	VPCLMULQDQ  		$0x00, gh, t3, t2       /* second phase reduction */ 												\
+  VPSRLDQ					$4, t2, t2 						  /* shift-R only 1-DW to obtain 2-DWs shift-R */ 		\
+	VPCLMULQDQ 			$0x10, gh, t3, gh 																													\
+	VPSLLDQ					$4, gh, gh              /* Shift-L 1-DW to obtain result with no shifts */ 	\
+	VPTERNLOGQ 			$0x96, t2, t1, gh       /* GH = GH xor T1 xor T2 */
+
+// func gcmAesGhashPrecomp128_vaes(keyData)
+TEXT ·gcmAesGhashPrecomp128_vaes(SB),NOSPLIT,$0
+
+///////////////////////////////////////////////////////////////////////////////////////////////
+/// In PRECOMPUTE, the commands filling Hashkey_i_k are not required for avx512
+/// functions, but are kept to allow users to switch cpu architectures between calls
+/// of pre, init, update, and finalize.
+//
+// gdata     [in/out] GPR, pointer to GCM key data structure, content updated
+// hk        [in] xmm, hash key
+// t1..t8    [clobbered] xmm
+// note:     Go asm supports only masked version of VINSERTI64X2; ipsec uses unmaksed
+//           Z31 added for masked ops 
+//
+#define precompute(gdata, hk, t1, z1, t2, z2, t3, z3, t4, z4, t5, z5, t6, z6, t7, z7, t8, z8) \
+	KXNORQ 					K1, K1, K1  																																\
+  VMOVDQA64 			hk, t5 																																			\
+	VINSERTI64X2		$3, hk, Z31, K1, z7	  																											\
+	                                      																											\
+	/* calculate HashKey^2<<1 mod poly         */ 																							\
+	ghashMul        (t5,hk,t1,t3,t4,t6,t2)         																							\
+	VMOVDQA64				t5, HashKey_2(gdata)																												\
+	KMOVW 					kMask2<>(SB), K1																														\
+	VINSERTI64X2		$2, t5, Z31, K1, z7																													\
+																																															\
+	/* calculate HashKey^3<<1 mod poly         */																								\
+	ghashMul        (t5,hk,t1,t3,t4,t6,t2) 																											\
+	VMOVDQU64				t5, HashKey_3(gdata)																												\
+	KMOVW 					kMask1<>(SB), K1																														\
+	VINSERTI64X2		$1, t5, Z31, K1, z7																													\
+																																															\
+	/* calculate HashKey^4<<1 mod poly         */																								\
+	ghashMul        (t5,hk,t1,t3,t4,t6,t2) 				 																							\
+	VMOVDQU64				t5, HashKey_4(gdata)																												\
+	KMOVW 					kMask0<>(SB), K1																														\
+	VINSERTI64X2		$0, t5, Z31, K1, z7																													\
+																																															\
+	/* switch to 4x128-bit computations now    */	  																						\
+	VSHUFI64X2			$0x00, z5, z5, z5     				  /* broadcast HashKey^4 across all z5 */	    \
+	VMOVDQA64				z7, z8												  /* save HashKey^4 to HashKey^1 in z8 */	    \
+																																															\
+	/* calculate HashKey^[5..8]<<1 mod poly    */																								\
+	ghashMul        (z7,z5,z1,z3,z4,z6,z2)        																							\
+	VMOVDQU64 			z7, HashKey_8(gdata)  					/* HashKey^8 to HashKey^5 in z7 now  */     \
+	VSHUFI64X2      $0x00, z7, z7, z5						    /* broadcast HashKey^8 across all z5 */			\
+																																															\
+	/* calculate HashKey^[9...48]<<1 mod poly  */   																						\
+  /* use HashKey^8 as multiplier w/ ZT8, ZT7 */   																						\
+	/* to allow deeper ooo execution           */   																						\
+	ghashMul        (z8,z5,z1,z3,z4,z6,z2)        																							\
+	VMOVDQU64 			z8, HashKey_12(gdata) 																											\
+	ghashMul        (z7,z5,z1,z3,z4,z6,z2)        																			        \
+	VMOVDQU64 			z7, HashKey_16(gdata) 																											\
+	ghashMul        (z8,z5,z1,z3,z4,z6,z2)        																							\
+	VMOVDQU64 			z8, HashKey_20(gdata) 																											\
+	ghashMul        (z7,z5,z1,z3,z4,z6,z2)        																							\
+	VMOVDQU64 			z7, HashKey_24(gdata) 					                                            \
+	ghashMul        (z8,z5,z1,z3,z4,z6,z2)        																							\
+	VMOVDQU64 			z8, HashKey_28(gdata) 																											\
+	ghashMul        (z7,z5,z1,z3,z4,z6,z2)        																							\			
+	VMOVDQU64 			z7, HashKey_32(gdata) 																											\
+	ghashMul        (z8,z5,z1,z3,z4,z6,z2)        																							\
+	VMOVDQU64 			z8, HashKey_36(gdata) 																											\
+	ghashMul        (z7,z5,z1,z3,z4,z6,z2)        																							\
+	VMOVDQU64 			z7, HashKey_40(gdata) 																											\
+	ghashMul        (z8,z5,z1,z3,z4,z6,z2)        																							\
+	VMOVDQU64 			z8, HashKey_44(gdata) 																											\
+	ghashMul        (z7,z5,z1,z3,z4,z6,z2)        																							\
+	VMOVDQU64 			z7, HashKey_48(gdata) 
+
+#define pKeyData BX
+
+// ipsec nasm uses vaes with two operands and xmm src/dst/ 
+#define encryptSingleBlock_128(ptr,x) \
+	VPXORQ 					(16*0)(ptr), x, x  \
+  VAESENC 				(16*1)(ptr), x, x \
+  VAESENC 				(16*2)(ptr), x, x \
+	VAESENC 				(16*3)(ptr), x, x \
+	VAESENC 				(16*4)(ptr), x, x \
+	VAESENC 				(16*5)(ptr), x, x \
+	VAESENC 				(16*6)(ptr), x, x \
+	VAESENC 				(16*7)(ptr), x, x \
+	VAESENC 				(16*8)(ptr), x, x \
+	VAESENC 				(16*9)(ptr), x, x \
+	VAESENCLAST 		(16*10)(ptr), x, x 
+
+  #define encryptSingleBlock_192(ptr,x) \
+	VPXORQ 					(16*0)(ptr), x, x  \
+  VAESENC 				(16*1)(ptr), x, x \
+  VAESENC 				(16*2)(ptr), x, x \
+	VAESENC 				(16*3)(ptr), x, x \
+	VAESENC 				(16*4)(ptr), x, x \
+	VAESENC 				(16*5)(ptr), x, x \
+	VAESENC 				(16*6)(ptr), x, x \
+	VAESENC 				(16*7)(ptr), x, x \
+	VAESENC 				(16*8)(ptr), x, x \
+	VAESENC 				(16*9)(ptr), x, x \
+	VAESENC 				(16*10)(ptr), x, x \
+	VAESENC 				(16*11)(ptr), x, x \
+	VAESENCLAST 		(16*12)(ptr), x, x 
+
+#define encryptSingleBlock_256(ptr,x) \
+	VPXORQ 					(16*0)(ptr), x, x  \
+  VAESENC 				(16*1)(ptr), x, x \
+  VAESENC 				(16*2)(ptr), x, x \
+	VAESENC 				(16*3)(ptr), x, x \
+	VAESENC 				(16*4)(ptr), x, x \
+	VAESENC 				(16*5)(ptr), x, x \
+	VAESENC 				(16*6)(ptr), x, x \
+	VAESENC 				(16*7)(ptr), x, x \
+	VAESENC 				(16*8)(ptr), x, x \
+	VAESENC 				(16*9)(ptr), x, x \
+	VAESENC 				(16*10)(ptr), x, x \
+	VAESENC 				(16*11)(ptr), x, x \
+	VAESENC 				(16*12)(ptr), x, x \
+	VAESENC 				(16*13)(ptr), x, x \
+	VAESENCLAST 		(16*14)(ptr), x, x 
+
+#define precomputeHash(encryptBlock) \
+	MOVQ 						keyData+0(FP), pKeyData                                                       \
+	VPXOR 					X6, X6, X6  								/* X6 = HashKey */                                \ 
+  encryptBlock    (pKeyData, X6) 			                                                          \
+	/* note: ipsec uses rel to specify rip-relative */                                            \
+	VPSHUFB  				shufMaskU<>(SB), X6, X6                                                       \
+	/* precomputation of HashKey<<1 mod poly from the HashKey */                                  \
+  VMOVDQA 				X6, X2                                                                        \
+  VPSLLQ  				$1, X6, X6                                                                    \
+	VPSRLQ  				$63, X2, X2                                                                   \
+	VMOVDQA 				X2, X1                                                                        \
+	VPSLLDQ 				$8, X2, X2                                                                    \
+	VPSRLDQ					$8, X1, X1                                                                    \
+	VPOR						X2, X6, X6                                                                    \
+	/* reduction */                                                                               \
+	VPSHUFD 				$0b00100100, X1, X2                                                           \
+	VPCMPEQD 				twoone<>(SB), X2, X2                                                          \
+	VPAND 					poly<>(SB), X2, X2                                                            \
+	VPXOR						X2, X6, X6									/* X6 holds the HashKey<<1 mod poly */            \
+	VMOVDQU 				X6, (HashKey)(pKeyData)			/* store HasKey<<1 mod poly */                    \
+	precompute      (pKeyData, X6, X0, Z0, X1, Z1, X2, Z2, X3, Z3, X4, Z4, X5, Z5, X7, Z7, X8, Z8)
+  precomputeHash  (encryptSingleBlock_128)
+RET
+
+// func gcmAesGhashPrecomp192_vaes(keyData)
+TEXT ·gcmAesGhashPrecomp192_vaes(SB),NOSPLIT,$0
+  precomputeHash  (encryptSingleBlock_192)
+RET
+
+// func gcmAesGhashPrecomp256_vaes(keyData)
+TEXT ·gcmAesGhashPrecomp256_vaes(SB),NOSPLIT,$0
+  precomputeHash  (encryptSingleBlock_256)
+RET
+
+#undef pKeyData
+
+#define ghashInnerLoop(hashk,hk,kp,offset,src,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2) \
+  VMOVDQU64             (hashk+offset)(kp), hk                                                                  \
+  VPCLMULQDQ            $0x11, hk, src, t1h          /* H = a1*b1                                            */ \
+  VPCLMULQDQ            $0x00, hk, src, t1l          /* L = a0*b0                                            */ \
+  VPCLMULQDQ            $0x01, hk, src, t1m1         /* M1 = a1*b0                                           */ \
+  VPCLMULQDQ            $0x10, hk, src, t1m2         /* M2 = a0*b1                                           */ \
+  VPXORQ                t1h, t0h, t0h                                                                           \ 
+  VPXORQ                t1l, t0l, t0l                                                                           \ 
+  VPXORQ                t1m1, t0m1, t0m1                                                                        \ 
+  VPXORQ                t1m2, t0m2, t0m2  
+
+/////////////////////////////////////////////////////////////////////////////////////////////////////////////////
+// Horizontal XOR - 4 x 128bits xored together
+// r, rx, ry    [in/out] zmm with 4x128bits to xor; 128bit output (rx=xmm, y=ymm)
+// t, tx, ty    [clobbered] zmm, xmm, ymm temp registers
+//
+#define vhpxori4x128(r,rx,ry,t,tx,ty,kn) \
+  VEXTRACTI64X4         $1, r, kn, ty                                                                           \
+  VPXORQ                ty, ry, ry                                                                              \
+  VEXTRACTI32X4         $1, ry, kn, tx                                                                          \
+  VPXORQ                tx, rx, rx
+
+/////////////////////////////////////////////////////////////////////////////////////////////////////////////////
+// Horizontal XOR - 2 x 128bits xored together
+// r, rx    [in/out] zmm with 2x128bits to xor; 128bit output (rx=xmm, y=ymm)
+// t, tx    [clobbered] zmm, xmm, ymm temp registers
+//
+#define vhpxori2x128(r,rx,tx,kn) \
+  VEXTRACTI32X4         $1, r, kn, tx                                                                           \
+  VPXORQ                tx, rx, rx                                                                              
+
+/////////////////////////////////////////////////////////////////////////////////////////////////////////////////
+// AVX512 reduction 
+// out          [out] zmm/ymm/xmm: result (must not be tmp1 or hi128)
+// poly         [in] zmm/ymm/xmm: polynomial
+// hi128        [in] zmm/ymm/xmm: high 128b of hash to reduce
+// lo128        [in] zmm/ymm/xmm: low 128b of hash to reduce
+// tmp0         [in] zmm/ymm/xmm: temporary register
+// tmp1         [in] zmm/ymm/xmm: temporary register
+//
+#define vclmulReduce(out,poly,hi128,lo128,tmp0,tmp1) \
+  VPCLMULQDQ            $0x01, lo128, poly, tmp0                                                                  \
+  VPSLLDQ               $8, tmp0, tmp0                    /* shift-L 2 DWs */                                     \
+  VPXORQ                tmp0, lo128, tmp0                 /* first phase of the reduction complete */             \
+  VPCLMULQDQ            $0x00, tmp0, poly, tmp1                                                                   \
+  VPSRLDQ               $4, tmp1, tmp1                    /* shift-R only 1-DW to obtain 2-DWs shift-R */         \
+  VPCLMULQDQ            $0x10, tmp0, poly, out                                                                    \
+  VPSLLDQ               $4, out, out                      /* shift-L 1-DW to obtain result with no shifts */      \
+  VPTERNLOGQ            $0x96, hi128, tmp1, out           /* out/ghash = out xor tmp1 xor hi128 */                
+
+#define ghashReduce(ghash,t0h,t0hx,t0hy,t0l,t0lx,t0ly,t0m1,t0m1x,t0m2,t0m2x,t1m1,t1m1x,t1m1y,t1m2,t1m2x,t1m2y,hk,kn) \
+  /* integrate tm into th and tl */                                                                                                \
+  VPXORQ                t0m2, t0m1, t0m1                                                                                           \
+  VPSRLDQ               $8, t0m1, t1m1                                                                                             \
+  VPSLLDQ               $8, t0m1, t1m2                                                                                             \
+  VPXORQ                t1m1, t0h, t0h                                                                                             \
+  VPXORQ                t1m2, t0l, t0l                                                                                             \
+  /* add TH and TL 128-bit words horizontally */                                                                                   \
+  vhpxori4x128          (t0h,t0hx,t0hy,t1m1,t1m1x,t1m1y,kn)                                                                        \
+  vhpxori4x128          (t0l,t0lx,t0ly,t1m2,t1m2x,t1m2y,kn)                                                                        \                                   
+  /* reduce */                                                                                                                     \
+  VMOVDQA64				      poly2<>(SB), hk					                                                    					                     \
+  vclmulReduce          (ghash,hk,t0hx,t0lx,t0m1x,t0m2x)
+
+#define ghash1(kp,t1hx,t1lx,t1m1x,t1m2x,hkx,cipherIn3x,hashk) \
+  VMOVDQU64             (hashk)(kp), hkx                                                                                           \
+  VPCLMULQDQ            $0x11, hkx, cipherIn3x, t1hx           /* th = a1*b1 */                                                    \
+  VPCLMULQDQ            $0x00, hkx, cipherIn3x, t1lx           /* tl = a0*b0 */                                                    \
+  VPCLMULQDQ            $0x01, hkx, cipherIn3x, t1m1x          /* tm1 = a1*b0 */                                                   \
+  VPCLMULQDQ            $0x10, hkx, cipherIn3x, t1m2x          /* tm2 = a0*b1 */                                                     
+
+#define ghash2(kp,t1hy,t1ly,t1m1y,t1m2y,hky,cipherIn3y,hashk) \
+  VMOVDQU64             (hashk)(kp), hky                                                                                           \
+  VPCLMULQDQ            $0x11, hky, cipherIn3y, t1hy           /* th = a1*b1 */                                                    \
+  VPCLMULQDQ            $0x00, hky, cipherIn3y, t1ly           /* tl = a0*b0 */                                                    \
+  VPCLMULQDQ            $0x01, hky, cipherIn3y, t1m1y          /* tm1 = a1*b0 */                                                   \
+  VPCLMULQDQ            $0x10, hky, cipherIn3y, t1m2y          /* tm2 = a0*b1 */                                                     
+
+#define ghash3(kp,t1h,t1l,t1m1,t1m2,hk,hky,cipherIn3,hashk) \
+  VMOVDQU64             (hashk)(kp), hky                                                                                           \
+  KMOVW 					      kMask2<>(SB), K3																														                               \
+  VINSERTI64X2          $2, (hashk+32)(kp), Z31, K3, hk                                                                            \
+  VPCLMULQDQ            $0x11, hk, cipherIn3, t1h           /* th = a1*b1 */                                                       \
+  VPCLMULQDQ            $0x00, hk, cipherIn3, t1l           /* tl = a0*b0 */                                                       \
+  VPCLMULQDQ            $0x01, hk, cipherIn3, t1m1          /* tm1 = a1*b0 */                                                      \
+  VPCLMULQDQ            $0x10, hk, cipherIn3, t1m2          /* tm2 = a0*b1 */                                                     
+
+#define ghash4(kp,t0h,t0l,t0m1,t0m2,hk,cipherIn0,hashk) \
+  VMOVDQU64             (hashk)(kp), hk                                                   /* first_result==1, blocks_left = 16 */  \
+  VPCLMULQDQ            $0x11, hk, cipherIn0, t0h                                         /* H = a1*b1                         */  \
+  VPCLMULQDQ            $0x00, hk, cipherIn0, t0l                                         /* L = a0*b0                         */  \
+  VPCLMULQDQ            $0x01, hk, cipherIn0, t0m1                                        /* M1 = a1*b0                        */  \
+  VPCLMULQDQ            $0x10, hk, cipherIn0, t0m2                                        /* M2 = a0*b1                        */  
+
+#define ghashcombine(t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2) \
+  VPXORQ                t1h, t0h, t0h                                                                                              \
+  VPXORQ                t1l, t0l, t0l                                                                                              \
+  VPXORQ                t1m1, t0m1, t0m1                                                                                           \
+  VPXORQ                t1m2, t0m2, t0m2     
+
+#define ghash5(kp,t0h,t0l,t0m1,t0m2,t1h,t1hx,t1l,t1lx,t1m1,t1m1x,t1m2,t1m2x,hk,hkx,cipherIn0,cipherIn1x,hashk) \
+  ghash4                (kp,t0h,t0l,t0m1,t0m2,hk,cipherIn0,hashk)                                                                  \
+  ghash1                (kp,t1hx,t1lx,t1m1x,t1m2x,hkx,cipherIn1x,hashk+64)                                                         \
+  ghashcombine          (t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)                     
+
+#define ghash6(kp,t0h,t0l,t0m1,t0m2,t1h,t1hy,t1l,t1ly,t1m1,t1m1y,t1m2,t1m2y,hk,hky,cipherIn0,cipherIn1y,hashk) \
+  ghash4                (kp,t0h,t0l,t0m1,t0m2,hk,cipherIn0,hashk)                                                                  \
+  ghash2                (kp,t1hy,t1ly,t1m1y,t1m2y,hky,cipherIn1y,hashk+64)                                                         \
+  ghashcombine          (t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)                     
+
+#define ghash7(kp,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,hk,hky,cipherIn0,cipherIn1,hashk) \
+  ghash4                (kp,t0h,t0l,t0m1,t0m2,hk,cipherIn0,hashk)                                                                  \
+  ghash3                (kp,t1h,t1l,t1m1,t1m2,hk,hky,cipherIn1,hashk+64)                                                           \
+  ghashcombine          (t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)                     
+
+#define ghash8(kp,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,hk,cipherIn0,cipherIn1,hashk) \
+  VMOVDQU64             (hashk)(kp), hk                                                   /* first_result==1, blocks_left = 16 */  \
+  VPCLMULQDQ            $0x11, hk, cipherIn0, t0h                                         /* H = a1*b1                         */  \
+  VPCLMULQDQ            $0x00, hk, cipherIn0, t0l                                         /* L = a0*b0                         */  \
+  VPCLMULQDQ            $0x01, hk, cipherIn0, t0m1                                        /* M1 = a1*b0                        */  \
+  VPCLMULQDQ            $0x10, hk, cipherIn0, t0m2                                        /* M2 = a0*b1                        */  \
+  ghashInnerLoop        (hashk,hk,kp,64,cipherIn1,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)                                             
+
+#define ghash9(kp,t0h,t0l,t0m1,t0m2,t1h,t1hx,t1l,t1lx,t1m1,t1m1x,t1m2,t1m2x,hk,hkx,cipherIn0,cipherIn1,cipherIn2x,hashk) \
+  ghash8                (kp,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,hk,cipherIn0,cipherIn1,hashk)                                      \
+  ghash1                (kp,t1hx,t1lx,t1m1x,t1m2x,hkx,cipherIn2x,hashk+128)                                                        \
+  ghashcombine          (t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)                     
+
+#define ghash10(kp,t0h,t0l,t0m1,t0m2,t1h,t1hy,t1l,t1ly,t1m1,t1m1y,t1m2,t1m2y,hk,hky,cipherIn0,cipherIn1,cipherIn2y,hashk) \
+  ghash8                (kp,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,hk,cipherIn0,cipherIn1,hashk)                                      \
+  ghash2                (kp,t1hy,t1ly,t1m1y,t1m2y,hky,cipherIn2y,hashk+128)                                                        \
+  ghashcombine          (t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)                     
+
+#define ghash11(kp,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,hk,hky,cipherIn0,cipherIn1,cipherIn2,hashk) \
+  ghash8                (kp,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,hk,cipherIn0,cipherIn1,hashk)                                      \
+  ghash3                (kp,t1h,t1l,t1m1,t1m2,hk,hky,cipherIn2,hashk+128)                                                          \
+  ghashcombine          (t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)                     
+
+#define ghash12(kp,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,hk,cipherIn0,cipherIn1,cipherIn2,hashk) \
+  VMOVDQU64             (hashk)(kp), hk                                                   /* first_result==1, blocks_left = 16 */  \
+  VPCLMULQDQ            $0x11, hk, cipherIn0, t0h                                         /* H = a1*b1                         */  \
+  VPCLMULQDQ            $0x00, hk, cipherIn0, t0l                                         /* L = a0*b0                         */  \
+  VPCLMULQDQ            $0x01, hk, cipherIn0, t0m1                                        /* M1 = a1*b0                        */  \
+  VPCLMULQDQ            $0x10, hk, cipherIn0, t0m2                                        /* M2 = a0*b1                        */  \
+  ghashInnerLoop        (hashk,hk,kp,64,cipherIn1,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)                                             \
+  ghashInnerLoop        (hashk,hk,kp,128,cipherIn2,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)
+
+#define ghash13(kp,t0h,t0l,t0m1,t0m2,t1h,t1hx,t1l,t1lx,t1m1,t1m1x,t1m2,t1m2x,hk,hkx,cipherIn0,cipherIn1,cipherIn2,cipherIn3x,hashk)  \
+  ghash12               (kp,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,hk,cipherIn0,cipherIn1,cipherIn2,hashk)                              \
+  ghash1                (kp,t1hx,t1lx,t1m1x,t1m2x,hkx,cipherIn3x,hashk+192)                                                          \
+  ghashcombine          (t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)                     
+
+#define ghash14(kp,t0h,t0l,t0m1,t0m2,t1h,t1hy,t1l,t1ly,t1m1,t1m1y,t1m2,t1m2y,hk,hky,cipherIn0,cipherIn1,cipherIn2,cipherIn3y,hashk)  \
+  ghash12               (kp,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,hk,cipherIn0,cipherIn1,cipherIn2,hashk)                              \
+  ghash2                (kp,t1hy,t1ly,t1m1y,t1m2y,hky,cipherIn3y,hashk+192)                                                          \
+  ghashcombine          (t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)                     
+
+#define ghash15(kp,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,hk,hky,cipherIn0,cipherIn1,cipherIn2,cipherIn3,hashk) \
+  ghash12               (kp,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,hk,cipherIn0,cipherIn1,cipherIn2,hashk)                            \
+  ghash3                (kp,t1h,t1l,t1m1,t1m2,hk,hky,cipherIn3,hashk+192)                                                          \
+  ghashcombine          (t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)                     
+
+#define ghash16fs(kp,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,hk,cipherIn0,cipherIn1,cipherIn2,cipherIn3,hashk) \
+  VMOVDQU64             (hashk)(kp), hk                                                   /* first_result==1, blocks_left = 16 */  \
+  VPCLMULQDQ            $0x11, hk, cipherIn0, t0h                                         /* H = a1*b1                         */  \
+  VPCLMULQDQ            $0x00, hk, cipherIn0, t0l                                         /* L = a0*b0                         */  \
+  VPCLMULQDQ            $0x01, hk, cipherIn0, t0m1                                        /* M1 = a1*b0                        */  \
+  VPCLMULQDQ            $0x10, hk, cipherIn0, t0m2                                        /* M2 = a0*b1                        */  \
+  ghashInnerLoop        (hashk,hk,kp,64,cipherIn1,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)                                             \
+  ghashInnerLoop        (hashk,hk,kp,128,cipherIn2,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)                                            \
+  ghashInnerLoop        (hashk,hk,kp,192,cipherIn3,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)                                            
+
+#define ghash16c(kp,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,hk,cipherIn1,cipherIn2,cipherIn3,hashk,prevH,prevL,prevM1,prevM2) \
+  ghashInnerLoop        (hashk,hk,kp,64,cipherIn1,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)                                             \
+  ghashInnerLoop        (hashk,hk,kp,128,cipherIn2,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)                                            \
+  ghashInnerLoop        (hashk,hk,kp,192,cipherIn3,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)                                            \ 
+  VMOVDQA64	            t0h, prevH                                                                                                 \
+  VMOVDQA64	            t0l, prevL                                                                                                 \
+  VMOVDQA64	            t0m1, prevM1                                                                                               \
+  VMOVDQA64	            t0m2, prevM2
+
+#define ghash16f(kp,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,hk,cipherIn0,cipherIn1,cipherIn2,cipherIn3,hashk,prevH,prevL,prevM1,prevM2) \
+  VMOVDQU64             (hashk)(kp), hk                                                   /* first_result==1, blocks_left = 16 */  \
+  VPCLMULQDQ            $0x11, hk, cipherIn0, t0h                                         /* H = a1*b1                         */  \
+  VPCLMULQDQ            $0x00, hk, cipherIn0, t0l                                         /* L = a0*b0                         */  \
+  VPCLMULQDQ            $0x01, hk, cipherIn0, t0m1                                        /* M1 = a1*b0                        */  \
+  VPCLMULQDQ            $0x10, hk, cipherIn0, t0m2                                        /* M2 = a0*b1                        */  \
+  ghash16c              (kp,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,hk,cipherIn1,cipherIn2,cipherIn3,hashk,prevH,prevL,prevM1,prevM2)
+
+// first_result==0, no reduce
+#define ghash16(kp,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,hk,cipherIn0,cipherIn1,cipherIn2,cipherIn3,hashk,prevH,prevL,prevM1,prevM2) \
+  VMOVDQA64	            prevH, t0h                                                                                                 \
+  VMOVDQA64	            prevL, t0l                                                                                                 \
+  VMOVDQA64	            prevM1, t0m1                                                                                               \
+  VMOVDQA64	            prevM2, t0m2                                                                                               \
+  ghashInnerLoop        (hashk,hk,kp,0,cipherIn0,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2)     /* first_result==0, blocks_left = 16 */  \
+  ghash16c              (kp,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,hk,cipherIn1,cipherIn2,cipherIn3,hashk,prevH,prevL,prevM1,prevM2)
+
+/*
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+; CALC_AAD_HASH: Calculates the hash of the data which will not be encrypted.
+; Input: The input data (A_IN), that data's length (A_LEN), and the hash key (HASH_KEY).
+; Output: The hash of the data (AAD_HASH).
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+	aad      - [in] AAD text pointer
+	aadLen   - [in] AAD text length
+  aadHash  - [in/out] ghash value
+	keyData  - [in] pointer to keys
+  z0..17   - [clobbered] ZMM registers 0..17
+  t1..t3  -  [clobbered] GP registers 1..3
+  maskreg  - [clobbered] mask register */
+
+// N.B. CalcAADHash1, CalcAADHash2, and CalcAADHash3 are identical macros
+// three independent macro instances are needed to support embedded labels for the non-std IV external GCMInit function 
+// that calls CalcAADHash three times, once for GCMInit, once for ivlen==12, and once for ivlen!=12 all within the same ASM function scope
+// because all three calls occur within scope it is necessary to have 3 sets of independent labels; multiple "as-is" instanaces would 
+// generate duplicate labels at macro expansion time during assemlby and cause the assembler to throw an error;
+// also, because passing labels as parameters on macro invocation is cumbersome for a large macro with 24 labels, three wrappers are provided below. 
+// All three implementations are identical except for the label names; 
+// the macro wrappers CalcAADHash1..3 take care of 24 labels to shorten parameter list for calling functions
+
+// first instance used by GCMInit, aadHashing for both internal and external GCMInit APIs
+#define CalcAADHash1(aad,aadLen,aadHash,aadHashZ,keyData,ctxData,z0,x0,y0,z1,x1,y1,z2,x2,y2,z3,x3,y3,z4,x4,y4,z5,x5,y5,z6,x6,z7,x7,z8,x8,y8,z9,x9,y9,z10,x10,y10,z11,x11,y11,z12,x12,y12,shuffleMask,z14,z15,z16,z17,t1,t2,t3,maskreg,hXorMask) \ 
+  _CalcAADHash(aad,aadLen,aadHash,aadHashZ,keyData,ctxData,z0,x0,y0,z1,x1,y1,z2,x2,y2,z3,x3,y3,z4,x4,y4,z5,x5,y5,z6,x6,z7,x7,z8,x8,y8,z9,x9,y9,z10,x10,y10,z11,x11,y11,z12,x12,y12,shuffleMask,z14,z15,z16,z17,t1,t2,t3,maskreg,hXorMask,_loop48x16a,_exitLoop48x16a,_lessthan32x16a,_lessthan16x16a,_block1a,_block2a,_block3a,_block4a,_block5a,_block6a,_block7a,_block8a,_block9a,_block10a,_block11a,_block12a,_block13a,_block14a,_block15a,_block16a,_donea)
+
+// second instance used by GCMInit in J0 calculation for non-standard IV length (len(iv)!=12)), internal and external GCMInit APIs
+#define CalcAADHash2(aad,aadLen,aadHash,aadHashZ,keyData,ctxData,z0,x0,y0,z1,x1,y1,z2,x2,y2,z3,x3,y3,z4,x4,y4,z5,x5,y5,z6,x6,z7,x7,z8,x8,y8,z9,x9,y9,z10,x10,y10,z11,x11,y11,z12,x12,y12,shuffleMask,z14,z15,z16,z17,t1,t2,t3,maskreg,hXorMask) \ 
+  _CalcAADHash(aad,aadLen,aadHash,aadHashZ,keyData,ctxData,z0,x0,y0,z1,x1,y1,z2,x2,y2,z3,x3,y3,z4,x4,y4,z5,x5,y5,z6,x6,z7,x7,z8,x8,y8,z9,x9,y9,z10,x10,y10,z11,x11,y11,z12,x12,y12,shuffleMask,z14,z15,z16,z17,t1,t2,t3,maskreg,hXorMask,_loop48x16b,_exitLoop48x16b,_lessthan32x16b,_lessthan16x16b,_block1b,_block2b,_block3b,_block4b,_block5b,_block6b,_block7b,_block8b,_block9b,_block10b,_block11b,_block12b,_block13b,_block14b,_block15b,_block16b,_doneb)
+
+// third instance used by GCMInit for external GCMInit API with standard IV length (len(iv)==12 branch case)
+#define CalcAADHash3(aad,aadLen,aadHash,aadHashZ,keyData,ctxData,z0,x0,y0,z1,x1,y1,z2,x2,y2,z3,x3,y3,z4,x4,y4,z5,x5,y5,z6,x6,z7,x7,z8,x8,y8,z9,x9,y9,z10,x10,y10,z11,x11,y11,z12,x12,y12,shuffleMask,z14,z15,z16,z17,t1,t2,t3,maskreg,hXorMask) \ 
+  _CalcAADHash(aad,aadLen,aadHash,aadHashZ,keyData,ctxData,z0,x0,y0,z1,x1,y1,z2,x2,y2,z3,x3,y3,z4,x4,y4,z5,x5,y5,z6,x6,z7,x7,z8,x8,y8,z9,x9,y9,z10,x10,y10,z11,x11,y11,z12,x12,y12,shuffleMask,z14,z15,z16,z17,t1,t2,t3,maskreg,hXorMask,_loop48x16c,_exitLoop48x16c,_lessthan32x16c,_lessthan16x16c,_block1c,_block2c,_block3c,_block4c,_block5c,_block6c,_block7c,_block8c,_block9c,_block10c,_block11c,_block12c,_block13c,_block14c,_block15c,_block16c,_donec)
+
+// CalcAADHash macro implementation uses parametric labels to enable multiple macro instances, otherwise labels would be multiply defined and would not assemble
+// parametric loop labels are prefixed with "_"; these are replaced on macro expansion with an instance-specific label, from the input parameters, e.g., _loop48x16 --> _loop48x16a/b/c
+#define _CalcAADHash(aad,aadLen,aadHash,aadHashZ,keyData,ctxData,z0,x0,y0,z1,x1,y1,z2,x2,y2,z3,x3,y3,z4,x4,y4,z5,x5,y5,z6,x6,z7,x7,z8,x8,y8,z9,x9,y9,z10,x10,y10,z11,x11,y11,z12,x12,y12,shuffleMask,z14,z15,z16,z17,t1,t2,t3,maskreg,hXorMask,_loop48x16,_exitLoop48x16,_lessthan32x16,_lessthan16x16,_block1,_block2,_block3,_block4,_block5,_block6,_block7,_block8,_block9,_block10,_block11,_block12,_block13,_block14,_block15,_block16,_done) \ 
+	  MOVQ						aad, t1 																									                                       \ 
+	  MOVQ						aadLen, t2																								                                       \
+	  ORQ							t2, t2																										                                       \ 
+	  JZ 							_done																					                                                   \
+    KXNORQ 					hXorMask, hXorMask, hXorMask  														                                       \
+                                                                                                                     \ 
+  _loop48x16:																							    			                                                 \
+	  CMPQ						t2, $(48*16)																							                                       \
+	  JL 							_exitLoop48x16																	                                                 \ 
+	  VMOVDQU64				(64*0)(t1), z1										/* Blocks 0-3   */ 			                                       \
+	  VMOVDQU64				(64*1)(t1), z2  									/* Blocks 4-7   */ 			                                       \
+	  VMOVDQU64				(64*2)(t1), z3  									/* Blocks 8-11  */ 			                                       \
+	  VMOVDQU64				(64*3)(t1), z4  									/* Blocks 12-15 */ 			                                       \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+	  VPSHUFB					shuffleMask, z3, z3   																		                                       \
+	  VPSHUFB					shuffleMask, z4, z4                                                                              \
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash16f        (keyData, z0, z5, z6, z7, z8, z9, z10, z11, z12, z1, z2, z3, z4, HashKey_48, z14, z15, z16, z17) \
+	  VMOVDQU64				(16*16 + 64*0)(t1), z1						/* Blocks 16-19 */ 			                                       \
+	  VMOVDQU64				(16*16 + 64*1)(t1), z2  				  /* Blocks 20-23 */ 			                                       \
+	  VMOVDQU64				(16*16 + 64*2)(t1), z3  				  /* Blocks 24-27 */ 			                                       \
+	  VMOVDQU64				(16*16 + 64*3)(t1), z4  					/* Blocks 28-31 */ 			                                       \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+	  VPSHUFB					shuffleMask, z3, z3   																		                                       \
+	  VPSHUFB					shuffleMask, z4, z4                                                                              \
+    ghash16         (keyData, z0, z5, z6, z7, z8, z9, z10, z11, z12, z1, z2, z3, z4, HashKey_32, z14, z15, z16, z17) \
+	  VMOVDQU64				(32*16 + 64*0)(t1), z1						/* Blocks 32-35 */ 			                                       \
+	  VMOVDQU64				(32*16 + 64*1)(t1), z2  				  /* Blocks 36-39 */ 			                                       \
+	  VMOVDQU64				(32*16 + 64*2)(t1), z3  				  /* Blocks 40-43 */ 			                                       \
+	  VMOVDQU64				(32*16 + 64*3)(t1), z4  					/* Blocks 44-47 */ 			                                       \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+	  VPSHUFB					shuffleMask, z3, z3   																		                                       \
+	  VPSHUFB					shuffleMask, z4, z4                                                                              \
+    ghash16         (keyData, z0, z5, z6, z7, z8, z9, z10, z11, z12, z1, z2, z3, z4, HashKey_16, z14, z15, z16, z17) \
+    ghashReduce     (aadHash,z0,x0,y0,z5,x5,y5,z6,x6,z7,x7,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    SUBQ            $(48*16), t2                                                                                     \
+    JE              _done                                                                                            \
+    ADDQ            $(48*16), t1                                                                                     \
+    JMP             _loop48x16                                                                                       \
+                                                                                                                     \
+  _exitLoop48x16:						                                                                                         \								
+    CMPQ             t2, $(32*16)                                                                                    \
+    JL               _lessthan32x16                                                                                  \
+    /* Get next 16 blocks */                                                                                         \                                                                                
+	  VMOVDQU64				(64*0)(t1), z1						                          			                                       \
+	  VMOVDQU64				(64*1)(t1), z2  				   			                                                                 \
+	  VMOVDQU64				(64*2)(t1), z3  				   			                                                                 \
+	  VMOVDQU64				(64*3)(t1), z4  					 			                                                                 \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+	  VPSHUFB					shuffleMask, z3, z3   																		                                       \
+	  VPSHUFB					shuffleMask, z4, z4                                                                              \
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash16f        (keyData, z0, z5, z6, z7, z8, z9, z10, z11, z12, z1, z2, z3, z4, HashKey_32, z14, z15, z16, z17) \
+	  VMOVDQU64				(16*16 + 64*0)(t1), z1						                                                               \
+	  VMOVDQU64				(16*16 + 64*1)(t1), z2  				   			                                                         \
+	  VMOVDQU64				(16*16 + 64*2)(t1), z3  				   			                                                         \
+	  VMOVDQU64				(16*16 + 64*3)(t1), z4  					 			                                                         \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+	  VPSHUFB					shuffleMask, z3, z3   																		                                       \
+	  VPSHUFB					shuffleMask, z4, z4                                                                              \
+    ghash16         (keyData, z0, z5, z6, z7, z8, z9, z10, z11, z12, z1, z2, z3, z4, HashKey_16, z14, z15, z16, z17) \
+    ghashReduce     (aadHash,z0,x0,y0,z5,x5,y5,z6,x6,z7,x7,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    SUBQ            $(32*16), t2                                                                                     \
+    JE              _done                                                                                            \
+    ADDQ            $(32*16), t1                                                                                     \
+    JMP             _lessthan16x16                                                                                   \
+                                                                                                                     \
+  _lessthan32x16:                                                                                                    \
+    CMPQ             t2, $(16*16)                                                                                    \
+    JL               _lessthan16x16                                                                                  \
+    /* Get next 16 blocks */                                                                                         \                                                                                
+	  VMOVDQU64				(64*0)(t1), z1						                          			                                       \
+	  VMOVDQU64				(64*1)(t1), z2  				   			                                                                 \
+	  VMOVDQU64				(64*2)(t1), z3  				   			                                                                 \
+	  VMOVDQU64				(64*3)(t1), z4  					 			                                                                 \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+	  VPSHUFB					shuffleMask, z3, z3   																		                                       \
+	  VPSHUFB					shuffleMask, z4, z4                                                                              \
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash16fs       (keyData, z0, z5, z6, z7, z8, z9, z10, z11, z12, z1, z2, z3, z4, HashKey_16)                     \
+    ghashReduce     (aadHash,z0,x0,y0,z5,x5,y5,z6,x6,z7,x7,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    SUBQ            $(16*16), t2                                                                                     \
+    JE              _done                                                                                            \
+    ADDQ            $(16*16), t1                                                                                     \
+                                                                                                                     \
+  /* Less than 16x16 bytes remaining */                                                                              \
+  _lessthan16x16:                                                                                                    \
+    MOVQ            (byteLen2MaskPtr)(ctxData), t3                                                                   \
+    LEAQ            (t3)(t2*8), t3                                                                                   \ 
+    /* calculate number of blocks to ghash, including partial bytes */                                               \
+    ADDQ            $15, t2                                                                                          \
+    ANDQ            $-16, t2      /* 1 to 16 blocks possible here */                                                 \
+    SHRQ            $4, t2                                                                                           \
+    CMPQ            t2, $1                                                                                           \
+    JE              _block1                                                                                          \
+    CMPQ            t2, $2                                                                                           \
+    JE              _block2                                                                                          \
+    CMPQ            t2, $3                                                                                           \
+    JE              _block3                                                                                          \
+    CMPQ            t2, $4                                                                                           \
+    JE              _block4                                                                                          \
+    CMPQ            t2, $5                                                                                           \
+    JE              _block5                                                                                          \
+    CMPQ            t2, $6                                                                                           \
+    JE              _block6                                                                                          \
+    CMPQ            t2, $7                                                                                           \
+    JE              _block7                                                                                          \
+    CMPQ            t2, $8                                                                                           \
+    JE              _block8                                                                                          \
+    CMPQ            t2, $9                                                                                           \
+    JE              _block9                                                                                          \
+    CMPQ            t2, $10                                                                                          \
+    JE              _block10                                                                                         \
+    CMPQ            t2, $11                                                                                          \
+    JE              _block11                                                                                         \
+    CMPQ            t2, $12                                                                                          \
+    JE              _block12                                                                                         \
+    CMPQ            t2, $13                                                                                          \
+    JE              _block13                                                                                         \
+    CMPQ            t2, $14                                                                                          \
+    JE              _block14                                                                                         \
+    CMPQ            t2, $15                                                                                          \ 
+    JE              _block15                                                                                         \ 
+                                                                                                                     \
+  /* fall through for 16 blocks */                                                                                   \
+  /* The flow of each of these cases is identical:      */                                                           \
+  /*  - load blocks plain text                          */                                                           \
+  /*  - shuffle loaded blocks                           */                                                           \
+  /*  - xor in current hash value into block 0          */                                                           \
+  /*  - perform up multiplications with ghash keys      */                                                           \
+  /*  - jump to reduction code                          */                                                           \
+  _block16:                                                                                                          \
+    SUBQ            $(64 * 3 * 8), t3                                                                                \
+    KMOVQ           (t3), maskreg                                                                                    \
+    VMOVDQU8        (64*0)(t1), z1                                                                                   \
+    VMOVDQU8        (64*1)(t1), z2                                                                                   \
+    VMOVDQU8        (64*2)(t1), z3                                                                                   \
+    VMOVDQU8.Z      (64*3)(t1), maskreg, z4                                                                          \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+	  VPSHUFB					shuffleMask, z3, z3   																		                                       \
+	  VPSHUFB					shuffleMask, z4, z4                                                                              \
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash16fs       (keyData, z0, z5, z6, z7, z8, z9, z10, z11, z12, z1, z2, z3, z4, HashKey_16)                     \
+    ghashReduce     (aadHash,z0,x0,y0,z5,x5,y5,z6,x6,z7,x7,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    JMP             _done                                                                                            \
+                                                                                                                     \
+  _block15:                                                                                                          \
+    SUBQ            $(64 * 3 * 8), t3                                                                                \
+    KMOVQ           (t3), maskreg                                                                                    \
+    VMOVDQU8        (64*0)(t1), z1                                                                                   \
+    VMOVDQU8        (64*1)(t1), z2                                                                                   \
+    VMOVDQU8        (64*2)(t1), z3                                                                                   \
+    VMOVDQU8.Z      (64*3)(t1), maskreg, z4                                                                          \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+	  VPSHUFB					shuffleMask, z3, z3   																		                                       \
+	  VPSHUFB					shuffleMask, z4, z4                                                                              \
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash15         (keyData,z0,z5,z6,z7,z8,z9,z10,z11,z12,y12,z1,z2,z3,z4,HashKey_15)                               \
+    ghashReduce     (aadHash,z0,x0,y0,z5,x5,y5,z6,x6,z7,x7,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    JMP             _done                                                                                            \
+                                                                                                                     \
+  _block14:                                                                                                          \
+    SUBQ            $(64 * 3 * 8), t3                                                                                \
+    KMOVQ           (t3), maskreg                                                                                    \
+    VMOVDQU8        (64*0)(t1), z1                                                                                   \
+    VMOVDQU8        (64*1)(t1), z2                                                                                   \
+    VMOVDQU8        (64*2)(t1), z3                                                                                   \
+    VMOVDQU8.Z      (64*3)(t1), maskreg, z4                                                                          \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+	  VPSHUFB					shuffleMask, z3, z3   																		                                       \
+	  VPSHUFB					shuffleMask, z4, z4                                                                              \
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash14         (keyData,z0,z5,z6,z7,z8,y8,z9,y9,z10,y10,z11,y11,z12,y12,z1,z2,z3,y4,HashKey_14)                 \
+    ghashReduce     (aadHash,z0,x0,y0,z5,x5,y5,z6,x6,z7,x7,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    JMP             _done                                                                                            \
+                                                                                                                     \
+  _block13:                                                                                                          \
+    SUBQ            $(64 * 3 * 8), t3                                                                                \
+    KMOVQ           (t3), maskreg                                                                                    \
+    VMOVDQU8        (64*0)(t1), z1                                                                                   \
+    VMOVDQU8        (64*1)(t1), z2                                                                                   \
+    VMOVDQU8        (64*2)(t1), z3                                                                                   \
+    VMOVDQU8.Z      (64*3)(t1), maskreg, z4                                                                          \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+	  VPSHUFB					shuffleMask, z3, z3   																		                                       \
+	  VPSHUFB					shuffleMask, z4, z4                                                                              \
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash13         (keyData,z0,z5,z6,z7,z8,x8,z9,x9,z10,x10,z11,x11,z12,x12,z1,z2,z3,x4,HashKey_13)                 \
+    ghashReduce     (aadHash,z0,x0,y0,z5,x5,y5,z6,x6,z7,x7,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    JMP             _done                                                                                            \
+                                                                                                                     \
+  _block12:                                                                                                          \
+    SUBQ            $(64 * 2 * 8), t3                                                                                \
+    KMOVQ           (t3), maskreg                                                                                    \
+    VMOVDQU8        (64*0)(t1), z1                                                                                   \
+    VMOVDQU8        (64*1)(t1), z2                                                                                   \
+    VMOVDQU8.Z      (64*2)(t1), maskreg, z3                                                                          \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+	  VPSHUFB					shuffleMask, z3, z3   																		                                       \
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash12         (keyData,z0,z5,z6,z7,z8,z9,z10,z11,z12,z1,z2,z3,HashKey_12)                                      \
+    ghashReduce     (aadHash,z0,x0,y0,z5,x5,y5,z6,x6,z7,x7,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    JMP             _done                                                                                            \
+                                                                                                                     \
+  _block11:                                                                                                          \
+    SUBQ            $(64 * 2 * 8), t3                                                                                \
+    KMOVQ           (t3), maskreg                                                                                    \
+    VMOVDQU8        (64*0)(t1), z1                                                                                   \
+    VMOVDQU8        (64*1)(t1), z2                                                                                   \
+    VMOVDQU8.Z      (64*2)(t1), maskreg, z3                                                                          \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+	  VPSHUFB					shuffleMask, z3, z3   																		                                       \
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash11         (keyData,z0,z5,z6,z7,z8,z9,z10,z11,z12,y12,z1,z2,z3,HashKey_11)                                  \
+    ghashReduce     (aadHash,z0,x0,y0,z5,x5,y5,z6,x6,z7,x7,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    JMP             _done                                                                                            \
+                                                                                                                     \
+  _block10:                                                                                                          \
+    SUBQ            $(64 * 2 * 8), t3                                                                                \
+    KMOVQ           (t3), maskreg                                                                                    \
+    VMOVDQU8        (64*0)(t1), z1                                                                                   \
+    VMOVDQU8        (64*1)(t1), z2                                                                                   \
+    VMOVDQU8.Z      (64*2)(t1), maskreg, z3                                                                          \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+	  VPSHUFB					shuffleMask, z3, z3   																		                                       \
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash10         (keyData,z0,z5,z6,z7,z8,y8,z9,y9,z10,y10,z11,y11,z12,y12,z1,z2,y3,HashKey_10)                    \
+    ghashReduce     (aadHash,z0,x0,y0,z5,x5,y5,z6,x6,z7,x7,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    JMP             _done                                                                                            \
+                                                                                                                     \
+  _block9:                                                                                                           \
+    SUBQ            $(64 * 2 * 8), t3                                                                                \
+    KMOVQ           (t3), maskreg                                                                                    \
+    VMOVDQU8        (64*0)(t1), z1                                                                                   \
+    VMOVDQU8        (64*1)(t1), z2                                                                                   \
+    VMOVDQU8.Z      (64*2)(t1), maskreg, z3                                                                          \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+	  VPSHUFB					shuffleMask, z3, z3   																		                                       \
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash9          (keyData,z0,z5,z6,z7,z8,x8,z9,x9,z10,x10,z11,x11,z12,x12,z1,z2,x3,HashKey_9)                     \
+    ghashReduce     (aadHash,z0,x0,y0,z5,x5,y5,z6,x6,z7,x7,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    JMP             _done                                                                                            \
+                                                                                                                     \
+  _block8:                                                                                                           \
+    SUBQ            $(64 * 8), t3                                                                                    \
+    KMOVQ           (t3), maskreg                                                                                    \
+    VMOVDQU8        (64*0)(t1), z1                                                                                   \
+    VMOVDQU8.Z      (64*1)(t1), maskreg, z2                                                                          \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash8          (keyData,z0,z3,z4,z5,z6,z7,z10,z11,z12,z1,z2,HashKey_8)                                          \
+    ghashReduce     (aadHash,z0,x0,y0,z3,x3,y3,z4,x4,z5,x5,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    JMP             _done                                                                                            \
+                                                                                                                     \
+  _block7:                                                                                                           \
+    SUBQ            $(64 * 8), t3                                                                                    \
+    KMOVQ           (t3), maskreg                                                                                    \
+    VMOVDQU8        (64*0)(t1), z1                                                                                   \
+    VMOVDQU8.Z      (64*1)(t1), maskreg, z2                                                                          \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash7          (keyData,z0,z3,z4,z5,z6,z7,z10,z11,z12,y12,z1,z2,HashKey_7)                                      \
+    ghashReduce     (aadHash,z0,x0,y0,z3,x3,y3,z4,x4,z5,x5,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    JMP             _done                                                                                            \
+                                                                                                                     \
+  _block6:                                                                                                           \
+    SUBQ            $(64 * 8), t3                                                                                    \
+    KMOVQ           (t3), maskreg                                                                                    \
+    VMOVDQU8        (64*0)(t1), z1                                                                                   \
+    VMOVDQU8.Z      (64*1)(t1), maskreg, z2                                                                          \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash6          (keyData,z0,z5,z6,z7,z8,y8,z9,y9,z10,y10,z11,y11,z12,y12,z1,y2,HashKey_6)                        \
+    ghashReduce     (aadHash,z0,x0,y0,z5,x5,y5,z6,x6,z7,x7,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    JMP             _done                                                                                            \
+                                                                                                                     \
+  _block5:                                                                                                           \
+    SUBQ            $(64 * 8), t3                                                                                    \
+    KMOVQ           (t3), maskreg                                                                                    \
+    VMOVDQU8        (64*0)(t1), z1                                                                                   \
+    VMOVDQU8.Z      (64*1)(t1), maskreg, z2                                                                          \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+	  VPSHUFB					shuffleMask, z2, z2   																		                                       \
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash5          (keyData,z0,z5,z6,z7,z8,x8,z9,x9,z10,x10,z11,x11,z12,x12,z1,x2,HashKey_5)                        \
+    ghashReduce     (aadHash,z0,x0,y0,z5,x5,y5,z6,x6,z7,x7,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    JMP             _done                                                                                            \
+                                                                                                                     \
+  _block4:                                                                                                           \
+    KMOVQ           (t3), maskreg                                                                                    \
+    VMOVDQU8.Z      (64*0)(t1), maskreg, z1                                                                          \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash4          (keyData,z0,z3,z4,z5,z12,z1,HashKey_4)                                                           \
+    ghashReduce     (aadHash,z0,x0,y0,z3,x3,y3,z4,x4,z5,x5,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    JMP             _done                                                                                            \
+                                                                                                                     \
+  _block3:                                                                                                           \
+    KMOVQ           (t3), maskreg                                                                                    \
+    VMOVDQU8.Z      (64*0)(t1), maskreg, z1                                                                          \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash3          (keyData,z0,z3,z4,z5,z12,y12,z1,HashKey_3)                                                       \
+    ghashReduce     (aadHash,z0,x0,y0,z3,x3,y3,z4,x4,z5,x5,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    JMP             _done                                                                                            \
+                                                                                                                     \ 
+  _block2:                                                                                                           \
+    KMOVQ           (t3), maskreg                                                                                    \
+    VMOVDQU8.Z      (64*0)(t1), maskreg, z1                                                                          \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash2          (keyData,y0,y3,y4,y5,y12,y1,HashKey_2)                                                           \
+    ghashReduce     (aadHash,z0,x0,y0,z3,x3,y3,z4,x4,z5,x5,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    JMP             _done                                                                                            \
+                                                                                                                     \
+  _block1:                                                                                                           \
+    KMOVQ           (t3), maskreg                                                                                    \
+    VMOVDQU8.Z      (64*0)(t1), maskreg, z1                                                                          \
+	  VPSHUFB					shuffleMask, z1, z1   																		                                       \ 
+    VPXORQ          aadHashZ, z1, z1																		                                             \
+    ghash1          (keyData,x0,x3,x4,x5,x12,x1,HashKey_1)                                                           \
+    ghashReduce     (aadHash,z0,x0,y0,z3,x3,y3,z4,x4,z5,x5,z10,x10,y10,z11,x11,y11,x12,hXorMask)                     \
+    JMP             _done                                                                                            \
+                                                                                                                     \
+  _done:            /* GHASH(aad) is returned in aadHash */
+
+/*
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+;;; GCM_INIT initializes a gcm_context_data struct to prepare for encoding/decoding.
+;;; Input: gcm_key_data * (GDATA_KEY), gcm_context_data *(GDATA_CTX), IV,
+;;; Additional Authentication data (A_IN), Additional Data length (A_LEN).
+;;; Output: Updated GDATA_CTX with the hash of A_IN (AadHash) and initialized other parts of GDATA_CTX.
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  keyData  - [in] GCM expanded keys pointer
+  ctxData  - [in] GCM context pointer
+  iv       - [in] IV pointer
+  aad      - [in] AAD pointer
+  aadLen   - [in] AAD length in bytes
+  gpr1..3  - [clobbered] GP registers 1..3
+  maskreg  - [clobbered] mask register
+  aadHash  - [out] XMM for AAD_HASH value (xmm14)
+  curCount - [out] XMM with current counter (xmm2)
+  z0..17   - [clobbered] ZMM registers 0..17
+  ivLen    - [in] IV length */
+
+// gcm context structure offsets
+// note: should move remaining go forced-alignment data fields to CTX struct
+#define ctxAadHash          (16*0)
+#define ctxAadLen           (16*1)
+#define ctxInLen            (16*1 + 8)
+#define ctxPBlockEncKey     (16*2)
+#define ctxOrigIV           (16*3)
+#define ctxCurCount         (16*4)
+#define ctxPBLen            (16*5)
+// remaining fields do not exist in C IPSEC library
+// required for go to compensate for lack of 
+// 64-bit alignment directive in go ASM
+// note: need 64-bit asm data alignment workaround to eliminate go-level alignment
+#define ctxDdqAddBE4444Ptr  (16*5 + 8)
+#define ctxDdqAddBE1234Ptr  (16*6)
+#define ctxShuffleMaskPtr   (16*6 + 8)
+#define byteLen2MaskPtr     (16*7)
+
+// calculate current count for standard IV length of 12 bytes
+#define CalcCC(key,ctx,iv,ivLen,curCountZ,curCount,z0,x0,y0,z1,x1,y1,z2,x2,y2,z3,x3,y3,z4,x4,y4,z5,x5,y5,z6,x6,z7,x7,z8,x8,y8,z9,x9,y9,z10,x10,y10,z11,x11,y11,z12,x12,y12,z14,z15,z16,z17,gpr1,gpr2,gpr3,maskreg1,maskreg2,shuffleMask) \
+  /* read 12 IV bytes and pad with 0x00000001 */                                     \
+  VMOVDQU8        oneF<>(SB), curCount                                               \
+  MOVQ            iv, gpr2                                                           \
+  MOVQ            $0x0000000000000fff, gpr1                                          \
+  KMOVQ           gpr1, maskreg1                                                     \                                                 
+  VMOVDQU8        (gpr2), maskreg1, curCount      /* ctr = IV | 0x1 */
+
+// calculate J0 for non-standard IV length
+#define CalcJ0(key,ctx,iv,ivLen,j0,j0x,z0,x0,y0,z1,x1,y1,z2,x2,y2,z3,x3,y3,z4,x4,y4,z5,x5,y5,z6,x6,z7,x7,z8,x8,y8,z9,x9,y9,z10,x10,y10,z11,x11,y11,z12,x12,y12,z14,z15,z16,z17,t1,t2,t3,mask1,mask2,shuffleMask) \
+  /* J0 = GHASH(IV || 0s+64 || len(IV)64) */                                         \
+  /* s = 16 * RoundUp(len(IV)/16) - len(IV) */                                       \
+  /* calculate GHASH of (IV || 0s) */                                                \
+  VPXORQ                j0x, j0x, j0x                                                \
+  CalcAADHash3          (iv,ivLen,j0x,j0,key,ctx,z0,x0,y0,z1,x1,y1,z2,x2,y2,z3,x3,y3,z4,x4,y4,z5,x5,y5,z6,x6,z7,x7,z8,x8,y8,z9,x9,y9,z10,x10,y10,z11,x11,y11,z12,x12,y12,shuffleMask,z14,z15,z16,z17,t1,t2,t3,mask1,mask2) \
+  /* calculate GHASH of last 16-byte block */                                        \
+  MOVQ                  ivLen, t1                                                    \
+  SHLQ                  $3, t1                                                       \
+  VMOVQ                 t1, x2                                                       \
+  VPXORQ                j0, z2, z2                                                   \
+  VPXORQ                z7, z7, z7                                                   \
+  VPXORQ                z6, z6, z6                                                   \
+  VPXORQ                z5, z5, z5                                                   \
+  vclMulStep2Block1     (key,z1,x1,y1,z2,x2,y2,z0,z3,x3,y3,z4,x4,y4,z7,z6,z5)        \
+  /* reduce */                                                                       \
+  VMOVDQA64             poly2<>(SB), x8                                              \
+  vclmulReduce          (j0x,x8,x1,x2,x0,x3)                                         \
+  VPSHUFB               shufMaskU<>(SB), j0x, j0x
+
+#define GCMInit(keyData,ctxData,iv,aad,aadLen,gpr1,gpr2,gpr3,maskreg1,maskreg2,aadHash,aadHashZ,curCount,curCountZ,z0,x0,y0,z1,x1,y1,z2,x2,y2,z3,x3,y3,z4,x4,y4,z5,x5,y5,z6,x6,z7,x7,z8,x8,y8,z9,x9,y9,z10,x10,y10,z11,x11,y11,z12,x12,y12,z13,z14,z15,z16,z17,ivLen,CalcCurCount,CalcAADHash) \
+  MOVQ            ctxShuffleMaskPtr(contextData), gpr2                               \
+  VMOVDQA64       (64*0)(gpr2), shuffleMask                                          \
+	VPXOR						aadHash, aadHash, aadHash                                          \
+	CalcAADHash	  	(aad,aadLen,aadHash,aadHashZ,keyData,ctxData,z0,x0,y0,z1,x1,y1,z2,x2,y2,z3,x3,y3,z4,x4,y4,z5,x5,y5,z6,x6,z7,x7,z8,x8,y8,z9,x9,y9,z10,x10,y10,z11,x11,y11,z12,x12,y12,shuffleMask,z14,z15,z16,z17,gpr1,gpr2,gpr3,maskreg1,maskreg2) \
+  MOVQ            aadLen, gpr1                                                       \
+  VMOVDQU64       aadHash, ctxAadHash(ctxData)    /* ctx.aad hash = aad_hash      */ \
+  MOVQ            gpr1, ctxAadLen(ctxData)        /* ctx.aad_length = aad_length  */ \
+  XORQ            gpr1, gpr1                                                         \
+  MOVQ            gpr1, ctxInLen(ctxData)         /* ctx.in_length = 0            */ \
+  MOVQ            gpr1, ctxPBLen(ctxData)         /* ctx.partial_block_length = 0 */ \
+  CalcCurCount    (keyData,ctxData,iv,ivLen,curCountZ,curCount,z0,x0,y0,z1,x1,y1,z2,x2,y2,z3,x3,y3,z4,x4,y4,z5,x5,y5,z6,x6,z7,x7,z8,x8,y8,z9,x9,y9,z10,x10,y10,z11,x11,y11,z12,x12,y12,z14,z15,z16,z17,gpr1,gpr2,gpr3,maskreg1,maskreg2,shuffleMask) \
+  VMOVDQU64       curCount, ctxOrigIV(ctxData)    /* ctx.orig_IV = iv */             \
+                                                                                     \
+  /* store IV as counter in LE format */                                             \
+	VPSHUFB					shufMaskU<>(SB), curCount, curCount   													   \
+  VMOVDQU         curCount, ctxCurCount(ctxData)  /* ctx.current_counter = iv */     \
+  GCMInitSP64
+
+// main encrypt/decrypt pipeline register usages for small/medium/large blocks 
+// these names and usages are aligned to ipsec almost completely with a few exceptions
+// note: explore compromises trading off fewer explicit macro parameters with more in-scope #defines
+#define ptr                      R10
+#define ia0                      R10
+#define dataOffset               R11
+#define ia1                      R12      
+#define hashkPtr                 AX
+#define dataLen                  AX
+#define length                   R13
+#define ctrCheck                 R15
+#define ia3                      R15
+#define xt0                      X0
+#define blk0x                    X1
+#define xt18                     X1
+#define xt19                     X2
+#define gcmInitCtrBlock          X2
+#define blk1x                    X2
+#define xt1                      X3
+#define xt2                      X4
+#define xt3                      X5
+#define xt4                      X6
+#define xt5                      X7
+#define aesPartialBlock          X8
+#define xt20                     X8
+#define ctrBlockx                X9
+#define xt6                      X10
+#define xt7                      X11 
+#define xt8                      X12
+#define xt9                      X13
+#define aadHashx                 X14
+#define xt10                     X15
+#define xt11                     X16
+#define redPoly                  X16
+#define xt12                     X17
+#define xt13                     X19
+#define xt14                     X20
+#define xt15                     X21
+#define ctrBlockSavex            X22
+#define shuffleMaskx             X29
+#define xt16                     X30 
+#define xt17                     X31
+#define yt0                      Y0
+#define yt18                     Y1
+#define blk0y                    Y1
+#define blk1y                    Y2
+#define yt19                     Y2
+#define yt1                      Y3
+#define yt2                      Y4
+#define yt3                      Y5
+#define yt4                      Y6
+#define yt5                      Y7
+#define yt20                     Y8
+#define ctrBlocky                Y9
+#define yt6                      Y10
+#define yt7                      Y11
+#define yt8                      Y12
+#define yt9                      Y13
+#define yt10                     Y15
+#define yt11                     Y16
+#define yt12                     Y17
+#define yt13                     Y19
+#define yt14                     Y20
+#define yt15                     Y21
+#define add1234y                 Y28           
+#define shuffleMasky             Y29
+#define yt16                     Y30 
+#define yt17                     Y31
+#define zt0                      Z0
+#define zt18                     Z1
+#define blk0                     Z1
+#define zt19                     Z2
+#define blk1                     Z2
+#define zt1                      Z3
+#define zt2                      Z4
+#define zt3                      Z5
+#define zt4                      Z6
+#define zt5                      Z7
+#define zt20                     Z8
+#define maskOutTopBlock          Z8 
+#define ctrBlockz                Z9
+#define zt6                      Z10
+#define zt7                      Z11
+#define zt8                      Z12
+#define zt9                      Z13
+#define aadHashz                 Z14
+#define zt10                     Z15
+#define zt11                     Z16
+#define zt12                     Z17
+#define ctrBlock2z               Z18 
+#define zt13                     Z19
+#define add8888                  Z19 
+#define zt14                     Z20
+#define addBE8888                Z20
+#define zt15                     Z21
+#define zt21                     Z22
+#define ctrBlockSave             Z22       
+#define zt22                     Z23
+#define addLE1234                zt22
+#define gh                       Z24
+#define gl                       Z25
+#define gm                       Z26
+#define addBE4x4                 Z27
+#define add4x4                   Z27
+#define add5678                  Z27
+#define addBE1234                Z28
+#define add1234                  Z28 
+#define shuffleMask              Z29
+#define zt16                     Z30 
+#define gh8Key                   Z30 
+#define zt17                     Z31
+#define gh4Key                   Z31 
+#define bigLoopNBlocks           48
+#define bigLoopDepth             32
+#define cipherBlk                (16*(bigLoopNBlocks-bigLoopDepth))
+
+#define aesRound(i,kp,b0,b1,b2,b3,t) \
+  VBROADCASTF64X2       (16*i)(kp), t                                                    \
+  VAESENC               t, b0, b0                                                        \
+  VAESENC               t, b1, b1                                                        \
+  VAESENC               t, b2, b2                                                        \
+  VAESENC               t, b3, b3   
+
+#define aesRoundLast(i,kp,b0,b1,b2,b3,t) \
+  VBROADCASTF64X2       (16*i)(kp), t                                                    \
+  VAESENCLAST           t, b0, b0                                                        \
+  VAESENCLAST           t, b1, b1                                                        \
+  VAESENCLAST           t, b2, b2                                                        \
+  VAESENCLAST           t, b3, b3   
+
+// NOPs replace reductions for ipsec cases "first_time" and "no_reduction"
+#define ghashNoReduce1(rl,rh,l1,l2,h1,h2,m1,m2)
+#define ghashNoReduce2(h1,h1x,h1y,h2,h2x,h2y,l1,l1x,l1y,l2,l2x,l2y,kn) 
+#define ghashNoReduce3(redP1,redPoly,gh1lx) 
+#define ghashNoReduce4(gh1hx,redT1,redT2,redP1,redPoly) 
+
+// for ipsec case "final_reduction", add mid products to high and low
+#define ghashReduce1(rl,rh,l1,l2,h1,h2,m1,m2) \
+  VPTERNLOGQ            $0x96, m2, h2, h1                                                \
+  VPXORQ                rh, h1, h1             /* h1 = h1+h2+m2>>64 */                   \
+  VPTERNLOGQ            $0x96, m1, l2, l1                                                \
+  VPXORQ                rl, l1, l1             /* l1 = l1+l2+m1>>64 */   
+
+// for ipsec case "final_reduction", horizontal xor low and high 4x128
+#define ghashReduce2(h1,h1x,h1y,h2,h2x,h2y,l1,l1x,l1y,l2,l2x,l2y,kn) \
+  vhpxori4x128(h1,h1x,h1y,h2,h2x,h2y,kn)                                                 \
+  vhpxori4x128(l1,l1x,l1y,l2,l2x,l2y,kn)                 
+
+// for ipsec case "final_reduction", reduction phase 1
+#define ghashReduce3(redP1,redPoly,gh1lx) \
+  VPCLMULQDQ            $0x01, gh1lx, redPoly, redP1                                     \
+  VPSLLDQ               $8, redP1, redP1               /* shift-L 2 DWs */               \
+  VPXORQ                redP1, gh1lx, redP1            /* reduce */  
+
+// for ipsec case "final_reduction", reduction phase 2
+#define ghashReduce4(gh1hx,redT1,redT2,redP1,redPoly) \
+  VPCLMULQDQ            $0x00, redP1, redPoly, redT1                                     \
+  VPSRLDQ               $4, redT1, redT1                                                 \
+  VPCLMULQDQ            $0x10, redP1, redPoly, redT2                                     \
+  VPSLLDQ               $4, redT2, redT2                                                 \
+  VPTERNLOGQ            $0x96, redT1, redT2, gh1hx    /* g1hx = g1hx x redT1 x redT2 */
+
+// ghash16Enc16Parallel gather op for ipsec "first_time"
+#define ghashGatherFirst(m1,m2,h1,h2,l1,l2,t1,t2,rl,rh,rm,redPoly) \
+  VPTERNLOGQ            $0x96, t1, t2, m1                                                \            
+  VPXORQ                m2, m1, rm                                                       \
+  VPXORQ                h2, h1, rh                                                       \
+  VPXORQ                l2, l1, rl                                                                  
+
+// ghash16EncParallel gather op for ipsec "no_reduction"
+#define ghashGather(m1,m2,h1,h2,l1,l2,t1,t2,rl,rh,rm,redPoly) \
+  VPTERNLOGQ            $0x96, t1, t2, m1                                                \            
+  VPTERNLOGQ            $0x96, m2, m1, rm                                                \
+  VPTERNLOGQ            $0x96, h2, h1, rh                                                \
+  VPTERNLOGQ            $0x96, l2, l1, rl                                                           
+
+// ghash16EncParallel gather op for ipsec "final_reduction"
+#define ghashGatherFinal(m1,m2,h1,h2,l1,l2,t1,t2,rl,rh,rm,redPoly) \
+  /* phase 1: add mid products */                                                        \
+  /* and load polynomial constant for reduction */                                       \
+  VPTERNLOGQ            $0x96, t2, t1, m1                                                \            
+  VPTERNLOGQ            $0x96, m2, rm, m1                                                \
+  VPSRLDQ               $8, m1, m2                                                       \
+  VPSLLDQ               $8, m1, m1                                                       \
+  VMOVDQA64				      poly2<>(SB), redPoly
+
+// ghash16EncParallel op for ipsec "no_ghash_in"
+#define ghashInNone(ghdat,ghin,ghashinBlkOffset,SP64) \
+  VMOVDQA64             (64*0 + ghashinBlkOffset)(SP64), ghdat
+
+// ghash16EncParallel op for ipsec a ghash input (ghin)
+#define ghashInCurrent(ghdat,ghin,ghashinBlkOffset,SP64) \
+  VPXORQ                (64*0 + ghashinBlkOffset)(SP64), ghin, ghdat
+
+// large block [768,...arbitrarily large, excluding fragments legnth-int(length/768)*768] encrypt/decrypt-specific shuffles for ghash16
+#define ghashShuffleEncBB(b0003,b0407,b0811,b1215,data1,data2,data3,data4,shuffleMask) \
+  VPSHUFB               shuffleMask, b0003, b0003                                                                      \
+  VPSHUFB               shuffleMask, b0407, b0407                                                                      \
+  VPSHUFB               shuffleMask, b0811, b0811                                                                      \
+  VPSHUFB               shuffleMask, b1215, b1215
+
+#define ghashShuffleDecBB(b0003,b0407,b0811,b1215,data1,data2,data3,data4,shuffleMask) \
+  VPSHUFB               shuffleMask, data1, b0003                                                                      \
+  VPSHUFB               shuffleMask, data2, b0407                                                                      \
+  VPSHUFB               shuffleMask, data3, b0811                                                                      \
+  VPSHUFB               shuffleMask, data4, b1215
+
+#define ghash16Key128(gdata,aeskey) 
+
+#define ghash16Key192256(gdata,aeskey) \
+  VBROADCASTF64X2       (16*11)(gdata), aeskey
+
+#define ghash16Aes128(b0003,b0407,b0811,b1215,aeskey1,aeskey2,gdata)
+
+#define ghash16AesExt(b0003,b0407,b0811,b1215,aeskey1,aeskey2,gdata) \
+  VAESENC               aeskey1, b0003, b0003                                                                          \
+  VAESENC               aeskey1, b0407, b0407                                                                          \
+  VAESENC               aeskey1, b0811, b0811                                                                          \
+  VAESENC               aeskey1, b1215, b1215                                                                          \
+  VBROADCASTF64X2       (16*12)(gdata), aeskey1                                                                        \
+  VAESENC               aeskey2, b0003, b0003                                                                          \
+  VAESENC               aeskey2, b0407, b0407                                                                          \
+  VAESENC               aeskey2, b0811, b0811                                                                          \
+  VAESENC               aeskey2, b1215, b1215
+
+#define ghash16Aes192(b0003,b0407,b0811,b1215,aeskey1,aeskey2,gdata) \
+  ghash16AesExt         (b0003,b0407,b0811,b1215,aeskey1,aeskey2,gdata)
+
+#define ghash16Aes256(b0003,b0407,b0811,b1215,aeskey1,aeskey2,gdata) \
+  ghash16AesExt         (b0003,b0407,b0811,b1215,aeskey1,aeskey2,gdata)                                                \
+  VBROADCASTF64X2       (16*13)(gdata), aeskey2                                                                        \
+  VAESENC               aeskey1, b0003, b0003                                                                          \
+  VAESENC               aeskey1, b0407, b0407                                                                          \
+  VAESENC               aeskey1, b0811, b0811                                                                          \
+  VAESENC               aeskey1, b1215, b1215                                                                          \
+  VBROADCASTF64X2       (16*14)(gdata), aeskey1                                                                        \
+  VAESENC               aeskey2, b0003, b0003                                                                          \
+  VAESENC               aeskey2, b0407, b0407                                                                          \
+  VAESENC               aeskey2, b0811, b0811                                                                          \
+  VAESENC               aeskey2, b1215, b1215
+
+// Main GCM macro stitching cipher with GHASH
+// - operates on single stream
+// - encrypts 16 blocks at a time
+// - ghash the 16 previously encrypted ciphertext blocks
+// - no partial block or multi_call handling here
+#define ghash16Enc16Parallel(gdata,out,in,dataOffset,ctrBE,ctrCheck,hashkeyOffset,aesBlkOffset,ghashinBlkOffset,shuffleMask,b0003,b0407,b0811,b1215,gh1h,gh1hx,gh1hy,gh1l,gh1lx,gh1ly,gh1m,gh1t,gh2h,gh2hx,gh2hy,gh2l,gh2lx,gh2ly,gh2m,gh2mx,gh2t,gh3h,gh3l,gh3m,gh3t,aeskey1,aeskey2,ghkey1,ghkey2,ghdat1,ghdat2,addBE4x4,addBE1234,t0ReduceL,t0ReduceH,t0ReduceM,dataDispl,ghashIn,SP64,overflow,noOverflow,gather,redPoly,getGhashInput,ghashReduceAddMid,ghashReduceHXOR,ghashReduceP1,ghashReduceP2,hXorMask,shuffleOp,aesExtKeys,aesExtRounds) \
+  /* prepare counter blocks */                                                                                         \
+  CMPB                  ctrCheck, $(256-16)                                                                            \
+  JAE                   overflow                                                                                       \
+  VPADDD                addBE1234, ctrBE, b0003                                                                        \
+  VPADDD                addBE4x4, b0003, b0407                                                                         \
+  VPADDD                addBE4x4, b0407, b0811                                                                         \
+  VPADDD                addBE4x4, b0811, b1215                                                                         \
+  JMP                   noOverflow                                                                                     \
+  overflow:                                                                                                            \
+  VPSHUFB               shuffleMask, ctrBE, ctrBE                                                                      \
+  /* generate LE 4x4 from BE using VPSHUFB; ipsec uses load from constant */                                           \
+  /* Go does not guarantee 64-bit aligned ASM constants therefore aligned load costs 2 loads w/ Go-managed table */    \
+  /* one to load aligned pointer, the second to load the data; instead VPSHUFB BE constant already in zmm */           \
+  VPSHUFB               shuffleMask, addBE4x4, b1215                                                                   \
+  VPSHUFB               shuffleMask, addBE1234, addLE1234                                                              \
+  VPADDD                addLE1234, ctrBE, b0003                                                                        \
+  VPADDD                b1215, b0003, b0407                                                                            \
+  VPADDD                b1215, b0407, b0811                                                                            \
+  VPADDD                b1215, b0811, b1215                                                                            \
+  VPSHUFB               shuffleMask, b0003, b0003                                                                      \
+  VPSHUFB               shuffleMask, b0407, b0407                                                                      \
+  VPSHUFB               shuffleMask, b0811, b0811                                                                      \
+  VPSHUFB               shuffleMask, b1215, b1215                                                                      \
+  noOverflow:                                                                                                          \
+  /* preload constants */                                                                                              \
+  VBROADCASTF64X2       (16*0)(gdata), aeskey1                                                                         \
+  getGhashInput         (ghdat1,ghashIn,ghashinBlkOffset,SP64)                                                         \
+  /* VMOVDQA64             (64*0 + ghashinBlkOffset)(SP64), ghdat1  */                                                 \
+  VMOVDQU64             (64*0 + hashkeyOffset)(gdata), ghkey1                                                          \
+  /* save counter for next round */                                                                                    \
+  VSHUFI64X2            $0xff, b1215, b1215, ctrBE                                                                     \
+  ADDB                  $16, ctrCheck                                                                                  \
+  /* preload constants */                                                                                              \
+  VBROADCASTF64X2       (16*1)(gdata), aeskey2                                                                         \
+  VMOVDQU64             (64*1 + hashkeyOffset)(gdata), ghkey2                                                          \
+  VMOVDQA64             (64*1 + ghashinBlkOffset)(SP64), ghdat2                                                        \
+  /* stitch AES rounds with GHASH */                                                                                   \
+  /* AES round 0                  */                                                                                   \
+  VPXORQ                aeskey1, b0003, b0003                                                                          \
+  VPXORQ                aeskey1, b0407, b0407                                                                          \
+  VPXORQ                aeskey1, b0811, b0811                                                                          \
+  VPXORQ                aeskey1, b1215, b1215                                                                          \
+  VBROADCASTF64X2       (16*2)(gdata), aeskey1                                                                         \
+  /* GHASH 4 blocks (15 to 12) */                                                                                      \
+  VPCLMULQDQ            $0x11, ghkey1, ghdat1, gh1h      /* a1*b1 */                                                   \
+  VPCLMULQDQ            $0x00, ghkey1, ghdat1, gh1l      /* a0*b0 */                                                   \
+  VPCLMULQDQ            $0x01, ghkey1, ghdat1, gh1m      /* a1*b0 */                                                   \
+  VPCLMULQDQ            $0x10, ghkey1, ghdat1, gh1t      /* a0*b1 */                                                   \
+  VMOVDQU64             (64*2 + hashkeyOffset)(gdata), ghkey1                                                          \
+  VMOVDQA64             (64*2 + ghashinBlkOffset)(SP64), ghdat1                                                        \
+  /* AES round 1                  */                                                                                   \
+  VAESENC               aeskey2, b0003, b0003                                                                          \
+  VAESENC               aeskey2, b0407, b0407                                                                          \
+  VAESENC               aeskey2, b0811, b0811                                                                          \
+  VAESENC               aeskey2, b1215, b1215                                                                          \
+  VBROADCASTF64X2       (16*3)(gdata), aeskey2                                                                         \
+  /* GHASH 4 blocks (11 to 8) */                                                                                       \
+  VPCLMULQDQ            $0x10, ghkey2, ghdat2, gh2m      /* a0*b1 */                                                   \
+  VPCLMULQDQ            $0x01, ghkey2, ghdat2, gh2t      /* a1*b0 */                                                   \
+  VPCLMULQDQ            $0x11, ghkey2, ghdat2, gh2h      /* a1*b1 */                                                   \
+  VPCLMULQDQ            $0x00, ghkey2, ghdat2, gh2l      /* a0*b0 */                                                   \
+  VMOVDQU64             (64*3 + hashkeyOffset)(gdata), ghkey2                                                          \
+  VMOVDQA64             (64*3 + ghashinBlkOffset)(SP64), ghdat2                                                        \
+  /* AES round 2 */                                                                                                    \
+  VAESENC               aeskey1, b0003, b0003                                                                          \
+  VAESENC               aeskey1, b0407, b0407                                                                          \
+  VAESENC               aeskey1, b0811, b0811                                                                          \
+  VAESENC               aeskey1, b1215, b1215                                                                          \
+  VBROADCASTF64X2       (16*4)(gdata), aeskey1                                                                         \
+  /* GHASH 4 blocks (7 to 4) */                                                                                        \
+  VPCLMULQDQ            $0x10, ghkey1, ghdat1, gh3m      /* a0*b1 */                                                   \
+  VPCLMULQDQ            $0x01, ghkey1, ghdat1, gh3t      /* a1*b0 */                                                   \
+  VPCLMULQDQ            $0x11, ghkey1, ghdat1, gh3h      /* a1*b1 */                                                   \
+  VPCLMULQDQ            $0x00, ghkey1, ghdat1, gh3l      /* a0*b0 */                                                   \
+  /* AES round 3 */                                                                                                    \
+  VAESENC               aeskey2, b0003, b0003                                                                          \
+  VAESENC               aeskey2, b0407, b0407                                                                          \
+  VAESENC               aeskey2, b0811, b0811                                                                          \
+  VAESENC               aeskey2, b1215, b1215                                                                          \
+  VBROADCASTF64X2       (16*5)(gdata), aeskey2                                                                         \
+  /* gather (xor) GHASH for 12 blocks */                                                                               \
+  VPTERNLOGQ            $0x96, gh3h, gh2h, gh1h                                                                        \
+  VPTERNLOGQ            $0x96, gh3l, gh2l, gh1l                                                                        \
+  VPTERNLOGQ            $0x96, gh3t, gh2t, gh1t                                                                        \
+  VPTERNLOGQ            $0x96, gh3m, gh2m, gh1m                                                                        \
+  /* AES round 4 */                                                                                                    \
+  VAESENC               aeskey1, b0003, b0003                                                                          \
+  VAESENC               aeskey1, b0407, b0407                                                                          \
+  VAESENC               aeskey1, b0811, b0811                                                                          \
+  VAESENC               aeskey1, b1215, b1215                                                                          \
+  VBROADCASTF64X2       (16*6)(gdata), aeskey1                                                                         \
+  /* load next 16 input blocks; recycle ghxx registers */                                                              \
+  VX512LDR              (64*0+dataDispl)(dataOffset)(in*1), gh3h                                                       \
+  VX512LDR              (64*1+dataDispl)(dataOffset)(in*1), gh3l                                                       \
+  VX512LDR              (64*2+dataDispl)(dataOffset)(in*1), gh3m                                                       \
+  VX512LDR              (64*3+dataDispl)(dataOffset)(in*1), gh3t                                                       \
+  /* AES round 5                  */                                                                                   \
+  VAESENC               aeskey2, b0003, b0003                                                                          \
+  VAESENC               aeskey2, b0407, b0407                                                                          \
+  VAESENC               aeskey2, b0811, b0811                                                                          \
+  VAESENC               aeskey2, b1215, b1215                                                                          \
+  VBROADCASTF64X2       (16*7)(gdata), aeskey2                                                                         \
+  /* GHASH 4 blocks (3 to 0) */                                                                                        \
+  VPCLMULQDQ            $0x10, ghkey2, ghdat2, gh2m      /* a0*b1 */                                                   \
+  VPCLMULQDQ            $0x01, ghkey2, ghdat2, gh2t      /* a1*b0 */                                                   \
+  VPCLMULQDQ            $0x11, ghkey2, ghdat2, gh2h      /* a1*b1 */                                                   \
+  VPCLMULQDQ            $0x00, ghkey2, ghdat2, gh2l      /* a0*b0 */                                                   \
+  /* AES round 6                  */                                                                                   \
+  VAESENC               aeskey1, b0003, b0003                                                                          \
+  VAESENC               aeskey1, b0407, b0407                                                                          \
+  VAESENC               aeskey1, b0811, b0811                                                                          \
+  VAESENC               aeskey1, b1215, b1215                                                                          \
+  VBROADCASTF64X2       (16*8)(gdata), aeskey1                                                                         \
+  /* gather GHASH in gh1l (low) and gh1h (high) */                                                                     \
+  gather                (gh1m,gh2m,gh1h,gh2h,gh1l,gh2l,gh1t,gh2t,t0ReduceL,t0ReduceH,t0ReduceM,redPoly)                \
+  /* AES round 7                  */                                                                                   \
+  VAESENC               aeskey2, b0003, b0003                                                                          \
+  VAESENC               aeskey2, b0407, b0407                                                                          \
+  VAESENC               aeskey2, b0811, b0811                                                                          \
+  VAESENC               aeskey2, b1215, b1215                                                                          \
+  VBROADCASTF64X2       (16*9)(gdata), aeskey2                                                                         \
+  /* add mid products to high and low (if ipsec "final_reduce" otherwise NOP) */                                       \
+  ghashReduceAddMid     (t0ReduceL,t0ReduceH,gh1l,gh2l,gh1h,gh2h,gh1m,gh2m)                                            \
+  /* AES round 8                  */                                                                                   \
+  VAESENC               aeskey1, b0003, b0003                                                                          \
+  VAESENC               aeskey1, b0407, b0407                                                                          \
+  VAESENC               aeskey1, b0811, b0811                                                                          \
+  VAESENC               aeskey1, b1215, b1215                                                                          \
+  VBROADCASTF64X2       (16*10)(gdata), aeskey1                                                                        \ 
+  /* horizontal XOR of low and high 4x128 into gh1h and gh1l(if ipsec "final_reduce" otherwise NOP) */                 \
+  ghashReduceHXOR       (gh1h,gh1hx,gh1hy,gh2h,gh2hx,gh2hy,gh1l,gh1lx,gh1ly,gh2l,gh2lx,gh2ly,hXorMask)                 \
+  /* AES round 9                  */                                                                                   \
+  VAESENC               aeskey2, b0003, b0003                                                                          \
+  VAESENC               aeskey2, b0407, b0407                                                                          \
+  VAESENC               aeskey2, b0811, b0811                                                                          \
+  VAESENC               aeskey2, b1215, b1215                                                                          \
+  /* load AES round 11 key (AES192 or AES256), or NOP (AES128) */                                                      \
+  aesExtKeys            (gdata,aeskey2)                                                                                \                     
+  ghashReduceP1         (gh2lx,redPoly,gh1lx)                                                                          \
+  /* AES rounds to 11 (AES192), AES rounds to 13 (AES256), otherwise NOP (AES128) */                                   \
+  aesExtRounds          (b0003,b0407,b0811,b1215,aeskey1,aeskey2,gdata)                                                \
+  ghashReduceP2         (gh1hx,gh2hx,gh2mx,gh2lx,redPoly)                                                              \
+  /* AES round 10/12/14 for 128/192/256*/                                                                              \
+  VAESENCLAST           aeskey1, b0003, b0003                                                                          \
+  VAESENCLAST           aeskey1, b0407, b0407                                                                          \
+  VAESENCLAST           aeskey1, b0811, b0811                                                                          \
+  VAESENCLAST           aeskey1, b1215, b1215                                                                          \
+  /* XOR against input */                                                                                              \
+  VPXORQ                gh3h, b0003, b0003                                                                             \
+  VPXORQ                gh3l, b0407, b0407                                                                             \
+  VPXORQ                gh3m, b0811, b0811                                                                             \
+  VPXORQ                gh3t, b1215, b1215                                                                             \
+  /* store output */                                                                                                   \
+  VX512STR              b0003, (64*0+dataDispl)(dataOffset)(out*1)                                                     \
+  VX512STR              b0407, (64*1+dataDispl)(dataOffset)(out*1)                                                     \
+  VX512STR              b0811, (64*2+dataDispl)(dataOffset)(out*1)                                                     \
+  VX512STR              b1215, (64*3+dataDispl)(dataOffset)(out*1)                                                     \
+  /* shuffle data blocks for GHASH */                                                                                  \
+  shuffleOp             (b0003,b0407,b0811,b1215,gh3h,gh3l,gh3m,gh3t,shuffleMask)                                      \
+  /* store shuffled data for GHASH */                                                                                  \
+  VMOVDQA64             b0003, (64*0+aesBlkOffset)(SP64)                                                               \
+  VMOVDQA64             b0407, (64*1+aesBlkOffset)(SP64)                                                               \
+  VMOVDQA64             b0811, (64*2+aesBlkOffset)(SP64)                                                               \
+  VMOVDQA64             b1215, (64*3+aesBlkOffset)(SP64)                                                                
+
+#define ghashEncNx16Parallel(in,out,gdataKey,dataOffset,ctrBE,shuffleMask,zt0,zt1,zt2,zt3,zt4,xt4,yt4,zt5,xt5,yt5,zt6,zt7,zt8,xt8,yt8,zt9,xt9,yt9,zt10,xt10,zt11,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,zt21,zt22,gth,gtl,gtm,addBE4x4,addBE1234,ghash,ctrCheck,redPoly,hXorMask,shuffleOp,extKey,extAes) \
+  ghash16Enc16Parallel  (gdataKey,out,in,dataOffset,ctrBE,ctrCheck,HashKey_32,0,256,shuffleMask,zt0,zt1,zt2,zt3,zt4,xt4,yt4,zt5,xt5,yt5,zt6,zt7,zt8,xt8,yt8,zt9,xt9,yt9,zt10,xt10,zt11,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,zt21,addBE4x4,addBE1234,gtl,gth,gtm,0,ghash,SP64,L6,L7,ghashGather,redPoly,ghashInNone,ghashNoReduce1,ghashNoReduce2,ghashNoReduce3,ghashNoReduce4,hXorMask,shuffleOp,extKey,extAes)              \
+  ghash16Enc16Parallel  (gdataKey,out,in,dataOffset,ctrBE,ctrCheck,HashKey_16,256,512,shuffleMask,zt0,zt1,zt2,zt3,zt4,xt4,yt4,zt5,xt5,yt5,zt6,zt7,zt8,xt8,yt8,zt9,xt9,yt9,zt10,xt10,zt11,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,zt21,addBE4x4,addBE1234,gtl,gth,gtm,256,ghash,SP64,L8,L9,ghashGatherFinal,redPoly,ghashInNone,ghashReduce1,ghashReduce2,ghashReduce3,ghashReduce4,hXorMask,shuffleOp,extKey,extAes)             \ 
+  /* xor cipher block 0 with GHASH */                                                                                                                                                                                                                                                                                                                                                                                                 \
+  VMOVDQA64             zt4, ghash                                                                                                                                                                                                                                                                                                                                                                                                    \
+  /* re-start the pipeline */                                                                                                                                                                                                                                                                                                                                                                                                         \
+  ghash16Enc16Parallel  (gdataKey,out,in,dataOffset,ctrBE,ctrCheck,HashKey_48,512,0,shuffleMask,zt0,zt1,zt2,zt3,zt4,xt4,yt4,zt5,xt5,yt5,zt6,zt7,zt8,xt8,yt8,zt9,xt9,yt9,zt10,xt10,zt11,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,zt21,addBE4x4,addBE1234,gtl,gth,gtm,512,ghash,SP64,L10,L11,ghashGatherFirst,redPoly,ghashInCurrent,ghashNoReduce1,ghashNoReduce2,ghashNoReduce3,ghashNoReduce4,hXorMask,shuffleOp,extKey,extAes)  \
+  ADDQ                  $(bigLoopNBlocks*16), dataOffset
+
+// large block [768,...,arbitrarily large] encrypt/decrypt-specific shuffles for initialBlocks16()
+#define shuffleEncBB(b0003,b0407,b0811,b1215,t0,t1,t2,t3,shuffleMask) \
+  VPSHUFB               shuffleMask, b0003, b0003                                                                      \
+  VPSHUFB               shuffleMask, b0407, b0407                                                                      \
+  VPSHUFB               shuffleMask, b0811, b0811                                                                      \
+  VPSHUFB               shuffleMask, b1215, b1215                                                                      
+
+#define shuffleDecBB(b0003,b0407,b0811,b1215,t0,t1,t2,t3,shuffleMask) \
+  VPSHUFB               shuffleMask, t0, b0003                                                                         \
+  VPSHUFB               shuffleMask, t1, b0407                                                                         \
+  VPSHUFB               shuffleMask, t2, b0811                                                                         \
+  VPSHUFB               shuffleMask, t3, b1215                                                                      
+
+#define aesExtLB128(kp,b0003,b0407,b0811,b1215,t) \
+  aesRoundLast          (10,kp,b0003,b0407,b0811,b1215,t)
+
+#define aesExtLB192(kp,b0003,b0407,b0811,b1215,t) \
+  aesRound              (10,kp,b0003,b0407,b0811,b1215,t)                                                              \
+  aesRound              (11,kp,b0003,b0407,b0811,b1215,t)                                                              \
+  aesRoundLast          (12,kp,b0003,b0407,b0811,b1215,t)
+
+#define aesExtLB256(kp,b0003,b0407,b0811,b1215,t) \
+  aesRound              (10,kp,b0003,b0407,b0811,b1215,t)                                                              \
+  aesRound              (11,kp,b0003,b0407,b0811,b1215,t)                                                              \
+  aesRound              (12,kp,b0003,b0407,b0811,b1215,t)                                                              \
+  aesRound              (13,kp,b0003,b0407,b0811,b1215,t)                                                              \
+  aesRoundLast          (14,kp,b0003,b0407,b0811,b1215,t)
+
+#define initialBlocks16(in,out,kp,dataOffset,ghash,ctr,ctrCheck,addBE4x4,addBE1234,t0,t1,t2,t3,t4,b0003,b0407,b0811,b1215,shuffleMask,blockOffset,dataDispl,SP64,overflow,noOverflow,inst,shuffleBlockOp,aesFinal) \
+  CMPB                  ctrCheck, $(256-16)                                                                            \
+  JAE                   overflow                                                                                       \
+  VPADDD                addBE1234, ctr, b0003                                                                          \
+  VPADDD                addBE4x4, b0003, b0407                                                                         \
+  VPADDD                addBE4x4, b0407, b0811                                                                         \
+  VPADDD                addBE4x4, b0811, b1215                                                                         \
+  JMP                   noOverflow                                                                                     \
+  /* note: CTR overflow test cases may not exist in go test suite */                                                   \
+  overflow:                                                                                                            \
+  VPSHUFB               shuffleMask, ctr, ctr                                                                          \ 
+  /* generate LE 4x4 from BE using VPSHUFB; ipsec uses load from constant */                                           \
+  /* go does not guarantee 64-bit aligned asm constants therefore aligned load costs 2 loads w/ go-managed table */    \
+  /* one to load aligned pointer, the second to load the data; instead VPSHUFB BE constant already in zmm */           \
+  VPSHUFB               shuffleMask, addBE4x4, b1215                                                                   \
+  VPSHUFB               shuffleMask, addBE1234, addLE1234                                                              \
+  VPADDD                addLE1234, ctr, b0003                                                                          \
+  VPADDD                b1215, b0003, b0407                                                                            \
+  VPADDD                b1215, b0407, b0811                                                                            \
+  VPADDD                b1215, b0811, b1215                                                                            \
+  VPSHUFB               shuffleMask, b0003, b0003                                                                      \
+  VPSHUFB               shuffleMask, b0407, b0407                                                                      \
+  VPSHUFB               shuffleMask, b0811, b0811                                                                      \
+  VPSHUFB               shuffleMask, b1215, b1215                                                                      \
+  noOverflow:                                                                                                          \
+  /* prepare counter blocks */                                                                                         \
+  VSHUFI64X2            $255, b1215, b1215, ctr                                                                        \
+  ADDB                  $16, ctrCheck                                                                                  \   
+  /* load 16 blocks of data */                                                                                         \
+  VX512LDR              (64*0+dataDispl)(dataOffset)(in*1), t0                                                         \
+  VX512LDR              (64*1+dataDispl)(dataOffset)(in*1), t1                                                         \
+  VX512LDR              (64*2+dataDispl)(dataOffset)(in*1), t2                                                         \
+  VX512LDR              (64*3+dataDispl)(dataOffset)(in*1), t3                                                         \
+  /* AES encryption rounds */                                                                                          \
+  VBROADCASTF64X2       (kp), t4                                                                                       \
+  VPXORQ                t4, b0003, b0003                                                                               \
+  VPXORQ                t4, b0407, b0407                                                                               \
+  VPXORQ                t4, b0811, b0811                                                                               \
+  VPXORQ                t4, b1215, b1215                                                                               \
+  aesRound              (1,kp,b0003,b0407,b0811,b1215,t4)                                                              \
+  aesRound              (2,kp,b0003,b0407,b0811,b1215,t4)                                                              \
+  aesRound              (3,kp,b0003,b0407,b0811,b1215,t4)                                                              \
+  aesRound              (4,kp,b0003,b0407,b0811,b1215,t4)                                                              \
+  aesRound              (5,kp,b0003,b0407,b0811,b1215,t4)                                                              \
+  aesRound              (6,kp,b0003,b0407,b0811,b1215,t4)                                                              \
+  aesRound              (7,kp,b0003,b0407,b0811,b1215,t4)                                                              \
+  aesRound              (8,kp,b0003,b0407,b0811,b1215,t4)                                                              \
+  aesRound              (9,kp,b0003,b0407,b0811,b1215,t4)                                                              \
+  /* aes final rounds to: 10 for AES128, 12 for AES192, 14 for AES256 */                                               \
+  aesFinal              (kp,b0003,b0407,b0811,b1215,t4)                                                                \
+  /* xor against input */                                                                                              \
+  VPXORQ                t0, b0003, b0003                                                                               \
+  VPXORQ                t1, b0407, b0407                                                                               \
+  VPXORQ                t2, b0811, b0811                                                                               \
+  VPXORQ                t3, b1215, b1215                                                                               \
+  /* store */                                                                                                          \
+  VX512STR              b0003, (64*0+dataDispl)(dataOffset)(out*1)                                                     \
+  VX512STR              b0407, (64*1+dataDispl)(dataOffset)(out*1)                                                     \
+  VX512STR              b0811, (64*2+dataDispl)(dataOffset)(out*1)                                                     \
+  VX512STR              b1215, (64*3+dataDispl)(dataOffset)(out*1)                                                     \
+  /* shuffleBlockOp selects an encrypt- or decrypt-specific shuffle for 4x4 128-bit blocks */                          \
+  shuffleBlockOp        (b0003,b0407,b0811,b1215,t0,t1,t2,t3,shuffleMask)                                              \
+  /* for initial block XOR cipher block 0 with GHASH for the next round */                                             \
+  /* for later blocks AND cipher block 0 with itself for identity to allow macro reuse */                              \
+  inst                  ghash, b0003, b0003                                                                            \
+  /* for go need SP64, a 64-bit aligned temp var stack ptr; unable to control alignment as in ipsec */                 \
+  /* alternatively could use VMOVDQU64 unaligned stores but on critical path likely to degrade thruput */              \
+  VMOVDQA64             b0003, (64*0+blockOffset)(SP64)                                                                \
+  VMOVDQA64             b0407, (64*1+blockOffset)(SP64)                                                                \
+  VMOVDQA64             b0811, (64*2+blockOffset)(SP64)                                                                \
+  VMOVDQA64             b1215, (64*3+blockOffset)(SP64)                                                                
+        
+#define initialBlocksNx16(in,out,kp,dataOffset,ghash,ctr,ctrx,ctrCheck,t0,t1,t2,t3,t4,t4x,t4y,t5,t5x,t5y,t6,t7,t8,t8x,t8y,t9,t9x,t9y,t10,t10x,t11,t12,t13,t14,t15,t16,t17,t18,t19,t20,t21,t22,gh,gl,gm,addBE4x4,addBE1234,shuffleMask,nBlocks,depthBlock,SP64,redPoly,hXorMask,shuffleOpInit,shuffleOpGhash,g16extKey,g16extAes,initExtAes) \
+  VMOVD                 ctrx, ctrCheck                                                                                 \
+  ANDQ                  $255, ctrCheck                                                                                 \
+  VSHUFI64X2            $0, ctr, ctr, ctr            /* in LE after init, convert to BE */                             \
+  VPSHUFB               shuffleMask, ctr, ctr                                                                          \                                                                  
+  initialBlocks16       (in,out,kp,dataOffset,ghash,ctr,ctrCheck,addBE4x4,addBE1234,t0,t1,t2,t3,t4,t5,t6,t7,t8,shuffleMask,0,0,SP64,L0,L1,VPXORQ,shuffleOpInit,initExtAes)   \     
+  initialBlocks16       (in,out,kp,dataOffset,t5,ctr,ctrCheck,addBE4x4,addBE1234,t0,t1,t2,t3,t4,t5,t6,t7,t8,shuffleMask,256,256,SP64,L2,L3,VPANDQ,shuffleOpInit,initExtAes)  \ 
+  ghash16Enc16Parallel  (kp,out,in,dataOffset,ctr,ctrCheck,HashKey_48,512,0,shuffleMask,t0,t1,t2,t3,t4,t4x,t4y,t5,t5x,t5y,t6,t7,t8,t8x,t8y,t9,t9x,t9y,t10,t10x,t11,t12,t13,t14,t15,t16,t17,t18,t19,t20,t21,addBE4x4,addBE1234,gl,gh,gm,512,ghash,SP64,L4,L5,ghashGatherFirst,redPoly,ghashInNone,ghashNoReduce1,ghashNoReduce2,ghashNoReduce3,ghashNoReduce4,hXorMask,shuffleOpGhash,g16extKey,g16extAes) \
+  ADDQ                  $(nBlocks*16), dataOffset                                                                                   
+
+#define ghashLast16(kp,hashk,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,t2h,t2l,t2m1,t2m2,blk1,blk2,hk1,hk2,cipherBlk,SP64)  \
+  /* load next 8 cipher blocks and ghash keys */                                                                      \
+  VMOVDQA64             (cipherBlk)(SP64), blk1                                                                       \
+  VMOVDQA64             (cipherBlk + 64*1)(SP64), blk2                                                                \
+  VMOVDQU64             (hashk + 64*0)(kp), hk1                                                                       \
+  VMOVDQU64             (hashk + 64*1)(kp), hk2                                                                       \
+  /* ghash blocks 0-3 */                                                                                              \
+  VPCLMULQDQ            $0x11, hk1, blk1, t1h             /* th = a1*b1 */                                            \
+  VPCLMULQDQ            $0x00, hk1, blk1, t1l             /* tl = a0*b0 */                                            \
+  VPCLMULQDQ            $0x01, hk1, blk1, t1m1            /* tm1 = a1*b0 */                                           \
+  VPCLMULQDQ            $0x10, hk1, blk1, t1m2            /* tm2 = a0*b1 */                                           \
+  /* ghash blocks 4-7 */                                                                                              \
+  VPCLMULQDQ            $0x11, hk2, blk2, t2h             /* tth = a1*b1 */                                           \
+  VPCLMULQDQ            $0x00, hk2, blk2, t2l             /* ttl = a0*b0 */                                           \
+  VPCLMULQDQ            $0x01, hk2, blk2, t2m1            /* ttm1 = a1*b0 */                                          \
+  VPCLMULQDQ            $0x10, hk2, blk2, t2m2            /* ttm2 = a0*b1 */                                          \
+  /* update sums */                                                                                                   \
+  VPTERNLOGQ            $0x96, t2h, t1h, t0h               /* t0h = t0h + t1h + gh */                                 \
+  VPTERNLOGQ            $0x96, t2l, t1l, t0l               /* t0l = t0l + t1l + gl */                                 \
+  VPTERNLOGQ            $0x96, t2m1, t1m1, t0m1            /* t0m1 = t0m1 + t1m1 + gm */                              \
+  VPTERNLOGQ            $0x96, t2m2, t1m2, t0m2            /* t0m1 = t0m1 + t1m1 + gm */                              
+
+#define ghashLastNx16(kp,ghash,t0h,t0hx,t0hy,t0l,t0lx,t0ly,t0m1,t0m2,t1h,t1l,t1m1,t1m2,t2h,t2l,t2m1,t2m1x,t2m1y,t2m2,t2m2x,t2m2y,blk1,blk2,hk1,hk2,gh,gl,gm,SP64,hXorMask) \
+  /* load cipher blocks and ghash keys */                                                                             \
+  VMOVDQA64             (cipherBlk)(SP64), blk1                                                                       \
+  VMOVDQA64             (cipherBlk+64)(SP64), blk2                                                                    \
+  VMOVDQU64             (HashKey_32 + 64*0)(kp), hk1                                                                  \
+  VMOVDQU64             (HashKey_32 + 64*1)(kp), hk2                                                                  \
+  /* ghash blocks 0-3 */                                                                                              \
+  VPCLMULQDQ            $0x11, hk1, blk1, t0h             /* th = a1*b1 */                                            \
+  VPCLMULQDQ            $0x00, hk1, blk1, t0l             /* tl = a0*b0 */                                            \
+  VPCLMULQDQ            $0x01, hk1, blk1, t0m1            /* tm1 = a1*b0 */                                           \
+  VPCLMULQDQ            $0x10, hk1, blk1, t0m2            /* tm2 = a0*b1 */                                           \
+  /* ghash blocks 4-7 */                                                                                              \
+  VPCLMULQDQ            $0x11, hk2, blk2, t1h             /* tth = a1*b1 */                                           \
+  VPCLMULQDQ            $0x00, hk2, blk2, t1l             /* ttl = a0*b0 */                                           \
+  VPCLMULQDQ            $0x01, hk2, blk2, t1m1            /* ttm1 = a1*b0 */                                          \
+  VPCLMULQDQ            $0x10, hk2, blk2, t1m2            /* ttm2 = a0*b1 */                                          \
+  VPTERNLOGQ            $0x96, gh, t1h, t0h               /* t0h = t0h + t1h + gh */                                  \
+  VPTERNLOGQ            $0x96, gl, t1l, t0l               /* t0l = t0l + t1l + gl */                                  \
+  VPTERNLOGQ            $0x96, gm, t1m1, t0m1             /* t0m1 = t0m1 + t1m1 + gm */                               \
+  VPXORQ                t1m2, t0m2, t0m2                  /* t0m2 = t0m2 + t1m2 */                                    \
+  ghashLast16           (kp,HashKey_32+128,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,t2h,t2l,t2m1,t2m2,blk1,blk2,hk1,hk2,cipherBlk+128,SP64) \
+  ghashLast16           (kp,HashKey_32+256,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,t2h,t2l,t2m1,t2m2,blk1,blk2,hk1,hk2,cipherBlk+256,SP64) \
+  ghashLast16           (kp,HashKey_32+384,t0h,t0l,t0m1,t0m2,t1h,t1l,t1m1,t1m2,t2h,t2l,t2m1,t2m2,blk1,blk2,hk1,hk2,cipherBlk+384,SP64) \
+  /* integrate tm into th and tl */                                                                                   \
+  VPXORQ                t0m2, t0m1, t0m1                                                                              \
+  VPSRLDQ               $8, t0m1, t1m1                                                                                \
+  VPSLLDQ               $8, t0m1, t1m2                                                                                \
+  VPXORQ                t1m1, t0h, t0h                                                                                \
+  VPXORQ                t1m2, t0l, t0l                                                                                \
+  /* add th and tl 128-bit words horizontally */                                                                      \
+  vhpxori4x128          (t0h,t0hx,t0hy,t2m1,t2m1x,t2m1y,hXorMask)                                                     \
+  vhpxori4x128          (t0l,t0lx,t0ly,t2m2,t2m2x,t2m2y,hXorMask)                                                     \
+  /* reduce */                                                                                                        \
+  /* note: ipsec uses aligned load VMOVDQA64; move poly2 to go-managed buffer with 2 pointers to match */             \
+  VMOVDQU64             poly2<>(SB), hk1                                                                              \
+  vclmulReduce          (ghash,hk1,t0h,t0l,t0m1,t0m2)
+
+/* storeBlocks0 is a NOP */
+#define storeBlocks0(s0,s0x,s0y,s1,s1x,s1y,out,dstOffset,dataOffset,maskReg)   
+
+#define storeBlocks1(s0,s0x,s0y,s1,s1x,s1y,out,dstOffset,dataOffset,maskReg) \
+  VMOVDQU8                  s0x, (dstOffset)(dataOffset)(out*1)
+
+#define storeBlocks2(s0,s0x,s0y,s1,s1x,s1y,out,dstOffset,dataOffset,maskReg) \
+  VMOVDQU8                  s0y, (dstOffset)(dataOffset)(out*1)
+
+#define storeBlocks3(s0,s0x,s0y,s1,s1x,s1y,out,dstOffset,dataOffset,maskReg) \
+  VMOVDQU8                  s0y, (dstOffset)(dataOffset)(out*1)                            \    
+  VEXTRACTI32X4             $2, s0, maskReg, (32+dstOffset)(dataOffset)(out*1)
+
+#define storeBlocks4(s0,s0x,s0y,s1,s1x,s1y,out,dstOffset,dataOffset,maskReg) \
+  VMOVDQU8                  s0, (dstOffset)(dataOffset)(out*1)                            
+
+#define storeBlocks5(s0,s0x,s0y,s1,s1x,s1y,out,dstOffset,dataOffset,maskReg) \
+  storeBlocks4              (s0,s0x,s0y,s1,s1x,s1y,out,dstOffset,dataOffset,maskReg)       \
+  storeBlocks1              (s1,s1x,s1y,s0,s0x,s0y,out,dstOffset+64,dataOffset,maskReg)
+
+#define storeBlocks6(s0,s0x,s0y,s1,s1x,s1y,out,dstOffset,dataOffset,maskReg) \
+  storeBlocks4              (s0,s0x,s0y,s1,s1x,s1y,out,dstOffset,dataOffset,maskReg)       \
+  storeBlocks2              (s1,s1x,s1y,s0,s0x,s0y,out,dstOffset+64,dataOffset,maskReg)
+
+#define storeBlocks7(s0,s0x,s0y,s1,s1x,s1y,out,dstOffset,dataOffset,maskReg) \
+  storeBlocks4              (s0,s0x,s0y,s1,s1x,s1y,out,dstOffset,dataOffset,maskReg)       \
+  storeBlocks3              (s1,s1x,s1y,s0,s0x,s0y,out,dstOffset+64,dataOffset,maskReg)
+
+#define storeBlocks8(s0,s0x,s0y,s1,s1x,s1y,out,dstOffset,dataOffset,maskReg) \
+  storeBlocks4              (s0,s0x,s0y,s1,s1x,s1y,out,dstOffset,dataOffset,maskReg)       \
+  storeBlocks4              (s1,s1x,s1y,s0,s0x,s0y,out,dstOffset+64,dataOffset,maskReg)
+
+#define loadBlocks0(d0,d0x,d0y,d1,d1x,d1y,in,srcOffset,dataOffset,maskReg)   
+
+#define loadBlocks1(d0,d0x,d0y,d1,d1x,d1y,in,srcOffset,dataOffset,maskReg) \
+  VMOVDQU8                  (srcOffset)(dataOffset)(in*1), d0x
+
+#define loadBlocks2(d0,d0x,d0y,d1,d1x,d1y,in,srcOffset,dataOffset,maskReg) \
+  VMOVDQU8                  (srcOffset)(dataOffset)(in*1), d0y
+
+#define loadBlocks3(d0,d0x,d0y,d1,d1x,d1y,in,srcOffset,dataOffset,maskReg) \
+  VMOVDQU8                  (srcOffset)(dataOffset)(in*1), d0y                            \    
+  VINSERTI64X2              $2, (32+srcOffset)(dataOffset)(in*1), d0, maskReg, d0  
+
+#define loadBlocks4(d0,d0x,d0y,d1,d1x,d1y,in,srcOffset,dataOffset,maskReg) \
+  VMOVDQU8                  (srcOffset)(dataOffset)(in*1), d0                            
+
+#define loadBlocks5(d0,d0x,d0y,d1,d1x,d1y,in,srcOffset,dataOffset,maskReg) \
+  loadBlocks4               (d0,d0x,d0y,d1,d1x,d1y,in,srcOffset,dataOffset,maskReg)       \
+  loadBlocks1               (d1,d1x,d1y,d0,d0x,d0y,in,srcOffset+64,dataOffset,maskReg)
+
+#define loadBlocks6(d0,d0x,d0y,d1,d1x,d1y,in,srcOffset,dataOffset,maskReg) \
+  loadBlocks4               (d0,d0x,d0y,d1,d1x,d1y,in,srcOffset,dataOffset,maskReg)       \
+  loadBlocks2               (d1,d1x,d1y,d0,d0x,d0y,in,srcOffset+64,dataOffset,maskReg)
+
+#define loadBlocks7(d0,d0x,d0y,d1,d1x,d1y,in,srcOffset,dataOffset,maskReg) \
+  loadBlocks4               (d0,d0x,d0y,d1,d1x,d1y,in,srcOffset,dataOffset,maskReg)       \
+  loadBlocks3               (d1,d1x,d1y,d0,d0x,d0y,in,srcOffset+64,dataOffset,maskReg)
+
+#define loadBlocks8(d0,d0x,d0y,d1,d1x,d1y,in,srcOffset,dataOffset,maskReg) \
+  loadBlocks4               (d0,d0x,d0y,d1,d1x,d1y,in,srcOffset,dataOffset,maskReg)       \
+  loadBlocks4               (d1,d1x,d1y,d0,d0x,d0y,in,srcOffset+64,dataOffset,maskReg)
+
+// prepareAesCtrBlock0 is a NOP
+#define prepareAesCtrBlocks0(ctr,ctrx,ctry,zt0,xt0,yt0,zt1)  
+
+#define prepareAesCtrBlocks1(ctr,ctrx,ctry,zt0,xt0,yt0,zt1) \
+  VPADDD                      one<>(SB), ctrx, xt0                                        \
+  VEXTRACTI32X4               $0, zt0, ctrx            
+
+#define prepareAesCtrBlocks2(ctr,ctrx,ctry,zt0,xt0,yt0,zt1) \
+  VSHUFI64X2                  $0, ctry, ctry, yt0                                         \            
+  VPADDD                      add1234y, yt0, yt0                                          \
+  VEXTRACTI32X4               $1, zt0, ctrx            
+
+#define prepareAesCtrBlocks3to7(ctr,ctrx,ctry,zt0,xt0,yt0,zt1,ext,extSrc) \
+  VSHUFI64X2                  $0, ctr, ctr, ctr                                           \
+  VPADDD                      add1234, ctr, zt0                                           \
+  VPADDD                      add5678, ctr, zt1                                           \
+  VEXTRACTI32X4               $ext, extSrc, ctrx            
+
+#define prepareAesCtrBlocks3(ctr,ctrx,ctry,zt0,xt0,yt0,zt1) \
+  prepareAesCtrBlocks3to7     (ctr,ctrx,ctry,zt0,xt0,yt0,zt1,2,zt0)
+
+#define prepareAesCtrBlocks4(ctr,ctrx,ctry,zt0,xt0,yt0,zt1) \
+  prepareAesCtrBlocks3to7     (ctr,ctrx,ctry,zt0,xt0,yt0,zt1,3,zt0)
+
+#define prepareAesCtrBlocks5(ctr,ctrx,ctry,zt0,xt0,yt0,zt1) \
+  prepareAesCtrBlocks3to7     (ctr,ctrx,ctry,zt0,xt0,yt0,zt1,0,zt1)
+
+#define prepareAesCtrBlocks6(ctr,ctrx,ctry,zt0,xt0,yt0,zt1) \
+  prepareAesCtrBlocks3to7(ctr,ctrx,ctry,zt0,xt0,yt0,zt1,1,zt1)
+
+#define prepareAesCtrBlocks7(ctr,ctrx,ctry,zt0,xt0,yt0,zt1) \
+  prepareAesCtrBlocks3to7(ctr,ctrx,ctry,zt0,xt0,yt0,zt1,2,zt1)
+
+/* note: could replace op* with ope* everywhere, delete op* */
+#define blockOp0(opcode,d0,d0x,d0y,d1,d1x,d1y,s0,s0x,s0y,s1,s1x,s1y,m0,m0x,m0y,m1,m1x,m1y)   
+
+#define blockOp1(opcode,d0,d0x,d0y,d1,d1x,d1y,s0,s0x,s0y,s1,s1x,s1y,m0,m0x,m0y,m1,m1x,m1y)          \
+  opcode                m0x, s0x, d0x
+
+#define blockOp2(opcode,d0,d0x,d0y,d1,d1x,d1y,s0,s0x,s0y,s1,s1x,s1y,m0,m0x,m0y,m1,m1x,m1y)          \
+  opcode                m0y, s0y, d0y
+
+#define blockOp3(opcode,d0,d0x,d0y,d1,d1x,d1y,s0,s0x,s0y,s1,s1x,s1y,m0,m0x,m0y,m1,m1x,m1y)          \
+  opcode                m0, s0, d0
+
+#define blockOp4(opcode,d0,d0x,d0y,d1,d1x,d1y,s0,s0x,s0y,s1,s1x,s1y,m0,m0x,m0y,m1,m1x,m1y)          \
+  opcode                m0, s0, d0
+
+#define blockOp5(opcode,d0,d0x,d0y,d1,d1x,d1y,s0,s0x,s0y,s1,s1x,s1y,m0,m0x,m0y,m1,m1x,m1y)          \
+  opcode                m0, s0, d0                                                                  \
+  blockOp1              (opcode,d1,d1x,d1y,d0,d0x,d0y,s1,s1x,s1y,s0,s0x,s0y,m1,m1x,m1y,m0,m0x,m0y)
+
+#define blockOp6(opcode,d0,d0x,d0y,d1,d1x,d1y,s0,s0x,s0y,s1,s1x,s1y,m0,m0x,m0y,m1,m1x,m1y)          \
+  opcode                m0, s0, d0                                                                  \
+  blockOp2              (opcode,d1,d1x,d1y,d0,d0x,d0y,s1,s1x,s1y,s0,s0x,s0y,m1,m1x,m1y,m0,m0x,m0y)
+
+#define blockOp7(opcode,d0,d0x,d0y,d1,d1x,d1y,s0,s0x,s0y,s1,s1x,s1y,m0,m0x,m0y,m1,m1x,m1y)          \
+  opcode                m0, s0, d0                                                                  \
+  blockOp3              (opcode,d1,d1x,d1y,d0,d0x,d0y,s1,s1x,s1y,s0,s0x,s0y,m1,m1x,m1y,m0,m0x,m0y)
+
+#define blockOp8(opcode,d0,d0x,d0y,d1,d1x,d1y,s0,s0x,s0y,s1,s1x,s1y,m0,m0x,m0y,m1,m1x,m1y)          \
+  opcode                m0, s0, d0                                                                  \
+  opcode                m1, s1, d1
+
+#define blockOp1e(opcode,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,s0,s0x,s0y,s1,s1x,s1y,s2,s2x,s2y,s3,s3x,s3y,m0,m0x,m0y,m1,m1x,m1y,m2,m2x,m2y,m3,m3x,m3y) \
+  opcode                m0x, s0x, d0x
+
+#define blockOp2e(opcode,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,s0,s0x,s0y,s1,s1x,s1y,s2,s2x,s2y,s3,s3x,s3y,m0,m0x,m0y,m1,m1x,m1y,m2,m2x,m2y,m3,m3x,m3y) \
+  opcode                m0y, s0y, d0y
+
+#define blockOp3e(opcode,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,s0,s0x,s0y,s1,s1x,s1y,s2,s2x,s2y,s3,s3x,s3y,m0,m0x,m0y,m1,m1x,m1y,m2,m2x,m2y,m3,m3x,m3y) \
+  opcode                m0, s0, d0                                                                  
+
+#define blockOp4e(opcode,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,s0,s0x,s0y,s1,s1x,s1y,s2,s2x,s2y,s3,s3x,s3y,m0,m0x,m0y,m1,m1x,m1y,m2,m2x,m2y,m3,m3x,m3y) \
+  opcode                m0, s0, d0                                                                  
+
+#define blockOp5e(opcode,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,s0,s0x,s0y,s1,s1x,s1y,s2,s2x,s2y,s3,s3x,s3y,m0,m0x,m0y,m1,m1x,m1y,m2,m2x,m2y,m3,m3x,m3y) \
+  opcode                m0, s0, d0                                                                  \
+  opcode                m1x, s1x, d1x
+
+#define blockOp6e(opcode,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,s0,s0x,s0y,s1,s1x,s1y,s2,s2x,s2y,s3,s3x,s3y,m0,m0x,m0y,m1,m1x,m1y,m2,m2x,m2y,m3,m3x,m3y) \
+  opcode                m0, s0, d0                                                                  \
+  opcode                m1y, s1y, d1y
+
+#define blockOp7e(opcode,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,s0,s0x,s0y,s1,s1x,s1y,s2,s2x,s2y,s3,s3x,s3y,m0,m0x,m0y,m1,m1x,m1y,m2,m2x,m2y,m3,m3x,m3y) \
+  opcode                m0, s0, d0                                                                  \
+  opcode                m1, s1, d1
+
+#define blockOp8e(opcode,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,s0,s0x,s0y,s1,s1x,s1y,s2,s2x,s2y,s3,s3x,s3y,m0,m0x,m0y,m1,m1x,m1y,m2,m2x,m2y,m3,m3x,m3y) \
+  opcode                m0, s0, d0                                                                  \
+  opcode                m1, s1, d1                                                                  
+
+#define blockOp9(opcode,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,s0,s0x,s0y,s1,s1x,s1y,s2,s2x,s2y,s3,s3x,s3y,m0,m0x,m0y,m1,m1x,m1y,m2,m2x,m2y,m3,m3x,m3y) \
+  opcode                m0, s0, d0                                                                  \
+  opcode                m1, s1, d1                                                                  \
+  opcode                m2x, s2x, d2x
+
+#define blockOp10(opcode,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,s0,s0x,s0y,s1,s1x,s1y,s2,s2x,s2y,s3,s3x,s3y,m0,m0x,m0y,m1,m1x,m1y,m2,m2x,m2y,m3,m3x,m3y) \
+  opcode                m0, s0, d0                                                                  \
+  opcode                m1, s1, d1                                                                  \
+  opcode                m2y, s2y, d2y
+
+#define blockOp11(opcode,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,s0,s0x,s0y,s1,s1x,s1y,s2,s2x,s2y,s3,s3x,s3y,m0,m0x,m0y,m1,m1x,m1y,m2,m2x,m2y,m3,m3x,m3y) \
+  opcode                m0, s0, d0                                                                  \
+  opcode                m1, s1, d1                                                                  \
+  opcode                m2, s2, d2
+
+#define blockOp12(opcode,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,s0,s0x,s0y,s1,s1x,s1y,s2,s2x,s2y,s3,s3x,s3y,m0,m0x,m0y,m1,m1x,m1y,m2,m2x,m2y,m3,m3x,m3y) \
+  opcode                m0, s0, d0                                                                  \
+  opcode                m1, s1, d1                                                                  \
+  opcode                m2, s2, d2
+
+#define blockOp13(opcode,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,s0,s0x,s0y,s1,s1x,s1y,s2,s2x,s2y,s3,s3x,s3y,m0,m0x,m0y,m1,m1x,m1y,m2,m2x,m2y,m3,m3x,m3y) \
+  opcode                m0, s0, d0                                                                  \
+  opcode                m1, s1, d1                                                                  \
+  opcode                m2, s2, d2                                                                  \
+  opcode                m3x, s3x, d3x
+
+#define blockOp14(opcode,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,s0,s0x,s0y,s1,s1x,s1y,s2,s2x,s2y,s3,s3x,s3y,m0,m0x,m0y,m1,m1x,m1y,m2,m2x,m2y,m3,m3x,m3y) \
+  opcode                m0, s0, d0                                                                  \
+  opcode                m1, s1, d1                                                                  \
+  opcode                m2, s2, d2                                                                  \
+  opcode                m3y, s3y, d3y
+
+#define blockOp15(opcode,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,s0,s0x,s0y,s1,s1x,s1y,s2,s2x,s2y,s3,s3x,s3y,m0,m0x,m0y,m1,m1x,m1y,m2,m2x,m2y,m3,m3x,m3y) \
+  opcode                m0, s0, d0                                                                  \
+  opcode                m1, s1, d1                                                                  \
+  opcode                m2, s2, d2                                                                  \
+  opcode                m3, s3, d3
+
+#define blockOp16(opcode,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,s0,s0x,s0y,s1,s1x,s1y,s2,s2x,s2y,s3,s3x,s3y,m0,m0x,m0y,m1,m1x,m1y,m2,m2x,m2y,m3,m3x,m3y) \
+  opcode                m0, s0, d0                                                                  \
+  opcode                m1, s1, d1                                                                  \
+  opcode                m2, s2, d2                                                                  \
+  opcode                m3, s3, d3
+
+// initialBlocks()
+// This macro is used to "warm-up" pipeline for GHASH_8_ENCRYPT_8_PARALLEL
+// It is called only for data lengths 128 and above.
+// The flow is as follows:
+// - encrypt the initial %%numBlocks blocks (can be 0)
+// - encrypt the next 8 blocks and stitch with
+//   GHASH for the first %%num_initial_blocks
+//   - the last 8th block can be partial (lengths between 129 and 239)
+//   - partial block ciphering is handled within this macro
+//     - top bytes of such block are cleared for
+//       the subsequent GHASH calculations
+//   - PBlockEncKey needs to be setup in case of multi-call
+//     - top bytes of the block need to include encrypted counter block so that
+//       when handling partial block case text is read and XOR'ed against it.
+//       This needs to be in un-shuffled format. 
+
+#define shuffleBlocks(dst0,dst0x,dst0y,dst1,dst1x,dst1y,src0,src0x,src0y,src1,src1x,src1y,m,mx,my,blockOp) \
+  blockOp                   (VPSHUFB,dst0,dst0x,dst0y,dst1,dst1x,dst1y,src0,src0x,src0y,src1,src1x,src1y,m,mx,my,m,mx,my)
+
+#define shuffleBlocks16(dst0,dst0x,dst0y,dst1,dst1x,dst1y,dst2,dst2x,dst2y,dst3,dst3x,dst3y,src0,src0x,src0y,src1,src1x,src1y,src2,src2x,src2y,src3,src3x,src3y,m,mx,my,blockOp) \
+  blockOp                   (VPSHUFB,dst0,dst0x,dst0y,dst1,dst1x,dst1y,dst2,dst2x,dst2y,dst3,dst3x,dst3y,src0,src0x,src0y,src1,src1x,src1y,src2,src2x,src2y,src3,src3x,src3y,m,mx,my,m,mx,my,m,mx,my,m,mx,my)
+
+// medium block [256,767] encrypt/decrypt-specific shuffles for initialBlocks()
+#define shuffleEncMB(zt5,xt5,yt5,zt6,xt6,yt6,zt3,xt3,yt3,zt4,xt4,yt4,shuffleMask,shuffleMaskx,shuffleMasky,blockOp) \
+  shuffleBlocks             (zt5,xt5,yt5,zt6,xt6,yt6,zt3,xt3,yt3,zt4,xt4,yt4,shuffleMask,shuffleMaskx,shuffleMasky,blockOp)
+
+#define shuffleDecMB(zt5,xt5,yt5,zt6,xt6,yt6,zt3,xt3,yt3,zt4,xt4,yt4,shuffleMask,shuffleMaskx,shuffleMasky,blockOp) \
+  shuffleBlocks             (zt5,xt5,yt5,zt6,xt6,yt6,zt5,xt5,yt5,zt6,xt6,yt6,shuffleMask,shuffleMaskx,shuffleMasky,blockOp)
+
+// medium block [256,767] encrypt/decrypt-specific shuffles for initialBlocks()
+#define shuffleGhashEnc(z1,z2,z3,z4,mask) \
+  VPSHUFB                   mask, z3, z1                                                                                                                      \
+  VPSHUFB                   mask, z4, z2
+
+// medium block [256,767] encrypt/decrypt-specific shuffles for initialBlocks()
+#define shuffleGhashDec(z1,z2,z3,z4,mask) \
+  VPSHUFB                   mask, z1, z1                                                                                                                      \
+  VPSHUFB                   mask, z2, z2
+
+// medium block [256,767] encrypt/decrypt-specific shuffles for ghash8Encrypt8P()
+#define shuffleGhash8Enc(z1,z2,z3,z4,z5,z6,mask) \
+  VPSHUFB                   mask, z1, z5                                                                                                                      \
+  VPSHUFB                   mask, z2, z6
+
+// medium block [256,767] encrypt/decrypt-specific shuffles for ghash8Encrypt8P()
+#define shuffleGhash8Dec(z1,z2,z3,z4,z5,z6,mask) \
+  VPSHUFB                   mask, z3, z5                                                                                                                      \
+  VPSHUFB                   mask, z4, z6
+
+#define xorBlocks(dst0,dst0x,dst0y,dst1,dst1x,dst1y,src0a,src0ax,src0ay,src1a,src1ax,src1ay,src0b,src0bx,src0by,src1b,src1bx,src1by,blockOp) \
+  blockOp                   (VPXORQ,dst0,dst0x,dst0y,dst1,dst1x,dst1y,src0a,src0ax,src0ay,src1a,src1ax,src1ay,src0b,src0bx,src0by,src1b,src1bx,src1by)             
+
+#define xorBlocks16(dst0,dst0x,dst0y,dst1,dst1x,dst1y,dst2,dst2x,dst2y,dst3,dst3x,dst3y,src0a,src0ax,src0ay,src1a,src1ax,src1ay,src2a,src2ax,src2ay,src3a,src3ax,src3ay,src0b,src0bx,src0by,src1b,src1bx,src1by,src2b,src2bx,src2by,src3b,src3bx,src3by,blockOp) \
+  blockOp                   (VPXORQ,dst0,dst0x,dst0y,dst1,dst1x,dst1y,dst2,dst2x,dst2y,dst3,dst3x,dst3y,src0a,src0ax,src0ay,src1a,src1ax,src1ay,src2a,src2ax,src2ay,src3a,src3ax,src3ay,src0b,src0bx,src0by,src1b,src1bx,src1by,src2b,src2bx,src2by,src3b,src3bx,src3by)             
+
+#define aesEncryptBlockRound(i,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp) \
+  VBROADCASTF64X2           (16*i)(keyExp), k                                                                                                                 \
+  blockOp                   (VAESENC,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0003,b0003x,b0003y,b0407,b0407x,b0407y,k,kx,ky,k,kx,ky)      
+
+#define aesEncryptBlockRoundLast(i,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp) \
+  VBROADCASTF64X2           (16*i)(keyExp), k                                                                                                                 \
+  blockOp                   (VAESENCLAST,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0003,b0003x,b0003y,b0407,b0407x,b0407y,k,kx,ky,k,kx,ky)      
+
+#define aesEncryptBlockFinal128(b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp) \
+  aesEncryptBlockRoundLast  (10,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)
+
+#define aesEncryptBlockFinal192(b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp) \
+  aesEncryptBlockRound      (10,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)                                                               \
+  aesEncryptBlockRound      (11,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)                                                               \
+  aesEncryptBlockRoundLast  (12,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)
+
+#define aesEncryptBlockFinal256(b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp) \
+  aesEncryptBlockRound      (10,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)                                                               \
+  aesEncryptBlockRound      (11,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)                                                               \
+  aesEncryptBlockRound      (12,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)                                                               \
+  aesEncryptBlockRound      (13,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)                                                               \
+  aesEncryptBlockRoundLast  (14,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)
+
+#define aesEncryptBlocks(keyExp,k,kx,ky,b0003,b0003x,b0003y,b0407,b0407x,b0407y,d0003,d0003x,d0003y,d0407,d0407x,d0407y,blockOp,aesEncryptBlockFinal) \
+  VBROADCASTF64X2           (16*0)(keyExp), k                                                                                                                 \
+  blockOp                   (VPXORQ,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0003,b0003x,b0003y,b0407,b0407x,b0407y,k,kx,ky,k,kx,ky)                          \      
+  aesEncryptBlockRound      (1,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)                                                                \
+  aesEncryptBlockRound      (2,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)                                                                \
+  aesEncryptBlockRound      (3,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)                                                                \
+  aesEncryptBlockRound      (4,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)                                                                \
+  aesEncryptBlockRound      (5,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)                                                                \
+  aesEncryptBlockRound      (6,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)                                                                \
+  aesEncryptBlockRound      (7,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)                                                                \
+  aesEncryptBlockRound      (8,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)                                                                \
+  aesEncryptBlockRound      (9,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)                                                                \
+  /* aesEncryptBlockRoundLast  (10,b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp) */                                                         \ 
+  aesEncryptBlockFinal      (b0003,b0003x,b0003y,b0407,b0407x,b0407y,keyExp,k,kx,ky,blockOp)                                                                  \ 
+  blockOp                   (VPXORQ,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0003,b0003x,b0003y,b0407,b0407x,b0407y,d0003,d0003x,d0003y,d0407,d0407x,d0407y) 
+
+#define aesEncryptBlockRound16(i,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp) \
+  VBROADCASTF64X2           (16*i)(keyExp), k                                                                                                                 \
+  blockOp                   (VAESENC,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,k,kx,ky,k,kx,ky,k,kx,ky,k,kx,ky)
+
+#define aesEncryptBlockRoundLast16(i,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp) \
+  VBROADCASTF64X2           (16*i)(keyExp), k                                                                                                                 \
+  blockOp                   (VAESENCLAST,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,k,kx,ky,k,kx,ky,k,kx,ky,k,kx,ky)
+
+#define aesEncryptBlock16Final128(b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp) \
+  aesEncryptBlockRoundLast16  (10,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                    
+
+#define aesEncryptBlock16Final192(b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp) \
+  aesEncryptBlockRound16      (10,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                      \
+  aesEncryptBlockRound16      (11,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                      \
+  aesEncryptBlockRoundLast16  (12,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                    
+
+#define aesEncryptBlock16Final256(b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp) \
+  aesEncryptBlockRound16      (10,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                      \
+  aesEncryptBlockRound16      (11,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                      \
+  aesEncryptBlockRound16      (12,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                      \
+  aesEncryptBlockRound16      (13,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                      \
+  aesEncryptBlockRoundLast16  (14,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                    
+
+#define aesEncryptBlocks16(keyExp,k,kx,ky,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,d0003,d0003x,d0003y,d0407,d0407x,d0407y,d0811,d0811x,d0811y,d1215,d1215x,d1215y,blockOp,aesEncryptBlockFinal) \
+  VBROADCASTF64X2             (16*0)(keyExp), k                                                                                                                   \
+  xorBlocks16                 (b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,k,kx,ky,k,kx,ky,k,kx,ky,k,kx,ky,blockOp) \  
+  aesEncryptBlockRound16      (1,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                      \
+  aesEncryptBlockRound16      (2,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                      \
+  aesEncryptBlockRound16      (3,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                      \
+  aesEncryptBlockRound16      (4,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                      \
+  aesEncryptBlockRound16      (5,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                      \
+  aesEncryptBlockRound16      (6,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                      \
+  aesEncryptBlockRound16      (7,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                      \
+  aesEncryptBlockRound16      (8,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                      \
+  aesEncryptBlockRound16      (9,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                      \
+  aesEncryptBlockFinal        (b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,keyExp,k,kx,ky,blockOp)                        \
+  xorBlocks16                 (b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,b0003,b0003x,b0003y,b0407,b0407x,b0407y,b0811,b0811x,b0811y,b1215,b1215x,b1215y,d0003,d0003x,d0003y,d0407,d0407x,d0407y,d0811,d0811x,d0811y,d1215,d1215x,d1215y,blockOp)       
+
+// schoolbook multiply step 1
+#define vclMulStep1(hi,tmp,th,tm,tl) \
+  VPCLMULQDQ                $0x11, tmp, hi, th      /* th = a1*b1 */                                                                      \
+  VPCLMULQDQ                $0x00, tmp, hi, tl      /* tl = a0*b0 */                                                                      \
+  VPCLMULQDQ                $0x01, tmp, hi, tm      /* tm = a1*b0 */                                                                      \
+  VPCLMULQDQ                $0x10, tmp, hi, tmp     /* tmp = a0*b1 */                                                                     \
+  VPXORQ                    tmp, tm, tm             /* [th:tm:tl]  */ 
+
+#define vclMulStep1NK(kp,hi,tmp,th,tm,tl) \
+  VMOVDQU64                 (HashKey_4)(kp), tmp                                                                                          \
+  vclMulStep1               (hi,tmp,th,tm,tl)    
+
+#define vclMulStep1K(hi,tmp,th,tm,tl,hkey) \
+  VMOVDQA64                 hkey, tmp                                                                                                     \
+  vclMulStep1               (hi,tmp,th,tm,tl)    
+
+// note: block size 8 untested
+#define vclMulStep1Block8(kp,hi,hix,hiy,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,thx,thy,tm,tmx,tmy,tl,tlx,tly) \
+  vclMulStep1NK             (kp,hi,tmp1,th,tm,tl)
+
+#define vclMulStep1Block7(kp,hi,hix,hiy,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,thx,thy,tm,tmx,tmy,tl,tlx,tly) \
+  VMOVDQU64                 (HashKey_3)(kp), tmp2                                                                                         \
+  VMOVDQA64                 maskOutTopBlock, tmp1                                                                                         \
+  VPANDQ                    tmp1, tmp2, tmp2                                                                                              \
+  VPANDQ                    tmp1, hi, hi                                                                                                  \
+  vclMulStep1K              (hi,tmp1,th,tm,tl,tmp2)
+
+#define vclMulStep1Block6(kp,hi,hix,hiy,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,thx,thy,tm,tmx,tmy,tl,tlx,tly) \
+  VMOVDQU64                 (HashKey_2)(kp), tmp2                                                                                         \
+  vclMulStep1K              (hiy,tmp1y,thy,tmy,tly,tmp2y)
+
+#define vclMulStep1Block5(kp,hi,hix,hiy,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,thx,thy,tm,tmx,tmy,tl,tlx,tly) \
+  VMOVDQU64                 (HashKey_1)(kp), tmp2                                                                                         \
+  vclMulStep1K              (hix,tmp1x,thx,tmx,tlx,tmp2x)
+
+#define vclMulStep1Block1to4(kp,hi,hix,hiy,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,thx,thy,tm,tmx,tmy,tl,tlx,tly) \
+  VPXORQ                    th, th, th                                                                                                    \                           
+  VPXORQ                    tm, tm, tm                                                                                                    \                           
+  VPXORQ                    tl, tl, tl                                                                           
+
+#define vclMulStep1Block0(kp,hi,hix,hiy,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,thx,thy,tm,tmx,tmy,tl,tlx,tly) 
+
+#define ghashBlockPart1B0to7(aadHash,zt5,keyData,zt6,xt6,yt6,zt8,xt8,yt8,zt9,xt9,yt9,th,thx,thy,tm,tmx,tmy,tl,tlx,tly,vclMulBlkStep1) \
+  VPXORQ                    aadHash, zt5, zt5                                                                                             \
+  vclMulBlkStep1            (keyData,zt6,xt6,yt6,zt8,xt8,yt8,zt9,xt9,yt9,th,thx,thy,tm,tmx,tmy,tl,tlx,tly)
+
+#define ghashBlockPart1NOP(aadHash,zt5,keyData,zt6,xt6,yt6,zt8,xt8,yt8,zt9,xt9,yt9,th,thx,thy,tm,tmx,tmy,tl,tlx,tly,vclMulBlkStep1) 
+
+// schoolbook multiply step 2
+#define vclMulStep2HxorNOP(r,rx,ry,t,tx,ty,k) 
+
+#define vclMulStep2Hxor4(r,rx,ry,t,tx,ty,k) \  
+  vhpxori4x128              (r,rx,ry,t,tx,ty,k)
+
+#define vclMulStep2Hxor2(r,rx,ry,t,tx,ty,k) \  
+  vhpxori2x128              (r,rx,tx,kn)
+
+#define vclMulStep2(hi,hix,hiy,lo,lox,loy,tmp0,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,tm,tl,hxor) \
+  VMOVDQA64                 tmp2, tmp0                                                                                                    \                  
+  VPCLMULQDQ                $0x10, tmp0, lo, tmp1    /* tmp1 = a0*b1 */                                                                   \
+  VPCLMULQDQ                $0x11, tmp0, lo, tmp2    /* tmp2 = a1*b1 */                                                                   \
+  VPXORQ                    tmp2, th, th                                                                                                  \ 
+  VPCLMULQDQ                $0x00, tmp0, lo, tmp2    /* tmp2 = a0*b0 */                                                                   \
+  VPXORQ                    tmp2, tl, tl                                                                                                  \ 
+  VPCLMULQDQ                $0x01, tmp0, lo, tmp0    /* tmp0 = a1*b0 */                                                                   \
+  VPTERNLOGQ                $0x96, tmp0, tmp1, tm    /* tm = tm xor tmp1 xor tmp0 */                                                      \
+  VPSRLDQ                   $8, tm, tmp2                                                                                                  \
+  VPXORQ                    tmp2, th, hi                                                                                                  \
+  VPSLLDQ                   $8, tm, tmp2                                                                                                  \
+  VPXORQ                    tmp2, tl, lo                                                                                                  \
+  /* reduce to hix: high 128b, lox: low 128b */                                                                                           \
+  hxor                      (hi,hix,hiy,tmp2,tmp2x,tmp2y,hXorMask)                                                                        \                    
+  hxor                      (lo,lox,loy,tmp1,tmp1x,tmp1y,hXorMask)              
+
+#define vclMulStep2Block7(kp,hi,hix,hiy,lo,lox,loy,tmp0,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,tm,tl) \
+  VMOVDQU64                 (HashKey_7)(kp), tmp2                                                                                         \                                                                                   
+  vclMulStep2               (hi,hix,hiy,lo,lox,loy,tmp0,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,tm,tl,vclMulStep2Hxor4)
+
+#define vclMulStep2Block6(kp,hi,hix,hiy,lo,lox,loy,tmp0,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,tm,tl) \
+  VMOVDQU64                 (HashKey_6)(kp), tmp2                                                                                         \                                                                                   
+  vclMulStep2               (hi,hix,hiy,lo,lox,loy,tmp0,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,tm,tl,vclMulStep2Hxor4)
+
+#define vclMulStep2Block5(kp,hi,hix,hiy,lo,lox,loy,tmp0,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,tm,tl) \
+  VMOVDQU64                 (HashKey_5)(kp), tmp2                                                                                         \                                                                                   
+  vclMulStep2               (hi,hix,hiy,lo,lox,loy,tmp0,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,tm,tl,vclMulStep2Hxor4)
+
+#define vclMulStep2Block4(kp,hi,hix,hiy,lo,lox,loy,tmp0,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,tm,tl) \
+  VMOVDQU64                 (HashKey_4)(kp), tmp2                                                                                         \                                                                                   
+  vclMulStep2               (hi,hix,hiy,lo,lox,loy,tmp0,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,tm,tl,vclMulStep2Hxor4)
+
+#define vclMulStep2Block3(kp,hi,hix,hiy,lo,lox,loy,tmp0,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,tm,tl) \
+  VMOVDQU64                 (HashKey_3)(kp), tmp2                                                                                         \ 
+  VMOVDQA64                 maskOutTopBlock, tmp1                                                                                         \ 
+  VPANDQ                    tmp1, tmp2, tmp2                                                                                              \
+  VPANDQ                    tmp1, lo, lo                                                                                                  \                                                                                 
+  vclMulStep2               (hi,hix,hiy,lo,lox,loy,tmp0,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,tm,tl,vclMulStep2Hxor4)
+
+#define vclMulStep2Block2(kp,hi,hix,hiy,lo,lox,loy,tmp0,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,tm,tl) \
+  VMOVDQU64                 (HashKey_2)(kp), tmp2y                                                                                        \                                                                                   
+  vclMulStep2               (hi,hix,hiy,lo,lox,loy,tmp0,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,tm,tl,vclMulStep2Hxor4)
+
+#define vclMulStep2Block1(kp,hi,hix,hiy,lo,lox,loy,tmp0,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,tm,tl) \
+  VMOVDQU64                 (HashKey_1)(kp), tmp2x                                                                                        \                                                                                   
+  vclMulStep2               (hi,hix,hiy,lo,lox,loy,tmp0,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,tm,tl,vclMulStep2Hxor4)
+
+// not called by initialBlocks, conditioned on numBlocks > 0 
+#define vclMulStep2Block0(kp,hi,hix,hiy,lo,lox,loy,tmp0,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,tm,tl) \
+  VPXORQ                    hi, hi, hi                                                                                                    \
+  VPXORQ                    lo, lo, lo  
+
+// NOPs for initialBlocks to skip vclMulStep2 and vclMulReduce when numBlocks == 0
+#define vclMulStep2BlockNOP(kp,hi,hix,hiy,lo,lox,loy,tmp0,tmp1,tmp1x,tmp1y,tmp2,tmp2x,tmp2y,th,tm,tl) 
+#define vclMulReduceBlockNOP(aadHashx,xt8,xt6,xt5,xt7,xt9) 
+
+#define vclMulReduceBlock(aadHashx,xt8,xt6,xt5,xt7,xt9) \
+  VMOVDQU64                 poly2<>(SB), xt8 \                                                                                             
+  vclmulReduce              (aadHashx,xt8,xt6,xt5,xt7,xt9)
+
+#define aesEncryptRoundsIB128(zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8,zt6,xt6,yt6,zt5,xt5,yt5,zt7,xt8,yt8,zt9,xt9,yt9,th,tm,tl,aadHashx,xt7,ghashBlockPart2,ghashBlockReduce) \
+  /* AES rounds 1-3 */                                                                                                                    \
+  aesEncryptBlockRound      (1,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  aesEncryptBlockRound      (2,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  aesEncryptBlockRound      (3,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  /* GHASH blocks 0-3, then hxor (gather) hi/lo into xt6/xt5 */                                                                           \
+  ghashBlockPart2           (keyData,zt6,xt6,yt6,zt5,xt5,yt5,zt7,zt8,xt8,yt8,zt9,xt9,yt9,th,tm,tl)                                        \  
+  /* AES rounds 4-6 */                                                                                                                    \
+  aesEncryptBlockRound      (4,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  aesEncryptBlockRound      (5,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  aesEncryptBlockRound      (6,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  /* GHASH reduction, return result in aadHashx */                                                                                        \
+  ghashBlockReduce          (aadHashx,xt8,xt6,xt5,xt7,xt9)                                                                                \
+  /* AES rounds 7-10 */                                                                                                                   \
+  aesEncryptBlockRound      (7,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  aesEncryptBlockRound      (8,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  aesEncryptBlockRound      (9,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  aesEncryptBlockRoundLast  (10,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)
+
+#define aesEncryptRoundsIB192256(zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8,zt6,xt6,yt6,zt5,xt5,yt5,zt7,xt8,yt8,zt9,xt9,yt9,th,tm,tl,aadHashx,xt7,ghashBlockPart2,ghashBlockReduce) \
+  /* AES rounds 1-4 */                                                                                                                    \
+  aesEncryptBlockRound      (1,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  aesEncryptBlockRound      (2,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  aesEncryptBlockRound      (3,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  aesEncryptBlockRound      (4,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  /* GHASH blocks 0-3, then hxor (gather) hi/lo into xt6/xt5 */                                                                           \
+  ghashBlockPart2           (keyData,zt6,xt6,yt6,zt5,xt5,yt5,zt7,zt8,xt8,yt8,zt9,xt9,yt9,th,tm,tl)                                        \  
+  /* AES rounds 5-8 */                                                                                                                    \
+  aesEncryptBlockRound      (5,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  aesEncryptBlockRound      (6,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  aesEncryptBlockRound      (7,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  aesEncryptBlockRound      (8,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  /* GHASH reduction, return result in aadHashx */                                                                                        \
+  ghashBlockReduce          (aadHashx,xt8,xt6,xt5,xt7,xt9)                                                                                \
+  /* AES rounds 9-12 */                                                                                                                   \
+  aesEncryptBlockRound      (9,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                                \
+  aesEncryptBlockRound      (10,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                               \
+  aesEncryptBlockRound      (11,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)
+
+#define aesEncryptRoundsIB192(zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8,zt6,xt6,yt6,zt5,xt5,yt5,zt7,xt8,yt8,zt9,xt9,yt9,th,tm,tl,aadHashx,xt7,ghashBlockPart2,ghashBlockReduce) \
+  aesEncryptRoundsIB192256  (zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8,zt6,xt6,yt6,zt5,xt5,yt5,zt7,xt8,yt8,zt9,xt9,yt9,th,tm,tl,aadHashx,xt7,ghashBlockPart2,ghashBlockReduce) \
+  aesEncryptBlockRoundLast  (12,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)
+
+#define aesEncryptRoundsIB256(zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8,zt6,xt6,yt6,zt5,xt5,yt5,zt7,xt8,yt8,zt9,xt9,yt9,th,tm,tl,aadHashx,xt7,ghashBlockPart2,ghashBlockReduce) \
+  aesEncryptRoundsIB192256  (zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8,zt6,xt6,yt6,zt5,xt5,yt5,zt7,xt8,yt8,zt9,xt9,yt9,th,tm,tl,aadHashx,xt7,ghashBlockPart2,ghashBlockReduce) \
+  aesEncryptBlockRound      (12,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                               \
+  aesEncryptBlockRound      (13,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)                                               \
+  aesEncryptBlockRoundLast  (14,zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8)
+
+#define initialBlocks(keyData,contextData,out,in,length,dataOffset,numBlocks,ctr,ctrx,ctry,aadHashz,zt1,xt1,yt1,zt2,zt3,xt3,yt3,zt4,xt4,yt4,zt5,xt5,yt5,zt6,xt6,yt6,zt7,xt7,zt8,xt8,yt8,zt9,xt9,yt9,th,thx,thy,tm,tmx,tmy,tl,tlx,tly,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,blockOp,prepareAesCtrBlocks,loadInputBlocks,storeOutputBlocks,vclMulBlkStep1,ghashBlockPart1,ghashBlockPart2,ghashBlockReduce,shuffleMB,shuffleGhash,aesEncryptBlockFinalMB,aesEncryptRoundsMB) \
+  /* apply AES to first numBlocks > 0 (i.e., 1,2,..7) */                                                                                  \
+  /* store shuffled AES outputs in zt5, zt6 for ghash in the next stage */                                                                \
+  KXNORQ                    maskReg, maskReg, maskReg                                                                                     \
+  prepareAesCtrBlocks       (ctr,ctrx,ctry,zt3,xt3,yt3,zt4)                                                                               \
+  shuffleBlocks             (zt3,xt3,yt3,zt4,xt4,yt4,zt3,xt3,yt3,zt4,xt4,yt4,shuffleMask,shuffleMaskx,shuffleMasky,blockOp)               \ 
+  loadInputBlocks           (zt5,xt5,yt5,zt6,xt6,yt6,in,0,dataOffset,maskReg)                                                             \
+  aesEncryptBlocks          (keyData,zt1,xt1,yt1,zt3,xt3,yt3,zt4,xt4,yt4,zt5,xt5,yt5,zt6,xt6,yt6,blockOp,aesEncryptBlockFinalMB)          \
+  storeOutputBlocks         (zt3,xt3,yt3,zt4,xt4,yt4,out,0,dataOffset,maskReg)                                                            \
+  shuffleMB                 (zt5,xt5,yt5,zt6,xt6,yt6,zt3,xt3,yt3,zt4,xt4,yt4,shuffleMask,shuffleMaskx,shuffleMasky,blockOp)               \
+  SUBQ                      $(numBlocks*16), length                                                                                       \
+  ADDQ                      $(numBlocks*16), dataOffset                                                                                   \
+  /* apply AES to next 8 blocks, interleaved with ghash on first numBlocks */                                                             \
+  /* init CTR in zt3, zt4 */                                                                                                              \
+  VSHUFI64X2                $0, ctr, ctr, ctr                                                                                             \
+  VPADDD                    add1234, ctr, zt3                                                                                             \ 
+  VPADDD                    add5678, ctr, zt4                                                                                             \
+  VEXTRACTI32X4             $3, zt4, ctrx                                                                                                 \
+  VPSHUFB                   shuffleMask, zt3, zt3                                                                                         \
+  VPSHUFB                   shuffleMask, zt4, zt4                                                                                         \
+  loadBlocks8               (zt1,xt1,yt1,zt2,xt2,yt2,in,0,dataOffset,maskReg)                                                             \
+  /* AES round 0 */                                                                                                                       \
+  VBROADCASTF64X2           (16*0)(keyData), zt8                                                                                          \
+  xorBlocks                 (zt3,zt3x,zt3y,zt4,zt4x,zt4y,zt3,zt3x,zt3y,zt4,zt4x,zt4y,zt8,zt8x,zt8y,zt8,zt8x,zt8y,blockOp8)                \
+  /* GHASH blocks 4-7, store intermediates in th/tm/tl */                                                                                 \
+  ghashBlockPart1           (aadHashz,zt5,keyData,zt6,xt6,yt6,zt8,xt8,yt8,zt9,xt9,yt9,th,thx,thy,tm,tmx,tmy,tl,tlx,tly,vclMulBlkStep1)    \
+  /* AES rounds 1-10/12/14, interleaved with block GHASH and GHASH reductions (MB==mid block [256,767]) */                                \
+  aesEncryptRoundsMB        (zt3,zt3x,zt3y,zt4,zt4x,zt4y,keyData,zt8,zt8x,zt8y,blockOp8,zt6,xt6,yt6,zt5,xt5,yt5,zt7,xt8,yt8,zt9,xt9,yt9,th,tm,tl,aadHashx,xt7,ghashBlockPart2,ghashBlockReduce) \
+  xorBlocks                 (zt3,zt3x,zt3y,zt4,zt4x,zt4y,zt3,zt3x,zt3y,zt4,zt4x,zt4y,zt1,zt1x,zt1y,zt2,zt2x,zt2y,blockOp8)                \
+  /* store output blocks */                                                                                                               \
+  storeBlocks8              (zt3,-,-,zt4,-,-,out,0,dataOffset,maskReg)                                                                    \
+  ADDQ                      $128, dataOffset                                                                                              \
+  SUBQ                      $128, length                                                                                                  \
+  /* shuffle AES results for GHASH */                                                                                                     \
+  shuffleGhash              (zt1,zt2,zt3,zt4,shuffleMask)                                                                                 \
+  /* combine GHASHed value with corresponding AES output */                                                                               \
+  VPXORQ                    aadHashz, zt1, zt1                                                                                            \
+  JMP                       GCMEncDec_initialBlocksEncrypted                                                                  
+ 
+//  ghash8Encrypt8P reductions, part 1
+//  IPSEC gather modes "reduction" and "final reduction" are not being used currently
+//  so the "R" and "FR" variants are just placeholders if they are needed in a future implementation
+#define ghashGatherP1R(gh1h,gh1l,gh2h,gh2l,gh1m1,gh1m2,gh2m1,gh2m2,t0ReduceH,t0ReduceL,t0ReduceM) 
+#define ghashGatherP1FR(gh1h,gh1l,gh2h,gh2l,gh1m1,gh1m2,gh2m1,gh2m2,t0ReduceH,t0ReduceL,t0ReduceM) 
+#define ghashGatherP1NR(gh1h,gh1l,gh2h,gh2l,gh1m1,gh1m2,gh2m1,gh2m2,t0ReduceH,t0ReduceL,t0ReduceM) \
+  VPTERNLOGQ                $0x96, gh2m1, gh1m2, gh1m1      /* tm: gh1m1 ^= gh1m2 ^ gh2m1 */                  \
+  VPTERNLOGQ                $0x96, gh2m2, gh1m1, t0ReduceM  /* tm: t0ReduceM ^= gh1m1 ^ gh2m2 */              \
+  VPTERNLOGQ                $0x96, gh2h, gh1h, t0ReduceH    /* th: t0ReduceH ^= gh1h ^ gh2h */                \
+  VPTERNLOGQ                $0x96, gh2l, gh1l, t0ReduceL    /* tl: t0ReduceL ^= gh1l ^ gh2l */
+
+//  ghash8Encrypt8P reductions, part 2
+//  IPSEC gather modes "reduction" and "final reduction" are not being used currently
+//  so the "R" and "FR" variants are just placeholders if they are needed in a future implementation
+#define ghashGatherP2R(h1,h1x,h1y,l1,l1x,l1y,h2,h2x,h2y,l2,l2x,l2y,kn) 
+#define ghashGatherP2FR(h1,h1x,h1y,l1,l1x,l1y,h2,h2x,h2y,l2,l2x,l2y,kn) 
+#define ghashGatherP2NR(h1,h1x,h1y,l1,l1x,l1y,h2,h2x,h2y,l2,l2x,l2y,kn) \
+  vhpxori4x128              (h1,h1x,h1y,h2,h2x,h2y,kn)                                                        \
+  vhpxori4x128              (l1,l1x,l1y,l2,l2x,l2y,kn)
+
+// ghash8Encrypt8P 8 block load/store 
+// 8 full blocks / use unmasked ops
+#define ghash8Load(b0003,b0407,in,dataOffset,ia0,ia1,length,mask) \
+  VMOVDQU8                  (dataOffset)(in*1), b0003                                                         \
+  VMOVDQU8                  (64*1)(dataOffset)(in*1), b0407 
+
+#define ghash8Store(b0003,b0407,d0407,out,dataOffset,ia0,ia1,length,mask,maskOp) \
+  VMOVDQU8                  b0003, (dataOffset)(out*1)                                                        \
+  VMOVDQU8                  b0407, (64*1)(dataOffset)(out*1) 
+
+// ghash8Encrypt8P 0-7 block load/store 
+// uses masked ops for partial block portions
+#define ghash8LoadMask(b0003,b0407,in,dataOffset,ia0,ia1,length,mask) \
+  MOVQ                      (byteLen2MaskPtr)(contextData), ia0                                               \
+  MOVQ                      length, ia1                                                                       \
+  SUBQ                      $64, ia1                                                                          \
+  KMOVQ                     (ia0)(ia1*8), mask                                                                \
+  VMOVDQU8                  (dataOffset)(in*1), b0003                                                         \
+  VMOVDQU8.Z                (64*1)(dataOffset)(in*1), mask, b0407                  
+
+// ghash8 masked store for encrypt
+#define ghash8StoreMaskOpEnc(b0407,d0407,mask) \
+  VMOVDQU8.Z                b0407, mask, b0407
+
+// ghash8 masked store for decrypt
+#define ghash8StoreMaskOpDec(b0407,d0407,mask) \
+  VMOVDQU8.Z                d0407, mask, d0407
+
+// maskOp parameter selects encrypt- or decrypt-specific top byte mask operation
+#define ghash8StoreMask(b0003,b0407,d0407,out,dataOffset,ia0,ia1,length,mask,maskOp) \
+  VMOVDQU8                  b0003, (dataOffset)(out*1)                                                        \
+  VMOVDQU8                  b0407, mask, (64*1)(dataOffset)(out*1)                                            \
+  /* clear empty top bytes for downstream ghash */                                                            \
+  maskOp                    (b0407,d0407,mask)
+
+#define ghashLast8(kp,bl47,bl47x,bl47y,bl03,bl03x,bl03y,zth,ztm,ztl,zt01,zt01x,zt02,zt02x,zt02y,zt03,zt03x,zt03y,aadHash,gh,gl,gm) \
+  vclMulStep1NK             (kp,bl47,zt01,zth,ztm,ztl)                                                                              \
+  VPXORQ                    gh, zth, zth                                                                                            \
+  VPXORQ                    gl, ztl, ztl                                                                                            \
+  VPXORQ                    gm, ztm, ztm                                                                                            \      
+  VMOVDQU64                 (HashKey_8)(kp), zt03                                                                                   \
+  vclMulStep2               (bl47,bl47x,bl47y,bl03,bl03x,bl03y,zt01,zt02,zt02x,zt02y,zt03,zt03x,zt03y,zth,ztm,ztl,vclMulStep2Hxor4) \
+  VMOVDQA64                 poly2<>(SB), zt03x                                                                                      \
+  vclmulReduce              (aadHash,zt03x,bl47x,bl03x,zt01x,zt02x)
+
+#define ghash8AesFinalRound128(zt1,zt2,keyData,zt3,blockOp8,zt4,zt5,in,dataOffset,ia0,ia1,length,maskReg,load8Blocks) \
+  /* load input data */                                                                                                             \
+  load8Blocks               (zt4,zt5,in,dataOffset,ia0,ia1,length,maskReg)                                                          \
+  /* AES last round */                                                                                                              \
+  aesEncryptBlockRoundLast  (10,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)
+
+#define ghash8AesFinalRound192(zt1,zt2,keyData,zt3,blockOp8,zt4,zt5,in,dataOffset,ia0,ia1,length,maskReg,load8Blocks) \
+  aesEncryptBlockRound      (10,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)                                                            \
+  aesEncryptBlockRound      (11,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)                                                            \
+  /* load input data */                                                                                                              \
+  load8Blocks               (zt4,zt5,in,dataOffset,ia0,ia1,length,maskReg)                                                           \
+  /* AES last round */                                                                                                               \
+  aesEncryptBlockRoundLast  (12,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)
+
+#define ghash8AesFinalRound256(zt1,zt2,keyData,zt3,blockOp8,zt4,zt5,in,dataOffset,ia0,ia1,length,maskReg,load8Blocks) \
+  aesEncryptBlockRound      (10,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)                                                            \
+  aesEncryptBlockRound      (11,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)                                                            \
+  aesEncryptBlockRound      (12,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)                                                            \
+  aesEncryptBlockRound      (13,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)                                                            \
+  /* load input data */                                                                                                              \
+  load8Blocks               (zt4,zt5,in,dataOffset,ia0,ia1,length,maskReg)                                                           \
+  /* AES last round */                                                                                                               \
+  aesEncryptBlockRoundLast  (14,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)
+
+// Main GCM macro stitching cipher with GHASH
+//  - operates on single stream
+//  - encrypts 8 blocks at a time
+//  - ghash the 8 previously encrypted ciphertext blocks
+//  For partial block case and multi_call , AES_PARTIAL_BLOCK on output
+//  contains encrypted counter block
+//
+#define ghash8Encrypt8P(keyData,out,in,dataOffset,ctr1,ctr2,ghashInAesOutB03,ghashInAesOutB47,aesPartialBlock,ia0,ia1,length,gh4Key,gh8Key,shuffleMask,zt1,zt2,zt3,zt4,zt5,gh1h,gh1hx,gh1hy,gh1l,gh1lx,gh1ly,gh1m1,gh1m2,gh2h,gh2hx,gh2hy,gh2l,gh2lx,gh2ly,gh2m1,gh2m2,maskReg,t0ReduceL,t0ReduceH,t0ReduceM,ghashGatherP1,load8Blocks,store8Blocks,shuffleGhash8,maskOp,aesFinalRound) \
+  /* populate conuter blocks for cipher part */                                                               \
+  VMOVDQA64                 ctr1, zt1                                                                         \
+  VMOVDQA64                 ctr2, zt2                                                                         \
+  /* stitch AES rounds with GHASH */                                                                          \
+  /* AES round 0 */                                                                                           \
+  VBROADCASTF64X2           (16*0)(keyData), zt3                                                              \
+  xorBlocks                 (zt1,-,-,zt2,-,-,zt1,-,-,zt2,-,-,zt3,-,-,zt3,-,-,blockOp8)                        \
+  /* GHASH 4 blocks */                                                                                        \
+  VPCLMULQDQ                $0x11, gh4Key, ghashInAesOutB47, gh1h       /* a1*b1 */                           \
+  VPCLMULQDQ                $0x00, gh4Key, ghashInAesOutB47, gh1l       /* a0*b0 */                           \
+  VPCLMULQDQ                $0x01, gh4Key, ghashInAesOutB47, gh1m1      /* a1*b0 */                           \
+  VPCLMULQDQ                $0x10, gh4Key, ghashInAesOutB47, gh1m2      /* a0*b1 */                           \
+  /* AES rounds 1-3 */                                                                                        \
+  aesEncryptBlockRound      (1,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)                                      \
+  aesEncryptBlockRound      (2,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)                                      \
+  aesEncryptBlockRound      (3,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)                                      \
+  /* GHASH 4 blocks */                                                                                        \
+  VPCLMULQDQ                $0x10, gh8Key, ghashInAesOutB03, gh2m1       /* a0*b1 */                          \
+  VPCLMULQDQ                $0x01, gh8Key, ghashInAesOutB03, gh2m2       /* a1*b0 */                          \
+  VPCLMULQDQ                $0x11, gh8Key, ghashInAesOutB03, gh2h        /* a1*b1 */                          \
+  VPCLMULQDQ                $0x00, gh8Key, ghashInAesOutB03, gh2l        /* a0*b1 */                          \
+  /* AES rounds 4-6 */                                                                                        \
+  aesEncryptBlockRound      (4,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)                                      \
+  aesEncryptBlockRound      (5,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)                                      \
+  aesEncryptBlockRound      (6,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)                                      \                     
+  /* gather GHASH part 1: store intermediate values in gh1m1, t0ReduceH/L/M */                                \ 
+  ghashGatherP1             (gh1h,gh1l,gh2h,gh2l,gh1m1,gh1m2,gh2m1,gh2m2,t0ReduceH,t0ReduceL,t0ReduceM)       \
+  /* AES rounds 7-10 */                                                                                       \
+  aesEncryptBlockRound      (7,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)                                      \
+  aesEncryptBlockRound      (8,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)                                      \
+  aesEncryptBlockRound      (9,zt1,-,-,zt2,-,-,keyData,zt3,-,-,blockOp8)                                      \
+  /* load input data */                                                                                       \
+  /* AES last round(s) and data load (switched on key length) */                                              \
+  aesFinalRound             (zt1,zt2,keyData,zt3,blockOp8,zt4,zt5,in,dataOffset,ia0,ia1,length,maskReg,load8Blocks)  \
+  xorBlocks                 (zt1,-,-,zt2,-,-,zt1,-,-,zt2,-,-,zt4,-,-,zt5,-,-,blockOp8)                        \
+  /* write output data */                                                                                     \
+  /* maskOp parameter selects an encrypt- or decrypt-specific masked store operator for the partial block */  \
+  store8Blocks              (zt1,zt2,zt5,out,dataOffset,ia0,ia1,length,maskReg,maskOp)                        \
+  /* shuffle for ghash */                                                                                     \
+  shuffleGhash8             (zt1,zt2,zt4,zt5,ghashInAesOutB03,ghashInAesOutB47,shuffleMask)
+
+// store 1..15 bytes to memory from AVX SIMD registers
+#define simdStoreAvx15(dst,src,size,tmp,idx) \
+  XORQ                      idx, idx                              \
+  TESTQ                     $8, size                              \
+  JZ                        simdStore_LT8                         \
+  VMOVQ                     src, (idx)(dst*1)                     \
+  VPSRLDQ                   $8, src, src                          \
+  ADDQ                      $8, idx                               \
+  simdStore_LT8:                                                  \
+    VMOVQ                   src, tmp                              \
+    TESTQ                   $4, size                              \
+    JZ                      simdStore_LT4                         \
+    MOVD                    tmp, (idx)(dst*1)                     \
+    SHRQ                    $32, tmp                              \
+    ADDQ                    $4, idx                               \
+  simdStore_LT4:                                                  \
+    TESTQ                   $2, size                              \
+    JZ                      simdStore_LT2                         \
+    MOVW                    tmp, (idx)(dst*1)                     \
+    SHRQ                    $16, tmp                              \
+    ADDQ                    $2, idx                               \
+  simdStore_LT2:                                                  \
+    TESTQ                   $1, size                              \
+    JZ                      simdStore_end                         \
+    MOVB                    tmp, (idx)(dst*1)                     \
+  simdStore_end:                         
+
+#define maskReg             K1
+#define hXorMask            K2
+
+#define initialBlocksLoad1(d0,d1,d2,d3,in,dataOffset,mask) \
+  VMOVDQU8.Z                (64*0)(dataOffset)(in*1), mask, d0
+
+#define initialBlocksLoad2(d0,d1,d2,d3,in,dataOffset,mask) \
+  VMOVDQU8                  (dataOffset)(in*1), d0                                                                                                      \
+  VMOVDQU8.Z                (64*1)(dataOffset)(in*1), mask, d1      
+
+#define initialBlocksLoad3(d0,d1,d2,d3,in,dataOffset,mask) \
+  VMOVDQU8                  (dataOffset)(in*1), d0                                                                                                      \
+  VMOVDQU8                  (64*1)(dataOffset)(in*1), d1                                                                                                \
+  VMOVDQU8.Z                (64*2)(dataOffset)(in*1), mask, d2      
+
+#define initialBlocksLoad4(d0,d1,d2,d3,in,dataOffset,mask) \
+  VMOVDQU8                  (dataOffset)(in*1), d0                                                                                                      \
+  VMOVDQU8                  (64*1)(dataOffset)(in*1), d1                                                                                                \
+  VMOVDQU8                  (64*2)(dataOffset)(in*1), d2                                                                                                \
+  VMOVDQU8.Z                (64*3)(dataOffset)(in*1), mask, d3      
+
+#define initialBlocksStore1(out,dataOffset,c0,c1,c2,c3,mask) \
+  VMOVDQU8                  c0, maskReg, (64*0)(dataOffset)(out*1)                                                                            
+
+#define initialBlocksStore2(out,dataOffset,c0,c1,c2,c3,mask) \
+  VMOVDQU8                  c0, (dataOffset)(out*1)                                                                                                     \
+  VMOVDQU8                  c1, maskReg, (64*1)(dataOffset)(out*1)                                                                            
+
+#define initialBlocksStore3(out,dataOffset,c0,c1,c2,c3,mask) \
+  VMOVDQU8                  c0, (dataOffset)(out*1)                                                                                                     \
+  VMOVDQU8                  c1, (64*1)(dataOffset)(out*1)                                                                                               \
+  VMOVDQU8                  c2, maskReg, (64*2)(dataOffset)(out*1)                                                                            
+
+#define initialBlocksStore4(out,dataOffset,c0,c1,c2,c3,mask) \
+  VMOVDQU8                  c0, (dataOffset)(out*1)                                                                                                     \
+  VMOVDQU8                  c1, (64*1)(dataOffset)(out*1)                                                                                               \
+  VMOVDQU8                  c2, (64*2)(dataOffset)(out*1)                                                                                               \
+  VMOVDQU8                  c3, maskReg, (64*3)(dataOffset)(out*1)                                                                            
+
+#define extract(numBlk)     $(numBlk-(((numBlk-1)/4)*4+1))   
+#define maskOffset(numBlk)  $(64*((numBlk-1)/4))
+
+#define initialBlocksCtxInit1(ctrz,ctrx,ctry,c0,c0x,c0y,c1,c2,c3,add1234,add1234y,add5678,add8888) \
+  VPADDD                    one<>(SB), ctrx, c0x                                                                                                        \
+
+#define initialBlocksCtxInit2(ctrz,ctrx,ctry,c0,c0x,c0y,c1,c2,c3,add1234,add1234y,add5678,add8888) \
+  VSHUFI64X2                $0, ctry, ctry, c0y                                                                                                         \
+  VPADDD                    add1234y, c0y, c0y                                                                                      
+
+#define initialBlocksCtxInit34(ctrz,ctrx,ctry,c0,c0x,c0y,c1,c2,c3,add1234,add1234y,add5678,add8888) \
+  VSHUFI64X2                $0, ctrz, ctrz, ctrz                                                                                                        \
+  VPADDD                    add1234, ctrz, c0                                                                                         
+
+#define initialBlocksCtxInit58(ctrz,ctrx,ctry,c0,c0x,c0y,c1,c2,c3,add1234,add1234y,add5678,add8888) \ 
+  initialBlocksCtxInit34    (ctrz,ctrx,ctry,c0,c0x,c0y,c1,c2,c3,add1234,add1234y,add5678,add8888)                                                       \
+  VPADDD                    add5678, ctrz, c1                                                                                                           \
+
+#define initialBlocksCtxInit912(ctrz,ctrx,ctry,c0,c0x,c0y,c1,c2,c3,add1234,add1234y,add5678,add8888) \
+  initialBlocksCtxInit58(ctrz,ctrx,ctry,c0,c0x,c0y,c1,c2,c3,add1234,add1234y,add5678,add8888)                                                           \
+  VPADDD                    add8888, c0, c2                                                                                                             \
+
+#define initialBlocksCtxInit1316(ctrz,ctrx,ctry,c0,c0x,c0y,c1,c2,c3,add1234,add1234y,add5678,add8888) \
+  initialBlocksCtxInit912(ctrz,ctrx,ctry,c0,c0x,c0y,c1,c2,c3,add1234,add1234y,add5678,add8888)                                                          \
+  VPADDD                    add8888, c1, c3      
+
+#define shuffle16EncSB(d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,c0,c0x,c0y,c1,c1x,c1y,c2,c2x,c2y,c3,c3x,c3y,m,mx,my,blockOpx) \
+  shuffleBlocks16           (d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,c0,c0x,c0y,c1,c1x,c1y,c2,c2x,c2y,c3,c3x,c3y,m,mx,my,blockOpx)
+
+#define shuffle16DecSB(d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,c0,c0x,c0y,c1,c1x,c1y,c2,c2x,c2y,c3,c3x,c3y,m,mx,my,blockOpx) \
+  shuffleBlocks16           (d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,m,mx,my,blockOpx)
+
+#define initialBlocksPartial(key,ctx,out,in,length,dataOffset,ctr,ctrx,ctry,hashInOut,zt1,xt1,yt1,c0,c0x,c0y,c1,c1x,c1y,c2,c2x,c2y,c3,c3x,c3y,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,xt7,cSrc,dSrc,ia0,ia1,maskReg,m,mx,my,partial,partialSingle,done,numBlk,initCTR,loadBlocks,storeBlocks,blockOpx,shuffleBlk,aesFinal) \
+  initCTR                   (ctr,ctrx,ctry,c0,c0x,c0y,c1,c2,c3,add1234,add1234y,add5678,add8888)                                                        \
+  /* load data, apply AES, store results */                                                                                                             \
+  MOVQ                      (byteLen2MaskPtr)(ctx), ia0                                                                                                 \
+  MOVQ                      length, ia1                                                                                                                 \
+  SUBQ                      maskOffset(numBlk), ia1                                                                                                     \
+  KMOVQ                     (ia0)(ia1*8), maskReg                                                                                                       \
+  VEXTRACTI32X4             extract(numBlk), cSrc, ctrx                                                                                                 \
+  shuffleBlocks16           (c0,c0x,c0y,c1,c1x,c1y,c2,c2x,c2y,c3,c3x,c3y,c0,c0x,c0y,c1,c1x,c1y,c2,c2x,c2y,c3,c3x,c3y,m,mx,my,blockOpx)                  \
+  loadBlocks                (d0,d1,d2,d3,in,dataOffset,maskReg)                                                                                         \
+  aesEncryptBlocks16        (key,zt1,xt1,yt1,c0,c0x,c0y,c1,c1x,c1y,c2,c2x,c2y,c3,c3x,c3y,d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,blockOpx,aesFinal) \
+  storeBlocks               (out,dataOffset,c0,c1,c2,c3,maskReg)                                                                                        \
+  VEXTRACTI32X4             extract(numBlk), cSrc, xt1                                                                                                  \
+  /* shuffle and prep for GHASH */                                                                                                                      \
+  VMOVDQU8.Z                cSrc, maskReg, cSrc                                                                                                         \
+  shuffleBlk                (d0,d0x,d0y,d1,d1x,d1y,d2,d2x,d2y,d3,d3x,d3y,c0,c0x,c0y,c1,c1x,c1y,c2,c2x,c2y,c3,c3x,c3y,m,mx,my,blockOpx)                  \
+  VEXTRACTI32X4             extract(numBlk), dSrc, xt7                                                                                                  \
+  ADDQ                      $(16*(numBlk-1)), dataOffset                                                                                                \
+  SUBQ                      $(16*(numBlk-1)), length                                                                                                    \
+  VPXORQ                    hashInOut, d0, d0	                                                                                                          \
+  /* if multicall API, call GHASH with N-1 block count for partial final block */                                                                       \
+  /* jump table controlled by target and label passed from caller */                                                                                    \
+  /* "partial" JL target arg allows multicall partial mode to change jump table on each macro instance */                                               \
+  /* single call API "partial" is defined == "partialSingle" to jump to GHASH(N) */                                                                     \
+  /* multi call API "partial" is defined != "partialSingle", to jump to GHASH(N-1) */                                                                   \
+  /* "partialSingle" label arg allows multiple instances of macro expansion to exist in one function */                                                 \
+  CMPQ                      length, $16                                                                                                                 \
+  JL                        partial                                                                                                                     \
+  SUBQ                      $16, length                                                                                                                 \
+  ADDQ                      $16, dataOffset                                                                                                             \
+  MOVQ                      length, ctxPBLen(ctx)                                                                                                       \
+  JMP                       done                                                                                                                        \
+  partialSingle:                                                                                                                                        \
+    MOVQ                    length, ctxPBLen(ctx)                                                                                                       \
+    VMOVDQU64               xt1, ctxPBlockEncKey(ctx)                                                                                                   \
+  done:
+
+// GcmEncDecSmall helper macros
+#define storePBvals(label,ctx,xt1) \
+  label:                                                                                                                                                \
+    MOVQ                    length, ctxPBLen(ctx)                                                                                                       \
+    VMOVDQU64               xt1, ctxPBlockEncKey(ctx)
+
+#define GcmEncDecSmallFinalizeSingle(length,xt7,hashInOutx)
+
+#define GcmEncDecSmallFinalizeMulti(length,xt7,hashInOutx) \
+  ORQ                       length, length                                                                                                              \
+  JE                        GcmEncDecSmall_done                                                                                                         \
+  VPXORQ                    xt7, hashInOutx, hashInOutx
+
+#define GcmEncDecSmall(key,ctx,out,in,len,dataOffset,length,numBlocks,ctrz,ctrx,ctry,hashInOutz,hashInOutx,zt0,zt1,zt2,zt3,zt4,zt5,zt6,zt7,zt8,zt9,zt10,zt11,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,zt21,zt22,ia0,ia1,maskReg,shuffleMask,shuffleSB,aesFinal,T1,T2,T3,T4,T5,T6,T7,T8,T9,T10,T11,T12,T13,T14,T15,T16,finalize) \
+  /* GcmEncDecSmall is used for both standard and non-standard IV length APIs, i.e., "single call" and "multi call" */                                  \
+  VMOVDQA64                 hashInOutx, xt2                                                                                                             \
+  CMPQ                      numBlocks, $8                                                                                                               \
+  JE                        small_8                                                                                                                     \
+  JL                        small_7to1                                                                                                                  \
+  CMPQ                      numBlocks, $12                                                                                                              \
+  JE                        small_12                                                                                                                    \
+  JL                        small_11to9                                                                                                                 \
+  /* 16, 15, 14, or 13 */                                                                                                                               \
+  CMPQ                      numBlocks, $16                                                                                                              \
+  JE                        small_16                                                                                                                    \
+  CMPQ                      numBlocks, $15                                                                                                              \
+  JE                        small_15                                                                                                                    \
+  CMPQ                      numBlocks, $14                                                                                                              \
+  JE                        small_14                                                                                                                    \
+  JMP                       small_13                                                                                                                    \
+  small_11to9:                                                                                                                                          \
+    /* 11, 10, or 9 */                                                                                                                                  \
+    CMPQ                    numBlocks, $11                                                                                                              \
+    JE                      small_11                                                                                                                    \
+    CMPQ                    numBlocks, $10                                                                                                              \
+    JE                      small_10                                                                                                                    \
+    JMP                     small_9                                                                                                                     \
+  small_7to1:                                                                                                                                           \
+    CMPQ                    numBlocks, $4                                                                                                               \
+    JE                      small_4                                                                                                                     \
+    JL                      small_3to1                                                                                                                  \
+    /* 7, 6, or 5 */                                                                                                                                    \
+    CMPQ                    numBlocks, $7                                                                                                               \
+    JE                      small_7                                                                                                                     \
+    CMPQ                    numBlocks, $6                                                                                                               \
+    JE                      small_6                                                                                                                     \
+    JMP                     small_5                                                                                                                     \
+  small_3to1:                                                                                                                                           \
+    /* 3, 2, or 1 */                                                                                                                                    \
+    CMPQ                    numBlocks, $3                                                                                                               \
+    JE                      small_3                                                                                                                     \
+    CMPQ                    numBlocks, $2                                                                                                               \
+    JE                      small_2                                                                                                                     \
+  /* there are four cases for each block count: single call i) partial and ii) full final blocks; multi call iii) partial and iv) full final blocks */  \
+  /* i,ii,iv): for partial and full single call final block sizes or for full multi call final block sizes, i.e., numBytes=16,32,48,..., the */         \
+  /*           jmp target is inside initialBlocksPartial then drops through to ghash(N) */                                                              \
+  /* iii):     for multicall partial final blocks, i.e., numBytes = 1,2,..,15, or 17,18,..,31, ...initialBlocksPartial updates PBvals, jmps to PMn, */  \
+  /*           then drops through to ghash(N-1) */                                                                                                      \
+  /* where PMn, n=1,2,...,16 are jmp labels for the partial multicall case on each block count */                                                       \
+  small_1:                                                                                                                                              \
+    initialBlocksPartial    (key,ctx,out,in,length,dataOffset,ctrz,ctrx,-,hashInOutz,zt1,xt1,yt1,zt3,xt3,-,-,-,-,-,-,-,-,-,-,zt5,xt5,-,-,-,-,-,-,-,-,-,-,xt7,zt3,zt5,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,T1,P1,D1,1,initialBlocksCtxInit1,initialBlocksLoad1,initialBlocksStore1,blockOp1e,shuffleSB,aesFinal) \
+    ghash1                  (keyData,xt12,xt13,xt14,xt15,xt20,xt5,HashKey_1)                                                                            \
+    JMP                     small_complete                                                                                                              \
+    storePBvals             (PM1,ctx,xt1)                                                                                                               \
+    VPXORQ                  xt7, xt2, hashInOutx                                                                                                        \
+    JMP                     GcmEncDecSmall_done                                                                                                         \
+  small_2:                                                                                                                                              \
+    initialBlocksPartial    (key,ctx,out,in,length,dataOffset,ctrz,ctrx,ctry,hashInOutz,zt1,xt1,yt1,zt3,-,yt3,-,-,-,-,-,-,-,-,-,zt5,-,yt5,-,-,-,-,-,-,-,-,-,xt7,zt3,zt5,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,T2,P2,D2,2,initialBlocksCtxInit2,initialBlocksLoad1,initialBlocksStore1,blockOp2e,shuffleSB,aesFinal)  \
+    ghash2                  (keyData,yt12,yt13,yt14,yt15,yt20,yt5,HashKey_2)                                                                            \
+    JMP                     small_complete                                                                                                              \
+    storePBvals             (PM2,ctx,xt1)                                                                                                               \
+    ghash1                  (keyData,xt12,xt13,xt14,xt15,xt20,xt5,HashKey_1)                                                                            \
+    JMP                     small_complete                                                                                                              \
+  small_3:                                                                                                                                              \
+    initialBlocksPartial    (key,ctx,out,in,length,dataOffset,ctrz,ctrx,-,hashInOutz,zt1,xt1,yt1,zt3,-,-,-,-,-,-,-,-,-,-,-,zt5,-,-,-,-,-,-,-,-,-,-,-,xt7,zt3,zt5,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,T3,P3,D3,3,initialBlocksCtxInit34,initialBlocksLoad1,initialBlocksStore1,blockOp3e,shuffleSB,aesFinal) \
+    ghash3                  (keyData,zt12,zt13,zt14,zt15,zt20,yt20,zt5,HashKey_3)                                                                       \
+    JMP                     small_complete                                                                                                              \
+    storePBvals             (PM3,ctx,xt1)                                                                                                               \
+    ghash2                  (keyData,yt12,yt13,yt14,yt15,yt20,yt5,HashKey_2)                                                                            \
+    JMP                     small_complete                                                                                                              \
+  small_4:                                                                                                                                              \
+    initialBlocksPartial    (key,ctx,out,in,length,dataOffset,ctrz,ctrx,-,hashInOutz,zt1,xt1,yt1,zt3,-,-,-,-,-,-,-,-,-,-,-,zt5,-,-,-,-,-,-,-,-,-,-,-,xt7,zt3,zt5,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,T4,P4,D4,4,initialBlocksCtxInit34,initialBlocksLoad1,initialBlocksStore1,blockOp4e,shuffleSB,aesFinal) \
+    ghash4                  (keyData,zt12,zt13,zt14,zt15,zt20,zt5,HashKey_4)                                                                            \
+    JMP                     small_complete                                                                                                              \
+    storePBvals             (PM4,ctx,xt1)                                                                                                               \
+    ghash3                  (keyData,zt12,zt13,zt14,zt15,zt20,yt20,zt5,HashKey_3)                                                                       \
+    JMP                     small_complete                                                                                                              \
+  small_5:                                                                                                                                              \
+    initialBlocksPartial    (key,ctx,out,in,length,dataOffset,ctrz,ctrx,-,hashInOutz,zt1,xt1,yt1,zt3,-,-,zt4,xt4,-,-,-,-,-,-,-,zt5,-,-,zt6,xt6,-,-,-,-,-,-,-,xt7,zt4,zt6,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,T5,P5,D5,5,initialBlocksCtxInit58,initialBlocksLoad2,initialBlocksStore2,blockOp5e,shuffleSB,aesFinal) \
+    ghash5                  (keyData,zt12,zt13,zt14,zt15,zt16,xt16,zt17,xt17,zt18,xt18,zt19,xt19,zt20,xt20,zt5,xt6,HashKey_5)                           \
+    JMP                     small_complete                                                                                                              \
+    storePBvals             (PM5,ctx,xt1)                                                                                                               \
+    ghash4                  (keyData,zt12,zt13,zt14,zt15,zt20,zt5,HashKey_4)                                                                            \
+    JMP                     small_complete                                                                                                              \
+  small_6:                                                                                                                                              \
+    initialBlocksPartial    (key,ctx,out,in,length,dataOffset,ctrz,ctrx,-,hashInOutz,zt1,xt1,yt1,zt3,-,-,zt4,-,yt4,-,-,-,-,-,-,zt5,-,-,zt6,-,yt6,-,-,-,-,-,-,xt7,zt4,zt6,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,T6,P6,D6,6,initialBlocksCtxInit58,initialBlocksLoad2,initialBlocksStore2,blockOp6e,shuffleSB,aesFinal) \
+    ghash6                  (keyData,zt12,zt13,zt14,zt15,zt16,yt16,zt17,yt17,zt18,yt18,zt19,yt19,zt20,yt20,zt5,yt6,HashKey_6)                           \
+    JMP                     small_complete                                                                                                              \
+    storePBvals             (PM6,ctx,xt1)                                                                                                               \
+    ghash5                  (keyData,zt12,zt13,zt14,zt15,zt16,xt16,zt17,xt17,zt18,xt18,zt19,xt19,zt20,xt20,zt5,xt6,HashKey_5)                           \
+    JMP                     small_complete                                                                                                              \
+  small_7:                                                                                                                                              \
+    initialBlocksPartial    (key,ctx,out,in,length,dataOffset,ctrz,ctrx,-,hashInOutz,zt1,xt1,yt1,zt3,-,-,zt4,-,-,-,-,-,-,-,-,zt5,-,-,zt6,-,-,-,-,-,-,-,-,xt7,zt4,zt6,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,T7,P7,D7,7,initialBlocksCtxInit58,initialBlocksLoad2,initialBlocksStore2,blockOp7e,shuffleSB,aesFinal) \
+    ghash7                  (keyData,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,yt20,zt5,zt6,HashKey_7)                                               \
+    JMP                     small_complete                                                                                                              \
+    storePBvals             (PM7,ctx,xt1)                                                                                                               \
+    ghash6                  (keyData,zt12,zt13,zt14,zt15,zt16,yt16,zt17,yt17,zt18,yt18,zt19,yt19,zt20,yt20,zt5,yt6,HashKey_6)                           \
+    JMP                     small_complete                                                                                                              \
+  small_8:                                                                                                                                              \
+    initialBlocksPartial    (key,ctx,out,in,length,dataOffset,ctrz,ctrx,-,hashInOutz,zt1,xt1,yt1,zt3,-,-,zt4,-,-,-,-,-,-,-,-,zt5,-,-,zt6,-,-,-,-,-,-,-,-,xt7,zt4,zt6,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,T8,P8,D8,8,initialBlocksCtxInit58,initialBlocksLoad2,initialBlocksStore2,blockOp8e,shuffleSB,aesFinal) \
+    ghash8                  (keyData,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,zt5,zt6,HashKey_8)                                                    \
+    JMP                     small_complete                                                                                                              \
+    storePBvals             (PM8,ctx,xt1)                                                                                                               \
+    ghash7                  (keyData,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,yt20,zt5,zt6,HashKey_7)                                               \
+    JMP                     small_complete                                                                                                              \
+  small_9:                                                                                                                                              \
+    initialBlocksPartial    (key,ctx,out,in,length,dataOffset,ctrz,ctrx,-,hashInOutz,zt1,xt1,yt1,zt3,-,-,zt4,-,-,zt8,xt8,-,-,-,-,zt5,-,-,zt6,-,-,zt10,xt10,-,-,-,-,xt7,zt8,zt10,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,T9,P9,D9,9,initialBlocksCtxInit912,initialBlocksLoad3,initialBlocksStore3,blockOp9,shuffleSB,aesFinal) \
+    ghash9                  (keyData,zt12,zt13,zt14,zt15,zt16,xt16,zt17,xt17,zt18,xt18,zt19,xt19,zt20,xt20,zt5,zt6,xt10,HashKey_9)                      \
+    JMP                     small_complete                                                                                                              \
+    storePBvals             (PM9,ctx,xt1)                                                                                                               \
+    ghash8                  (keyData,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,zt5,zt6,HashKey_8)                                                    \
+    JMP                     small_complete                                                                                                              \
+  small_10:                                                                                                                                             \
+    initialBlocksPartial    (key,ctx,out,in,length,dataOffset,ctrz,ctrx,-,hashInOutz,zt1,xt1,yt1,zt3,-,-,zt4,-,-,zt8,-,yt8,-,-,-,zt5,-,-,zt6,-,-,zt10,-,yt10,-,-,-,xt7,zt8,zt10,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,T10,P10,D10,10,initialBlocksCtxInit912,initialBlocksLoad3,initialBlocksStore3,blockOp10,shuffleSB,aesFinal) \
+    ghash10                 (keyData,zt12,zt13,zt14,zt15,zt16,yt16,zt17,yt17,zt18,yt18,zt19,yt19,zt20,yt20,zt5,zt6,yt10,HashKey_10)                     \
+    JMP                     small_complete                                                                                                              \
+    storePBvals             (PM10,ctx,xt1)                                                                                                              \
+    ghash9                  (keyData,zt12,zt13,zt14,zt15,zt16,xt16,zt17,xt17,zt18,xt18,zt19,xt19,zt20,xt20,zt5,zt6,xt10,HashKey_9)                      \
+    JMP                     small_complete                                                                                                              \
+  small_11:                                                                                                                                             \
+    initialBlocksPartial    (key,ctx,out,in,length,dataOffset,ctrz,ctrx,-,hashInOutz,zt1,xt1,yt1,zt3,-,-,zt4,-,-,zt8,-,-,-,-,-,zt5,-,-,zt6,-,-,zt10,-,-,-,-,-,xt7,zt8,zt10,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,T11,P11,D11,11,initialBlocksCtxInit912,initialBlocksLoad3,initialBlocksStore3,blockOp11,shuffleSB,aesFinal) \
+    ghash11                 (keyData,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,yt20,zt5,zt6,zt10,HashKey_11)                                         \
+    JMP                     small_complete                                                                                                              \
+    storePBvals             (PM11,ctx,xt1)                                                                                                              \
+    ghash10                 (keyData,zt12,zt13,zt14,zt15,zt16,yt16,zt17,yt17,zt18,yt18,zt19,yt19,zt20,yt20,zt5,zt6,yt10,HashKey_10)                     \
+    JMP                     small_complete                                                                                                              \
+  small_12:                                                                                                                                             \
+    initialBlocksPartial    (key,ctx,out,in,length,dataOffset,ctrz,ctrx,-,hashInOutz,zt1,xt1,yt1,zt3,-,-,zt4,-,-,zt8,-,-,-,-,-,zt5,-,-,zt6,-,-,zt10,-,-,-,-,-,xt7,zt8,zt10,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,T12,P12,D12,12,initialBlocksCtxInit912,initialBlocksLoad3,initialBlocksStore3,blockOp12,shuffleSB,aesFinal) \
+    ghash12                 (keyData,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,zt5,zt6,zt10,HashKey_12)                                              \
+    JMP                     small_complete                                                                                                              \
+    storePBvals             (PM12,ctx,xt1)                                                                                                              \
+    ghash11                 (keyData,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,yt20,zt5,zt6,zt10,HashKey_11)                                         \
+    JMP                     small_complete                                                                                                              \
+  small_13:                                                                                                                                             \
+    initialBlocksPartial    (key,ctx,out,in,length,dataOffset,ctrz,ctrx,-,hashInOutz,zt1,xt1,yt1,zt3,-,-,zt4,-,-,zt8,-,-,zt9,xt9,-,zt5,-,-,zt6,-,-,zt10,-,-,zt11,xt11,-,xt7,zt9,zt11,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,T13,P13,D13,13,initialBlocksCtxInit1316,initialBlocksLoad4,initialBlocksStore4,blockOp13,shuffleSB,aesFinal) \
+    ghash13                 (keyData,zt12,zt13,zt14,zt15,zt16,xt16,zt17,xt17,zt18,xt18,zt19,xt19,zt20,xt20,zt5,zt6,zt10,xt11,HashKey_13)                \
+    JMP                     small_complete                                                                                                              \
+    storePBvals             (PM13,ctx,xt1)                                                                                                              \
+    ghash12                 (keyData,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,zt5,zt6,zt10,HashKey_12)                                              \
+    JMP                     small_complete                                                                                                              \
+  small_14:                                                                                                                                             \
+    initialBlocksPartial    (key,ctx,out,in,length,dataOffset,ctrz,ctrx,-,hashInOutz,zt1,xt1,yt1,zt3,-,-,zt4,-,-,zt8,-,-,zt9,-,yt9,zt5,-,-,zt6,-,-,zt10,-,-,zt11,-,yt11,xt7,zt9,zt11,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,T14,P14,D14,14,initialBlocksCtxInit1316,initialBlocksLoad4,initialBlocksStore4,blockOp14,shuffleSB,aesFinal) \
+    ghash14                 (keyData,zt12,zt13,zt14,zt15,zt16,yt16,zt17,yt17,zt18,yt18,zt19,yt19,zt20,yt20,zt5,zt6,zt10,yt11,HashKey_14)                \
+    JMP                     small_complete                                                                                                              \
+    storePBvals             (PM14,ctx,xt1)                                                                                                              \
+    ghash13                 (keyData,zt12,zt13,zt14,zt15,zt16,xt16,zt17,xt17,zt18,xt18,zt19,xt19,zt20,xt20,zt5,zt6,zt10,xt11,HashKey_13)                \
+    JMP                     small_complete                                                                                                              \
+  small_15:                                                                                                                                             \
+    initialBlocksPartial    (key,ctx,out,in,length,dataOffset,ctrz,ctrx,-,hashInOutz,zt1,xt1,yt1,zt3,-,-,zt4,-,-,zt8,-,-,zt9,-,-,zt5,-,-,zt6,-,-,zt10,-,-,zt11,-,-,xt7,zt9,zt11,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,T15,P15,D15,15,initialBlocksCtxInit1316,initialBlocksLoad4,initialBlocksStore4,blockOp15,shuffleSB,aesFinal) \
+    ghash15                 (keyData,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,yt20,zt5,zt6,zt10,zt11,HashKey_15)                                    \
+    JMP                     small_complete                                                                                                              \
+    storePBvals             (PM15,ctx,xt1)                                                                                                              \
+    ghash14                 (keyData,zt12,zt13,zt14,zt15,zt16,yt16,zt17,yt17,zt18,yt18,zt19,yt19,zt20,yt20,zt5,zt6,zt10,yt11,HashKey_14)                \
+    JMP                     small_complete                                                                                                              \
+  small_16:                                                                                                                                             \
+    initialBlocksPartial    (key,ctx,out,in,length,dataOffset,ctrz,ctrx,-,hashInOutz,zt1,xt1,yt1,zt3,-,-,zt4,-,-,zt8,-,-,zt9,-,-,zt5,-,-,zt6,-,-,zt10,-,-,zt11,-,-,xt7,zt9,zt11,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,T16,P16,D16,16,initialBlocksCtxInit1316,initialBlocksLoad4,initialBlocksStore4,blockOp16,shuffleSB,aesFinal) \
+    ghash16fs               (keyData,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,zt5,zt6,zt10,zt11,HashKey_16)                                         \
+    JMP                     small_complete                                                                                                              \
+    storePBvals             (PM16,ctx,xt1)                                                                                                              \
+    ghash15                 (keyData,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,yt20,zt5,zt6,zt10,zt11,HashKey_15)                                    \
+  small_complete:                                                                                                                                       \
+    ghashReduce             (hashInOutx,zt12,xt12,yt12,zt13,xt13,yt13,zt14,xt14,zt15,xt15,zt18,xt18,yt18,zt19,xt19,yt19,xt20,hXorMask)                  \
+    finalize                (length,xt7,hashInOutx)                                                                                                     \
+  GcmEncDecSmall_done:
+
+// GcmEncDecSmall for multi call API 
+// supports non-std IV len
+// same as single call except for 
+// 1) jump table labels PM1,...,PM16 to support ghash(N-1) on MC partial final blocks
+// 2) finalize function (last argument)
+#define GcmEncDecSmallMulti(key,ctx,out,in,len,dataOffset,length,numBlocks,ctrz,ctrx,ctry,hashInOutz,hashInOutx,zt0,zt1,zt2,zt3,zt4,zt5,zt6,zt7,zt8,zt9,zt10,zt11,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,zt21,zt22,ia0,ia1,maskReg,shuffleMask,shuffleSB,aesFinal) \
+  GcmEncDecSmall(key,ctx,out,in,len,dataOffset,length,numBlocks,ctrz,ctrx,ctry,hashInOutz,hashInOutx,zt0,zt1,zt2,zt3,zt4,zt5,zt6,zt7,zt8,zt9,zt10,zt11,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,zt21,zt22,ia0,ia1,maskReg,shuffleMask,shuffleSB,aesFinal,PM1,PM2,PM3,PM4,PM5,PM6,PM7,PM8,PM9,PM10,PM11,PM12,PM13,PM14,PM15,PM16,GcmEncDecSmallFinalizeMulti)
+
+// GcmEncDecSmall for single call API
+#define GcmEncDecSmallSingle(key,ctx,out,in,len,dataOffset,length,numBlocks,ctrz,ctrx,ctry,hashInOutz,hashInOutx,zt0,zt1,zt2,zt3,zt4,zt5,zt6,zt7,zt8,zt9,zt10,zt11,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,zt21,zt22,ia0,ia1,maskReg,shuffleMask,shuffleSB,aesFinal) \
+  GcmEncDecSmall(key,ctx,out,in,len,dataOffset,length,numBlocks,ctrz,ctrx,ctry,hashInOutz,hashInOutx,zt0,zt1,zt2,zt3,zt4,zt5,zt6,zt7,zt8,zt9,zt10,zt11,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,zt21,zt22,ia0,ia1,maskReg,shuffleMask,shuffleSB,aesFinal,P1,P2,P3,P4,P5,P6,P7,P8,P9,P10,P11,P12,P13,P14,P15,P16,GcmEncDecSmallFinalizeSingle)
+
+#define GCMEncDec(keyData,contextData,out,in,plaintextLen,SP64,shuffleSB,shuffleMB,shuffleBB,shuffleGhash,shuffleGhash8,shuffleGhashBB,maskOp,aesFinalSB,aesFinalMB,aesRoundsMB,aesFinalGhashMB,g16extKey,g16extAes,initExtAes,GcmEncDecSmall) \
+  /* reduction every 48 blocks, depth 32 blocks                                                                     */ \
+  /*                                                                                                                */ \
+  /* Macro flow:                                                                                                    */ \
+  /* - for message size bigger than very_big_loop_nblocks process data                                              */ \
+  /*   with "very_big_loop" parameters                                                                              */ \
+  /* - for message size bigger than big_loop_nblocks process data                                                   */ \
+  /*   with "big_loop" parameters                                                                                   */ \
+  /* - calculate the number of 16byte blocks in the message                                                         */ \
+  /* - process (number of 16byte blocks) mod 8                                                                      */ \
+  /*   '%%_initial_num_blocks_is_# .. %%_initial_blocks_encrypted'                                                  */ \
+  /* - process 8 16 byte blocks at a time until all are done in %%_encrypt_by_8_new                                 */ \
+  /* note: should measure perf impact of unaligned table loads for init vs. go-level alignment cost                 */ \
+  /*       could impact small buffer encyption with reduced amortization                                            */ \
+  /*       pack go-aligned constant buffers to allow indexed access/fewer loads                                     */ \
+  ORQ                   plaintextLen, plaintextLen                                                                     \
+  JE                    GCMEncDec_done                                                                                 \ 
+  KXNORQ 					      hXorMask, hXorMask, hXorMask  														                                     \ 
+  XORQ                  dataOffset, dataOffset                                                                         \
+  ADDQ                  plaintextLen, ctxInLen(contextData)                                                            \
+  VMOVDQU64             ctxAadHash(contextData), aadHashx                                                              \
+  VMOVDQU64             gcmInitCtrBlock, ctrBlockx                                                                     \
+  MOVQ                  plaintextLen, length                                                                           \
+  MOVQ                  ctxDdqAddBE4444Ptr(contextData), ptr                                                           \
+  VMOVDQA64             (64*0)(ptr), addBE4x4                                                                          \
+  MOVQ                  ctxDdqAddBE1234Ptr(contextData), ptr                                                           \
+  VMOVDQA64             (64*0)(ptr), addBE1234                                                                         \
+  CMPQ                  length, $(bigLoopNBlocks*16)                                                                   \
+  JL                    GCMEncDec_messageBelowBigNBlocks                                                               \
+  initialBlocksNx16     (in,out,keyData,dataOffset,aadHashz,ctrBlockz,ctrBlockx,ctrCheck,zt0,zt1,zt2,zt3,zt4,xt4,yt4,zt5,xt5,yt5,zt6,zt7,zt8,xt8,yt8,zt9,xt9,yt9,zt10,xt10,zt11,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,zt21,zt22,gh,gl,gm,addBE4x4,addBE1234,shuffleMask,bigLoopNBlocks,bigLoopDepth,SP64,redPoly,hXorMask,shuffleBB,shuffleGhashBB,g16extKey,g16extAes,initExtAes) \
+  SUBQ                  $(bigLoopNBlocks*16), length                                                                   \
+  CMPQ                  length, $(bigLoopNBlocks*16)                                                                   \
+  JL                    GCMEncDec_noMoreBigNBlocks                                                                     \
+  GCMEncDec_encryptBigNBlocks:                                                                                         \
+    ghashEncNx16Parallel  (in,out,keyData,dataOffset,ctrBlockz,shuffleMask,zt0,zt1,zt2,zt3,zt4,xt4,yt4,zt5,xt5,yt5,zt6,zt7,zt8,xt8,yt8,zt9,xt9,yt9,zt10,xt10,zt11,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,zt21,zt22,gh,gl,gm,addBE4x4,addBE1234,aadHashz,ctrCheck,redPoly,hXorMask,shuffleGhashBB,g16extKey,g16extAes) \
+    SUBQ                  $(bigLoopNBlocks*16), length                                                                 \
+    CMPQ                  length, $(bigLoopNBlocks*16)                                                                 \
+    JGE                   GCMEncDec_encryptBigNBlocks                                                                  \
+  GCMEncDec_noMoreBigNBlocks:                                                                                          \
+    VPSHUFB               shuffleMaskx, ctrBlockx, ctrBlockx                                                           \
+    VMOVDQA64             ctrBlockx, ctrBlockSavex                                                                     \
+    ghashLastNx16         (keyData,aadHashz,zt0,xt0,yt0,zt1,xt1,yt1,zt2,zt3,zt4,zt5,zt6,zt7,zt8,zt9,zt10,xt10,yt10,zt11,xt11,yt11,zt12,zt13,zt14,zt15,gh,gl,gm,SP64,hXorMask) \
+    ORQ                   length, length                                                                               \
+    JZ                    GCMEncDec_ghashDone                                                                          \
+  GCMEncDec_messageBelowBigNBlocks:                                                                                    \  
+    /* initialize add1234 and add5678 constants for initialBlocks() */                                                 \
+    /* diffence vs. IPSEC here, because 64-byte aligned constants are not possible in go asm */                        \
+    /* here we are computing the LE constants from BE constants already in registers */                                \
+    /* whereas in IPSEC the constants are loaded from memory as needed during later VPADDD ops */                      \
+    /* note: addBE1234, addBE4x4 registers are overwritten here and replaced by add1234, add5678 (LE) */               \
+    /*       should document register reuse in front matter comments / table */                                        \
+    VPSHUFB               shuffleMask, addBE1234, add1234                                                              \
+    VPSHUFB               shuffleMask, addBE4x4, add4x4                                                                \
+    VPADDD                add4x4, add4x4, add8888                                                                      \
+    VPADDD                add1234, add4x4, add5678                                                                     \
+    CMPQ                  length, $(16*16)                                                                             \
+    JGE                   GCMEncDec_largeMessagePath                                                                   \
+                                                                                                                       \
+    /* small message path: */                                                                                          \
+    /*   i) less than 256 bytes remaining from a message > 768 OR */                                                   \
+    /*  ii) small messages < 256 bytes */                                                                              \
+    /* determine how many blocks to process, +1 if there is a partial block */                                         \
+    MOVQ                  length, ia1                                                                                  \
+    ADDQ                  $15, ia1                                                                                     \
+    SHRQ                  $4, ia1                                                                                      \
+    /* ia1 in the range [0, 16] */                                                                                     \
+    GcmEncDecSmall        (keyData,contextData,out,in,plaintextLen,dataOffset,length,ia1,ctrBlockz,ctrBlockx,ctrBlocky,aadHashz,aadHashx,zt0,zt1,zt2,zt3,zt4,zt5,zt6,zt7,zt8,zt9,zt10,zt11,zt12,zt13,zt14,zt15,zt16,zt17,zt18,zt19,zt20,zt21,zt22,ia0,ia3,maskReg,shuffleMask,shuffleSB,aesFinalSB) \
+    VMOVDQA64             ctrBlockx, ctrBlockSavex                                                                     \
+    JMP                   GCMEncDec_ghashDone                                                                          \
+                                                                                                                       \
+  GCMEncDec_largeMessagePath:                                                                                          \
+    /* large message path: */                                                                                          \
+    /*    i) between [256,767] bytes remaining from a message > 768 OR */                                              \
+    /*   ii) a small message between [256,767] bytes in length */                                                      \
+    /* note: debug add1234, add5678 were here */                                                                       \
+    /* note: maskOutTopBlock; use register synthesis to avoid allocation of an aligned go-level table */               \
+    /*       use VPTERNLOGQ with 0xff to set all bits [x x x] -> [1], mask top block, do .Z merge to clear */          \
+    /*       in IPSEC maskOutTopBlock is generated via read from table in memory */                                    \
+    /*       should document K3 in register use table  */                                                              \
+	  KMOVW 					      kMaskTopBlock<>(SB), K3																														           \
+    VPTERNLOGQ.Z          $0xff, maskOutTopBlock, maskOutTopBlock, K3, maskOutTopBlock                                 \
+    /* determine how many blocks to process in initial() */                                                            \
+    /* process one addiitonal block in initial() if there is a partial block */                                        \
+    MOVQ                  length, ia1                                                                                  \
+    ANDQ                  $0xff, ia1                                                                                   \
+    ADDQ                  $15, ia1                                                                                     \
+    SHRQ                  $4, ia1                                                                                      \
+    /* don't allow 8 initial() blocks since this will be handled by the x8 partial loop */                             \
+    ANDQ                  $7, ia1                                                                                      \
+    JE                    GCMEncDec_initialNumBlocksIs_0                                                               \
+    CMPQ                  ia1, $1                                                                                      \
+    JE                    GCMEncDec_initialNumBlocksIs_1                                                               \
+    CMPQ                  ia1, $2                                                                                      \
+    JE                    GCMEncDec_initialNumBlocksIs_2                                                               \
+    CMPQ                  ia1, $3                                                                                      \
+    JE                    GCMEncDec_initialNumBlocksIs_3                                                               \
+    CMPQ                  ia1, $4                                                                                      \
+    JE                    GCMEncDec_initialNumBlocksIs_4                                                               \
+    CMPQ                  ia1, $5                                                                                      \
+    JE                    GCMEncDec_initialNumBlocksIs_5                                                               \
+    CMPQ                  ia1, $6                                                                                      \
+    JE                    GCMEncDec_initialNumBlocksIs_6                                                               \
+    /* note: document ia0 and ptr shared use */                                                                        \
+  GCMEncDec_initialNumBlocksIs_7:                                                                                      \
+    initialBlocks         (keyData,contextData,out,in,length,dataOffset,7,ctrBlockz,ctrBlockx,ctrBlocky,aadHashz,zt0,xt0,yt0,zt1,zt2,xt2,yt2,zt3,xt3,yt3,zt4,xt4,yt4,zt5,xt5,yt5,zt6,xt6,zt7,xt7,yt7,zt8,xt8,yt8,zt9,xt9,yt9,zt10,xt10,yt10,zt11,xt11,yt11,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,blockOp7,prepareAesCtrBlocks7,loadBlocks7,storeBlocks7,vclMulStep1Block7,ghashBlockPart1B0to7,vclMulStep2Block7,vclMulReduceBlock,shuffleMB,shuffleGhash,aesFinalMB,aesRoundsMB) \
+  GCMEncDec_initialNumBlocksIs_6:                                                                                      \
+    initialBlocks         (keyData,contextData,out,in,length,dataOffset,6,ctrBlockz,ctrBlockx,ctrBlocky,aadHashz,zt0,xt0,yt0,zt1,zt2,xt2,yt2,zt3,xt3,yt3,zt4,xt4,yt4,zt5,xt5,yt5,zt6,xt6,zt7,xt7,yt7,zt8,xt8,yt8,zt9,xt9,yt9,zt10,xt10,yt10,zt11,xt11,yt11,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,blockOp6,prepareAesCtrBlocks6,loadBlocks6,storeBlocks6,vclMulStep1Block6,ghashBlockPart1B0to7,vclMulStep2Block6,vclMulReduceBlock,shuffleMB,shuffleGhash,aesFinalMB,aesRoundsMB) \
+  GCMEncDec_initialNumBlocksIs_5:                                                                                      \
+    initialBlocks         (keyData,contextData,out,in,length,dataOffset,5,ctrBlockz,ctrBlockx,ctrBlocky,aadHashz,zt0,xt0,yt0,zt1,zt2,xt2,yt2,zt3,xt3,yt3,zt4,xt4,yt4,zt5,xt5,yt5,zt6,xt6,zt7,xt7,yt7,zt8,xt8,yt8,zt9,xt9,yt9,zt10,xt10,yt10,zt11,xt11,yt11,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,blockOp5,prepareAesCtrBlocks5,loadBlocks5,storeBlocks5,vclMulStep1Block5,ghashBlockPart1B0to7,vclMulStep2Block5,vclMulReduceBlock,shuffleMB,shuffleGhash,aesFinalMB,aesRoundsMB) \
+  GCMEncDec_initialNumBlocksIs_4:                                                                                      \
+    initialBlocks         (keyData,contextData,out,in,length,dataOffset,4,ctrBlockz,ctrBlockx,ctrBlocky,aadHashz,zt0,xt0,yt0,zt1,zt2,xt2,yt2,zt3,xt3,yt3,zt4,xt4,yt4,zt5,xt5,yt5,zt6,xt6,zt7,xt7,yt7,zt8,xt8,yt8,zt9,xt9,yt9,zt10,xt10,yt10,zt11,xt11,yt11,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,blockOp4,prepareAesCtrBlocks4,loadBlocks4,storeBlocks4,vclMulStep1Block1to4,ghashBlockPart1B0to7,vclMulStep2Block4,vclMulReduceBlock,shuffleMB,shuffleGhash,aesFinalMB,aesRoundsMB) \
+  GCMEncDec_initialNumBlocksIs_3:                                                                                      \
+    initialBlocks         (keyData,contextData,out,in,length,dataOffset,3,ctrBlockz,ctrBlockx,ctrBlocky,aadHashz,zt0,xt0,yt0,zt1,zt2,xt2,yt2,zt3,xt3,yt3,zt4,xt4,yt4,zt5,xt5,yt5,zt6,xt6,zt7,xt7,yt7,zt8,xt8,yt8,zt9,xt9,yt9,zt10,xt10,yt10,zt11,xt11,yt11,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,blockOp3,prepareAesCtrBlocks3,loadBlocks3,storeBlocks3,vclMulStep1Block1to4,ghashBlockPart1B0to7,vclMulStep2Block3,vclMulReduceBlock,shuffleMB,shuffleGhash,aesFinalMB,aesRoundsMB) \
+  GCMEncDec_initialNumBlocksIs_2:                                                                                      \
+    initialBlocks         (keyData,contextData,out,in,length,dataOffset,2,ctrBlockz,ctrBlockx,ctrBlocky,aadHashz,zt0,xt0,yt0,zt1,zt2,xt2,yt2,zt3,xt3,yt3,zt4,xt4,yt4,zt5,xt5,yt5,zt6,xt6,zt7,xt7,yt7,zt8,xt8,yt8,zt9,xt9,yt9,zt10,xt10,yt10,zt11,xt11,yt11,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,blockOp2,prepareAesCtrBlocks2,loadBlocks2,storeBlocks2,vclMulStep1Block1to4,ghashBlockPart1B0to7,vclMulStep2Block2,vclMulReduceBlock,shuffleMB,shuffleGhash,aesFinalMB,aesRoundsMB) \
+  GCMEncDec_initialNumBlocksIs_1:                                                                                      \
+    initialBlocks         (keyData,contextData,out,in,length,dataOffset,1,ctrBlockz,ctrBlockx,ctrBlocky,aadHashz,zt0,xt0,yt0,zt1,zt2,xt2,yt2,zt3,xt3,yt3,zt4,xt4,yt4,zt5,xt5,yt5,zt6,xt6,zt7,xt7,yt7,zt8,xt8,yt8,zt9,xt9,yt9,zt10,xt10,yt10,zt11,xt11,yt11,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,blockOp1,prepareAesCtrBlocks1,loadBlocks1,storeBlocks1,vclMulStep1Block1to4,ghashBlockPart1B0to7,vclMulStep2Block1,vclMulReduceBlock,shuffleMB,shuffleGhash,aesFinalMB,aesRoundsMB) \
+  GCMEncDec_initialNumBlocksIs_0:                                                                                      \
+    initialBlocks         (keyData,contextData,out,in,length,dataOffset,0,ctrBlockz,ctrBlockx,ctrBlocky,aadHashz,zt0,xt0,yt0,zt1,zt2,xt2,yt2,zt3,xt3,yt3,zt4,xt4,yt4,zt5,xt5,yt5,zt6,xt6,zt7,xt7,yt7,zt8,xt8,yt8,zt9,xt9,yt9,zt10,xt10,yt10,zt11,xt11,yt11,ia0,ia1,maskReg,shuffleMask,shuffleMaskx,shuffleMasky,blockOp0,prepareAesCtrBlocks0,loadBlocks0,storeBlocks0,vclMulStep1Block0,ghashBlockPart1NOP,vclMulStep2BlockNOP,vclMulReduceBlockNOP,shuffleMB,shuffleGhash,aesFinalMB,aesRoundsMB) \
+  GCMEncDec_initialBlocksEncrypted:                                                                                    \
+    VMOVDQA64             ctrBlockx, ctrBlockSavex                                                                     \
+    /* move cipher blocks from initial blocks to input of by8 macro */                                                 \
+    /* and for ghashLast8/7 */                                                                                         \
+    /* ghash value has already been xor'd into block 0 */                                                              \
+    VMOVDQA64             zt0, blk0                                                                                    \
+    VMOVDQA64             zt1, blk1                                                                                    \
+    /* set initial hashkey pointer based on length */                                                                  \
+    /* in order to have only one reduction at the end */                                                               \
+    LEAQ                  (8*16)(length), ia1                                                                          \
+    ADDQ                  $15, ia1                                                                                     \
+    ANDQ                  $0x3f0, ia1                                                                                  \
+    LEAQ                  (HashKey + 16*1)(keyData), hashkPtr                                                          \
+    SUBQ                  ia1, hashkPtr                                                                                \
+    /* hashkPtr - points at the first hash key to start GHASH, gets incremented as message is processed */             \
+    /* preload constants */                                                                                            \
+    /* synthesize CTR offsets in registers to avoid go-level tables w/ forced 64-bit alignment */                      \
+    VPSUBQ                add1234, add5678, add8888                                                                    \
+    VPADDQ                add8888, add8888, add8888                                                                    \
+    VPSHUFB               shuffleMask, add8888, addBE8888                                                              \
+    VPXORQ                gh, gh, gh                                                                                   \
+    VPXORQ                gl, gl, gl                                                                                   \
+    VPXORQ                gm, gm, gm                                                                                   \
+    /* prepare CTR for 8 blocks */                                                                                     \
+    VSHUFI64X2            $0, ctrBlockz, ctrBlockz, ctrBlockz                                                          \
+    VPADDD                add5678, ctrBlockz, ctrBlock2z                                                               \
+    VPADDD                add1234, ctrBlockz, ctrBlockz                                                                \
+    VPSHUFB               shuffleMask, ctrBlockz, ctrBlockz                                                            \
+    VPSHUFB               shuffleMask, ctrBlock2z, ctrBlock2z                                                          \
+    /* process 7 full blocks plus a partial block */                                                                   \
+    CMPQ                  length, $128                                                                                 \
+    JL                    GCMEncDec_encryptBy8Partial                                                                  \
+                                                                                                                       \
+  GCMEncDec_encryptBy8Parallel:                                                                                        \
+    /* in_order vs. out_order is an optimization to increment the counter  */                                          \
+    /* without shuffling it back to little endian */                                                                   \
+    /* ctrCheck keeps track of when to increment so that carry is handled correctly */                                 \
+    VMOVQ                 ctrBlockSavex, ctrCheck                                                                      \
+  GCMEncDec_encryptBy8New:                                                                                             \
+    ANDW                  $255, ctrCheck                                                                               \
+    ADDW                  $8, ctrCheck                                                                                 \
+    VMOVDQU64             (4*16)(hashkPtr), gh4Key                                                                     \
+    VMOVDQU64             (0*16)(hashkPtr), gh8Key                                                                     \
+    ghash8Encrypt8P       (keyData,out,in,dataOffset,ctrBlockz,ctrBlock2z,blk0,blk1,aesPartialBlock,ia0,ia1,length,gh4Key,gh8Key,shuffleMask,zt0,zt1,zt2,zt3,zt4,zt5,xt5,yt5,zt6,xt6,yt6,zt7,zt8,zt9,xt9,yt9,zt10,xt10,yt10,zt11,zt12,maskReg,gl,gh,gm,ghashGatherP1NR,ghash8Load,ghash8Store,shuffleGhash8,maskOp,aesFinalGhashMB) \
+    ADDQ                  $(8*16), hashkPtr                                                                            \
+    ADDQ                  $128, dataOffset                                                                             \
+    SUBQ                  $128, length                                                                                 \
+    JZ                    GCMEncDec_encryptDone                                                                        \
+                                                                                                                       \
+    CMPW                  ctrCheck, $(256-8)                                                                           \
+    JAE                   GCMEncDec_encryptBy8                                                                         \
+                                                                                                                       \
+    VPADDD                addBE8888, ctrBlockz, ctrBlockz                                                              \
+    VPADDD                addBE8888, ctrBlock2z, ctrBlock2z                                                            \
+    CMPQ                  length, $128                                                                                 \
+    JL                    GCMEncDec_encryptBy8Partial                                                                  \
+    JMP                   GCMEncDec_encryptBy8New                                                                      \
+                                                                                                                       \
+  /* note: untested block begins here */                                                                               \                                                                                                                         
+  GCMEncDec_encryptBy8:                                                                                                \
+    VPSHUFB               shuffleMask, ctrBlockz, ctrBlockz                                                            \
+    VPSHUFB               shuffleMask, ctrBlock2z, ctrBlock2z                                                          \
+    VPADDD                add8888, ctrBlockz, ctrBlockz                                                                \
+    VPADDD                add8888, ctrBlock2z, ctrBlock2z                                                              \
+    VPSHUFB               shuffleMask, ctrBlockz, ctrBlockz                                                            \
+    VPSHUFB               shuffleMask, ctrBlock2z, ctrBlock2z                                                          \
+    CMPQ                  length, $128                                                                                 \
+    JGE                   GCMEncDec_encryptBy8New                                                                      \
+    /* note: untested block ends here */                                                                               \
+                                                                                                                       \
+  GCMEncDec_encryptBy8Partial:                                                                                         \
+    /* test to see if we need a by 8 with partial block. At this point             */                                  \
+    /* bytes remaining are either zero or between 113-127                          */                                  \
+    /* 'in_order' shuffle needed to align key for partial block xor                */                                  \      
+    /* 'out_order' is a little faster because it avoids extra shuffles             */                                  \
+    VMOVDQU64             (4*16)(hashkPtr), gh4Key                                                                     \
+    VMOVDQU64             (0*16)(hashkPtr), gh8Key                                                                     \
+    ghash8Encrypt8P       (keyData,out,in,dataOffset,ctrBlockz,ctrBlock2z,blk0,blk1,aesPartialBlock,ia0,ia1,length,gh4Key,gh8Key,shuffleMask,zt0,zt1,zt2,zt3,zt4,zt5,xt5,yt5,zt6,xt6,yt6,zt7,zt8,zt9,xt9,yt9,zt10,xt10,yt10,zt11,zt12,maskReg,gl,gh,gm,ghashGatherP1NR,ghash8LoadMask,ghash8StoreMask,shuffleGhash8,maskOp,aesFinalGhashMB) \
+    ADDQ                  $(8*16), hashkPtr                                                                            \
+    ADDQ                  $(128-16), dataOffset                                                                        \
+    SUBQ                  $(128-16), length                                                                            \
+  GCMEncDec_encryptDone:                                                                                               \
+    /* extract the last counter block in LE format */                                                                  \
+    VEXTRACTI32X4         $3, ctrBlock2z, ctrBlockSavex                                                                \
+    VPSHUFB               shuffleMaskx, ctrBlockSavex, ctrBlockSavex                                                   \ 
+    ghashLast8            (keyData,blk1,blk1x,blk1y,blk0,blk0x,blk0y,zt0,zt1,zt2,zt3,xt3,zt4,xt4,yt4,zt5,xt5,yt5,aadHashx,gh,gl,gm) \
+  GCMEncDec_ghashDone:                                                                                                 \
+    VMOVDQU64             ctrBlockSavex, ctxCurCount(contextData)                                                      \
+    VMOVDQU64             aadHashx, ctxAadHash(contextData)                                                            \
+  GCMEncDec_done:
+
+#define GcmCompleteMultiCallExt(ctx) \
+    VMOVDQU                (ctxAadHash)(ctx), X14                              \
+    MOVQ                   (ctxPBLen)(ctx), R12                                \
+    CMPQ                   R12, $0                                             \
+    JE                     GCMCompleteMultiCall_done                           \
+    ghashMul               (X14,X13,X0,X10,X11,X5,X6)                          \
+    VMOVDQU                X14, (ctxAadHash)(ctx)                              \
+  GCMCompleteMultiCall_done:
+
+#define GcmCompleteSingleCallExt(ctx) 
+
+// ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+// ; GCM_COMPLETE Finishes Encryption/Decryption of last partial block after GCM_UPDATE finishes
+// ; for single call version partial block was already completed by GCMEncDec; GcmComplete 
+// ; computes the authentication tag, T = MSBt(GHASH(H, A, C) ⊕ E(K, Y0)); aadHash is in X14
+// ; Input: A gcm_key_data * (GDATA_KEY), gcm_context_data (GDATA_CTX).
+// ; Output: Authorization Tag (AUTH_TAG) and Authorization Tag length (AUTH_TAG_LEN)
+// ; Clobbers rax, r10-r12, and xmm0-xmm2, xmm5-xmm6, xmm9-xmm11, xmm13-xmm15
+// ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+#define GCMComplete(keyData,ctx,tag,tagLen,encryptSingleBlock,multiCallExt) \
+  VMOVDQU                 (HashKey)(keyData), X13                                               \
+  /* start AES as early as possible */                                                          \
+  VMOVDQU                 (ctxOrigIV)(ctx), X9         /* X9 = Y0 */                            \
+  encryptSingleBlock      (keyData,X9)                 /* E(K,Y0) */                            \
+  multiCallExt            (ctx)                                                                 \
+  MOVQ                    (ctxAadLen)(ctx), R12        /* R12 = aadLen */                       \
+  MOVQ                    (ctxInLen)(ctx), dataLen                                              \
+  SHLQ                    $3, R12                      /* get num bits */                       \
+  VMOVQ                   R12, X15                     /* X15 = len(A) in bits */               \
+  SHLQ                    $3, dataLen                  /* len(C) in bits */                     \
+  VMOVQ                   dataLen, X1                                                           \
+  VPSLLDQ                 $8, X15, X15                 /* X15 = len(A) || 0x0 */                \
+  VPXORQ                  X1, X15, X15                 /* X15 = len(A) || len(C) */             \
+  /* aadHash is already in X14 for single call; multi call requires readback from ctx */        \
+  VPXORQ                  X15, X14, X14                                                         \
+  ghashMul                (X14,X13,X0,X10,X11,X5,X6)                                            \
+  VPSHUFB  				        shufMaskU<>(SB), X14, X14                                             \
+  VPXORQ                  X14, X9, X9                                                           \
+  gc_ReturnT:                                                                                   \
+    MOVQ                  tag, R10                     /* R10 = authTag */                      \
+    MOVQ                  tagLen, R11                  /* R11 = authTagLen */                   \
+    CMPQ                  R11, $16                                                              \
+    JE                    gc_T16                                                                \
+    CMPQ                  R11, $12                                                              \
+    JE                    gc_T12                                                                \
+    CMPQ                  R11, $8                                                               \
+    JE                    gc_T8                                                                 \
+    /* tag lengths 1,2,3,5,6,7,9,10,11,13,14,15 */                                              \
+    simdStoreAvx15        (R10,X9,R11,R12,AX)                                                   \
+    JMP                   gc_ReturnTDone                                                        \
+  gc_T8:                                                                                        \
+    VMOVQ                 X9, AX                                                                \
+    MOVQ                  AX, (R10)                                                             \
+    JMP                   gc_ReturnTDone                                                        \
+  gc_T12:                                                                                       \
+    VMOVQ                 X9, AX                                                                \
+    MOVQ                  AX, (R10)                                                             \
+    VPSRLDQ               $8, X9, X9                                                            \
+    VMOVD                 X9, AX                                                                \
+    MOVD                  AX, (8*1)(R10)                                                        \
+    JMP                   gc_ReturnTDone                                                        \
+  gc_T16:                                                                                       \
+    VMOVDQU               X9, (R10)                                                             \
+  gc_ReturnTDone:                                                                               \
+
+//wrong argument size 80; expected $...-152
+//crypto/aes/gcmv_amd64.s:2942:1: [amd64] gcmAesEnc128_vaes: unknown variable aadLen; offset 112 is aad_len+112(FP)
+//crypto/aes/gcmv_amd64.s:2943:1: [amd64] gcmAesEnc128_vaes: unknown variable _authTag; offset 128 is authTag_base+128(FP)
+//crypto/aes/gcmv_amd64.s:2944:1: [amd64] gcmAesEnc128_vaes: unknown variable _authTagLen; offset 136 is authTag_len+136(FP)
+//crypto/aes/gcmv_amd64.s:2981:1: [amd64] gcmAesEnc128_vaes: unknown variable _keyData; offset 0 is keyData_base+0(FP)
+//crypto/aes/gcmv_amd64.s:2982:1: [amd64] gcmAesEnc128_vaes: unknown variable _contextData; offset 24 is ctx+24(FP)
+//crypto/aes/gcmv_amd64.s:2983:1: [amd64] gcmAesEnc128_vaes: unknown variable _out; offset 32 is out_base+32(FP)
+//crypto/aes/gcmv_amd64.s:2984:1: [amd64] gcmAesEnc128_vaes: unknown variable _in; offset 56 is in_base+56(FP)
+//crypto/aes/gcmv_amd64.s:2985:1: [amd64] gcmAesEnc128_vaes: unknown variable _in; offset 64 is in_len+64(FP)
+//crypto/aes/gcmv_amd64.s:2986:1: [amd64] gcmAesEnc128_vaes: unknown variable _iv; offset 80 is iv_base+80(FP)
+//crypto/aes/gcmv_amd64.s:2987:1: [amd64] gcmAesEnc128_vaes: unknown variable _aad; offset 104 is aad_base+104(FP)
+
+
+// func gcmAesEnc128/192/256_vaes(keyData []byte, contextData *gcmContextData, out, in, iv, aad, authTag []byte)
+//  768 assumes 3 x 256-deep entries are placed on the stack for the pipeline
+//  +64 byte padding to allow for 64-byte alignment for temp variables on the stack
+TEXT ·gcmAesEnc128_vaes(SB),0,$832-152
+
+#define keyData 				DI
+#define contextData			SI
+#define out 						DX
+#define in  						CX
+#define plaintextLen		R8
+#define iv 							R9
+#define ivLen           R8
+#define aad 						R10
+#define SP64            R9                    // 64-byte aligned stack pointer for AES temps
+
+#define encrypt(aesFinalSB,aesFinalMB,aesRoundsMB,aesGhashMB,aesKeyGhashLB,aesExtGhashLB,aesExtLB,encDecSmall) \
+  GCMEncDec             (keyData,contextData,out,in,plaintextLen,SP64,shuffle16EncSB,shuffleEncMB,shuffleEncBB,shuffleGhashEnc,shuffleGhash8Enc,ghashShuffleEncBB,ghash8StoreMaskOpEnc,aesFinalSB,aesFinalMB,aesRoundsMB,aesGhashMB,aesKeyGhashLB,aesExtGhashLB,aesExtLB,encDecSmall)
+
+#define decrypt(aesFinalSB,aesFinalMB,aesRoundsMB,aesGhashMB,aesKeyGhashLB,aesExtGhashLB,aesExtLB,encDecSmall) \
+  GCMEncDec             (keyData,contextData,out,in,plaintextLen,SP64,shuffle16DecSB,shuffleDecMB,shuffleDecBB,shuffleGhashDec,shuffleGhash8Dec,ghashShuffleDecBB,ghash8StoreMaskOpDec,aesFinalSB,aesFinalMB,aesRoundsMB,aesGhashMB,aesKeyGhashLB,aesExtGhashLB,aesExtLB,encDecSmall)
+
+#define GCM128(op) \
+  op                    (aesEncryptBlock16Final128,aesEncryptBlockFinal128,aesEncryptRoundsIB128,ghash8AesFinalRound128,ghash16Key128,ghash16Aes128,aesExtLB128,GcmEncDecSmallSingle)
+
+#define GCM192(op) \
+  op                    (aesEncryptBlock16Final192,aesEncryptBlockFinal192,aesEncryptRoundsIB192,ghash8AesFinalRound192,ghash16Key192256,ghash16Aes192,aesExtLB192,GcmEncDecSmallSingle)
+
+#define GCM256(op) \
+  op                    (aesEncryptBlock16Final256,aesEncryptBlockFinal256,aesEncryptRoundsIB256,ghash8AesFinalRound256,ghash16Key192256,ghash16Aes256,aesExtLB256,GcmEncDecSmallSingle)
+
+#define GCM128Update(op) \
+  op                    (aesEncryptBlock16Final128,aesEncryptBlockFinal128,aesEncryptRoundsIB128,ghash8AesFinalRound128,ghash16Key128,ghash16Aes128,aesExtLB128,GcmEncDecSmallMulti)
+
+#define GCM192Update(op) \
+  op                    (aesEncryptBlock16Final192,aesEncryptBlockFinal192,aesEncryptRoundsIB192,ghash8AesFinalRound192,ghash16Key192256,ghash16Aes192,aesExtLB192,GcmEncDecSmallMulti)
+
+#define GCM256Update(op) \
+  op                    (aesEncryptBlock16Final256,aesEncryptBlockFinal256,aesEncryptRoundsIB256,ghash8AesFinalRound256,ghash16Key192256,ghash16Aes256,aesExtLB256,GcmEncDecSmallMulti)
+
+#define GCMComplete128(ghashExt,tag,tagLen) \
+  GCMComplete           (keyData,contextData,tag,tagLen,encryptSingleBlock_128,ghashExt) 
+
+#define GCMComplete192(ghashExt,tag,tagLen) \
+  GCMComplete           (keyData,contextData,tag,tagLen,encryptSingleBlock_192,ghashExt) 
+
+#define GCMComplete256(ghashExt,tag,tagLen) \
+  GCMComplete           (keyData,contextData,tag,tagLen,encryptSingleBlock_256,ghashExt) 
+
+#define GCMGetInputs \
+  MOVQ 						      keyData_base+0(FP), keyData                \
+	MOVQ						      ctx+24(FP), contextData                    \
+	MOVQ						      out_base+32(FP), out                       \
+	MOVQ						      in_base+56(FP), in                         \
+	MOVQ						      in_len+64(FP), plaintextLen                \
+  MOVQ						      iv_base+80(FP), iv                         \
+  MOVQ						      aad_base+104(FP), aad
+
+// initialize a 64-byte aligned stack pointer to support 
+// vector-aligned load/store on AES temp data bewteen pipe stages 
+#define GCMInitSP64 \
+  MOVQ                  SP, SP64                                   \
+  ADDQ                  $64, SP64                                  \
+  ANDQ                  $0xffffffffffffffc0, SP64
+
+#define GCMInitStdIV \
+  GCMGetInputs                                                                                                                                                                                                                                                      \            
+  GCMInit					      (keyData,contextData,iv,aad,aad_len+112(FP),R10,R11,R12,K1,K2,X14,Z14,X2,Z2,Z1,X1,Y1,Z3,X3,Y3,Z4,X4,Y4,Z5,X5,Y5,Z6,X6,Y6,Z7,X7,Y7,Z8,X8,Z9,X9,Z10,X10,Y10,Z11,X11,Y11,Z12,X12,Y12,Z13,X13,Y13,Z15,X15,Y15,Z16,Z17,Z18,Z19,Z20,-,CalcCC,CalcAADHash1)
+
+#define GCMEnc128       GCM128(encrypt)
+#define GCMDec128       GCM128(decrypt)
+#define GCMEnc192       GCM192(encrypt)
+#define GCMDec192       GCM192(decrypt)
+#define GCMEnc256       GCM256(encrypt)
+#define GCMDec256       GCM256(decrypt)
+#define GCMEnc128Update GCM128Update(encrypt)
+#define GCMDec128Update GCM128Update(decrypt)
+#define GCMEnc192Update GCM192Update(encrypt)
+#define GCMDec192Update GCM192Update(decrypt)
+#define GCMEnc256Update GCM256Update(encrypt)
+#define GCMDec256Update GCM256Update(decrypt)
+
+  GCMInitStdIV
+  GCMEnc128
+  GCMComplete128(GcmCompleteSingleCallExt,authTag_base+128(FP),authTag_len+136(FP))
+RET
+
+TEXT ·gcmAesEnc192_vaes(SB),0,$832-152
+  GCMInitStdIV
+  GCMEnc192
+  GCMComplete192(GcmCompleteSingleCallExt,authTag_base+128(FP),authTag_len+136(FP))
+RET
+
+TEXT ·gcmAesEnc256_vaes(SB),0,$832-152
+  GCMInitStdIV
+  GCMEnc256
+  GCMComplete256(GcmCompleteSingleCallExt,authTag_base+128(FP),authTag_len+136(FP))
+RET
+
+#define GCMUpdateGetInputs \
+  MOVQ 						      keyData_base+0(FP), keyData               \
+	MOVQ						      ctx+24(FP), contextData                   \
+	MOVQ						      out_base+32(FP), out                      \
+	MOVQ						      in_base+56(FP), in                        \
+	MOVQ						      in_len+64(FP), plaintextLen
+
+// func gcmAesEncUpdate128/192/256_vaes(keyData []byte, contextData *gcmContextData, out, in []byte)
+TEXT ·gcmAesEncUpdate128_vaes(SB),0,$832-80
+  GCMUpdateGetInputs
+  GCMEnc128Update
+RET
+
+TEXT ·gcmAesEncUpdate192_vaes(SB),0,$832-80
+  GCMUpdateGetInputs
+  GCMEnc192Update
+RET
+
+TEXT ·gcmAesEncUpdate256_vaes(SB),0,$832-80
+  GCMUpdateGetInputs
+  GCMEnc256Update
+RET
+
+// func gcmAesDecUpdate128/192/256_vaes(keyData []byte, contextData *gcmContextData, out, in []byte)
+TEXT ·gcmAesDecUpdate128_vaes(SB),0,$832-80
+  GCMUpdateGetInputs
+  GCMDec128Update
+RET
+
+TEXT ·gcmAesDecUpdate192_vaes(SB),0,$832-80
+  GCMUpdateGetInputs
+  GCMDec192Update
+RET
+
+TEXT ·gcmAesDecUpdate256_vaes(SB),0,$832-80
+  GCMUpdateGetInputs
+  GCMDec256Update
+RET
+
+// func gcmAesInitVarIv_vaes(keyData []byte, contextData *gcmContextData, iv, aad []byte)
+TEXT ·gcmAesInitVarIv_vaes(SB),0,$0-80
+  MOVQ 						      keyData_base+0(FP), keyData
+	MOVQ						      ctx+24(FP), contextData
+  MOVQ						      iv_base+32(FP), iv
+  MOVQ						      iv_len+40(FP), ivLen
+  MOVQ						      aad_base+56(FP), aad
+  CMPQ                  ivLen, $12
+  JE                    aesGcmInitVarIV_ivLen12
+  GCMInit					      (keyData,contextData,iv,aad,aad_len+64(FP),R10,R11,R12,K1,K2,X14,Z14,X2,Z2,Z1,X1,Y1,Z3,X3,Y3,Z4,X4,Y4,Z5,X5,Y5,Z6,X6,Y6,Z7,X7,Y7,Z8,X8,Z9,X9,Z10,X10,Y10,Z11,X11,Y11,Z12,X12,Y12,Z13,X13,Y13,Z15,X15,Y15,Z16,Z17,Z18,Z19,Z20,ivLen,CalcJ0,CalcAADHash1)
+  JMP                   aesGcmInitVarIV_done
+aesGcmInitVarIV_ivLen12:
+  GCMInit					      (keyData,contextData,iv,aad,aad_len+64(FP),R10,R11,R12,K1,K2,X14,Z14,X2,Z2,Z1,X1,Y1,Z3,X3,Y3,Z4,X4,Y4,Z5,X5,Y5,Z6,X6,Y6,Z7,X7,Y7,Z8,X8,Z9,X9,Z10,X10,Y10,Z11,X11,Y11,Z12,X12,Y12,Z13,X13,Y13,Z15,X15,Y15,Z16,Z17,Z18,Z19,Z20,ivLen,CalcCC,CalcAADHash2)
+aesGcmInitVarIV_done:
+RET
+
+// func gcmAesDec128/192/256_vaes(keyData []byte, contextData *gcmContextData, out, in, iv, aad, authTag []byte)
+//   768 assumes 3 x 256-deep entries are placed on the stack for the pipeline
+//   +64 byte padding to allow for 64-byte alignment for temp variables on the stack
+TEXT ·gcmAesDec128_vaes(SB),0,$832-152
+  GCMInitStdIV
+  GCMDec128
+  GCMComplete128(GcmCompleteSingleCallExt,authTag_base+128(FP),authTag_len+136(FP))
+RET
+
+TEXT ·gcmAesDec192_vaes(SB),0,$832-152
+  GCMInitStdIV
+  GCMDec192
+  GCMComplete192(GcmCompleteSingleCallExt,authTag_base+128(FP),authTag_len+136(FP))
+RET
+
+TEXT ·gcmAesDec256_vaes(SB),0,$832-152
+  GCMInitStdIV
+  GCMDec256
+  GCMComplete256(GcmCompleteSingleCallExt,authTag_base+128(FP),authTag_len+136(FP))
+RET
+
+#define tag         DX
+#define tagLen      CX
+
+// func gcmAesFinish128/192/256_vaes(keyData []byte, contextData *gcmContextData, authTag []byte)
+TEXT ·gcmAesFinish128_vaes(SB),0,$0-56
+
+#define GCMFinalizeGetInputs \
+  MOVQ 						      keyData_base+0(FP), keyData                \
+	MOVQ						      ctx+24(FP), contextData                    \
+	MOVQ						      authTag_base+32(FP), tag                   \
+	MOVQ						      authTag_len+40(FP), tagLen
+
+  GCMFinalizeGetInputs
+  GCMComplete128(GcmCompleteMultiCallExt,tag,tagLen)
+RET
+
+TEXT ·gcmAesFinish192_vaes(SB),0,$0-56
+  GCMFinalizeGetInputs
+  GCMComplete192(GcmCompleteMultiCallExt,tag,tagLen)
+RET
+
+TEXT ·gcmAesFinish256_vaes(SB),0,$0-56
+  GCMFinalizeGetInputs
+  GCMComplete256(GcmCompleteMultiCallExt,tag,tagLen)
+RET
+
-- 
2.34.1

